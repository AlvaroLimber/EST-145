---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Tema 2: Distribuciones muestrales

A partir de este tema la estadística esta vinculada con la *inferencia* sobre los *parámetros* de la información/datos.

## Muestras y población

> Definición: Una población es una colección de objetos, estos objetos tienen variables.

Sea nuestra población $U$, esta población puede ser finita o infinita

Finitas,
$$U=\{u_1, u_2, \ldots , u_i,...,u_N \}$$
Infinita,

$$U=\{u_1, u_2, \ldots , u_i,... \}$$


Cada elemento de $U$ tiene variables o características asociadas:

$$u_i=\{X_{i1}, X_{i2}, \ldots, X_{iP} \}$$
$$u_j=\{X_{j1}, X_{j2}, \ldots, X_{jP} \}$$

> Definición, Muestra: Una muestra es un subconjunto de U.

Normalmente una muestra tiene un tamaño $n$, el mecanismo para obtener la muestra de $U$ puede ser con reposición o sin reposición, en cualquier caso podemos anotar esto de la siguiente forma, sea $s$ una muestra:

$$s=\{u_{1}^*,u_2^*, \ldots, u_n^* \}$$
Note que los elementos $u_1$ y $u_1^*$ no necesariamente son los mismos.

El subconjunto $s$ no es único y en realidad existen muchas muestras posibles, según el contexto, esto depende:

  * Del tamaño de $N$, $n$
  * Del mecanismo s/rep, c/rep.

Ejercicios, 

  1. Sea la población $U=\{a,b,c,d,e,f\}$, se define una muestra de $n=3$, escriba todas las muestras posibles según ambos mecanismos de reposición.

Solución,  
  * (s/rep), 20: $s_1=\{a,b,c\}$, $s_2=\{a,b,d \}$, $\ldots$ ,$s_{20}=\{d,e,f\}$ 
  * (c/rep), 216: $s_1=\{a,a,a\}$, $s_2={a,a,b}$, $\ldots$, $s_{216}=\{f,f,f\}$  
  
  2. En una población de 90 estudiantes, si se define una muestra de 10 estudiantes, según ambos mecanismos de selección ¿Cuantas muestras se pueden armar?
  * s/rep: 5720645481903
  * c/rep: 34867844009999998976  

Sin reposición:

$$Muestras_{Posibles}=\binom{N}{n}$$

Con reposición:

$$Muestras_{Posibles}=N^n$$

Imaginemos a la primera variable de interés $X_1$, para el universo esta variable tiene los elementos:

$$X_1=\{X_{11}, X_{21}, X_{31}, \ldots, X_{N1} \}$$

Imaginemos que observamos a $X_1$, para la muestra.

$$X_1^*=\{X_{11}^*, X_{21}^*, X_{31}^*, \ldots, X_{n1}^* \}$$

Estos $X_{i1}^*$ para los $i=1,\ldots,n$ son variables aleatorias. Por lo tanto $X_1$ es un vector aleatorio de tamaño $n$.

De ahora en adelante vamos a trabajar con un solo vector aleatorio denominado $X$, de tal forma que este sea la colección de $n$ variables aleatorias. 

$$X=\{X_1,X_2,\ldots,X_n \}$$

> Definición.

La colección del vector aleatorio $X=\{X_1,X_2,\ldots,X_n \}$, son independientes e idénticamente distribuidas (iid) si la distribución conjunta de las $n$ variables puede ser escrita como $f(x_1,x_2,\ldots,x_n)=f(x_1)*f(x_2)*\ldots*f(x_n)$ y además todas las $x_i$ tienen la misma función de distribución $F(x)$.

> Definición

Sea $N$ el tamaño de la población y $n$ el tamaño de la muestra, ambos valores para fines de este capítulo son fijas o constantes.

## Parámetros, estadísticas y estimadores.

El objetivo de la estadística es aprender acerca de las características de una población. Estas características las vamos a llamar *parámetros*.

> Definición,

Un parámetro $\theta$ es una función sobre la población $U$.

$$\theta=f(U,X,Y,Z,\ldots)$$
Nota: Los parámetros de una población son constantes.

Ejemplo, sea el universo los 10 primeros números naturales y sus valores. $Y=\{1,2,3,4,5,6,7,8,9,10\}$. Sobre estos valores de esta población de $N=10$ se pueden calcular los siguientes parámetros.

  * Total
  
$$\theta_1=t_y=\sum_U y_i=55$$
  * Media
  
$$\theta_2=\mu_y=\frac{t_y}{N}=\frac{55}{10}=5.5$$

  * Máximo: $\theta_3=max(y)=10$
  * Mínimo: $\theta_4=max(y)=1$

Es posibles hacer transformaciones sobre $Y$, sea $Z$ una variables binaria que identifique a los números primos de $Y$; $1=primo$, $0=\sim primo$

$$Z=\{1,1,1,0,1,0,1,0,0,0 \}$$

Calcular el promedio de $Z$

$$\theta_5=\mu_z=\frac{5}{10}=0.5$$

Cuando obtenemos la media de un vector binario, obtenemos lo que se denomina un proporción

$$\theta_5=P_a=\frac{\#A}{N}$$

$$\theta_5=P_{primos}=\frac{\#primos}{N}$$


> Definición, estadística

Se denomina estadística a una función sobre la muestra

> Definición, estimador

Un estimador $\hat{\theta}$ para el parámetro $\theta$ es una **estadística** que busca aproximar/adivinar el valor de $\theta$

Ejemplo, para la variable $Y=\{1,2,3,4,5,6,7,8,9,10\}$, imaginar que se selecciona un muestra de tamaño $n=4$ s/rep.

La cantidad de muestras posibles es de 210, supongamos que realizamos 2 procesos de selección para la muestra y obtenemos:

  * $s_1=\{8, 1, 3, 7\}$
  * $s_2=\{8, 2, 6, 5\}$

Sabemos que el parámetro del total de $Y$ es $t_y=55$, ¿Qué? función se puede aplicar sobre la muestra para postular a un estimador que se aproxime a $t_y$ sobre las 2 muestras seleccionadas

$$\hat{\theta}_1=\hat{t}_y=\sum_s y_i$$

Para $s_1$ el valor del estimador es de $\hat{t}_{y,s1}=19$, $\hat{t}_{y,s2}=21$, los valores evaluados sobre una muestra y un estimador se conoce como estimación

$$\hat{\theta}_2=\hat{t}_y=\prod_s y_i$$

Las estimaciones con el estimador propuesto $\hat{t}_{y,s1}=8*1*3*7=168$, $\hat{t}_{y,s1}=480$

$$\hat{\theta}_3=\frac{\prod_s y_i}{3}$$

$\hat{t}_{y,s1}=56$, $\hat{t}_{y,s2}=160$

$$\hat{\theta}_4=\frac{\sum_{s}y_i^2}{2}$$

$\hat{t}_{y,s1}=61.5$, $\hat{t}_{y,s2}=64.5$

$$\hat{\theta}_5=\frac{N}{n} \sum_{s}y_i $$

$\hat{t}_{y,s1}=47.5$, $\hat{t}_{y,s2}=52.5$

## Distribución muestral
Recordar que una estadística es una función sobre la muestra y sobre los valores que toman las variables aleatorias vinculadas a esta. Como la estadística es una función sobre las muestras aleatorias (muestras posibles) las evaluaciones que se realizan para cada una de las muestras posibles (estimadores) conforman lo que vamos a denominar una distribución muestral. 

Por ejemplo si planteamos al estimador del parámetro del total, recordar:

$$\theta=t_y=\sum_U y_i$$
Un estimador para este parámetro será:

$$\hat{\theta}=\hat{t}_y=\frac{N}{n} \sum_s y_i$$

Este $\hat{\theta}$ es una estadística sobre las muestras aleatorias, por lo tanto podemos decir que existe una distribución de probabilidad para este estimador, a esa distribución de probabilidad se conoce como distribución muestral.

Ejemplo práctico.

Supongamos que de una población de 6 personas tenemos la información de sus ingresos mensuales. $Y_{Ingresos}=\{2000,3000,3500,0,6000,4500\}$. $N=6$

Supongamos que seleccionamos una muestra de tamaño $n=3$ de esta población, para ambos mecanismos de selección (s/rep, c/rep), se pide para ambos mecanismos:

  * Conocer la cantidad de muestras posibles y mostrar estas.
  * Para el estimador 
  
  $$\hat{\bar{Y}}=\frac{1}{n}\sum_s y_i$$
  
construir su distribución muestral y calcular su esperanza y su varianza
  * Para el estimador; 
  
  $$\hat{t}_y=\frac{N}{n}\sum_s y_i$$ 
  
construir su distribución muestral y calcular su esperanza y su varianza
  
Respuesta, 

(S/rep) Las muestras posibles son 20, estas muestras posibles son:
```{r}
Y<-c(2000,3000,3500,0,6000,4500)
s<-combn(Y,3)
s
```

Para el estimador de la media;

Tomar en cuenta que el valor del parámetro de la media poblacional es: $\mu_y=\sum_U y_i /N=3166.667$

```{r}
y<-apply(s,2,sum)/3 #Distribución muestral para el estimador de la media
hist(y)
abline(v=mean(Y),col="red",lwd=3)
# calcular la esperanza y la varianza
uy<-sum(y*(1/20)) # esperanza del estimador de la media
sum((y-uy)^2*(1/20)) # varianza de la media muestral
```

$$E[\hat{\theta}]=\sum_{Rs} \hat{\theta_s} P(\hat{\theta}=\hat{\theta_s})$$

$$V(\hat{\theta})=E[(\hat{\theta}-E[\hat{\theta}])^2]=\sum_{s}(\hat{\theta_s}-E[\hat{\theta}])^2*P(\hat{\theta}=\hat{\theta})$$

Nota, Si $E[\hat{\theta}]=\theta$ decimos que el estimador $\hat{\theta}$ es un estimador insesgado (sin **sesgo**)

El estimador de la media muestral, es un estimador insesgado de la media poblacional.

Para el estimador del total;

Tomar en cuenta que el valor del parámetro del total poblacional es: $t_y=\sum_U y_i=19000$

```{r}
ty<-apply(s,2,sum)*(6/3) #Distribución muestral para el estimador del total
hist(ty)
abline(v=sum(Y),col="red",lwd=3)
pty<-sum(ty*(1/20)) # esperanza
sum((ty-pty)^2*(1/20)) # varianza de la media muestral
```

$$E[\hat{t}_y]=E[N*\bar{Y}]=N E[\bar{Y}]=N*u_y=N*\frac{\sum_U y_i}{N}=\sum_U {y_i}=t_y$$
Repetir los cálculos para un muestreo con reposición. Muestras probables $6^3=N^n=216$.

```{r,eval=FALSE}
Y<-round(rnorm(25,30,5))
s<-combn(Y,10)
y<-apply(s,2,sum)/10
hist(y)
abline(v=mean(Y),col="red",lwd=2)
```

## Distribución muestral para la media
Recordar que para una población ($U$) con alguna variable $X$ de tipo cuantitativa se puede obtener el parámetro de la media, definido como:

$$\mu_x=\frac{\sum_U x_i}{N}$$

Esta variable $X$ en la población por lo tanto tiene su media $\mu_x$ y también tiene su varianza, denotada por $\sigma_x^2$.

> Teorema: 

Sean $X_1,X_2,\ldots,X_n$ variables aleatorias para una muestra de tamaño $n$ extraida de la población $U$, donde estas $X_i$ independientes e idénticamente distribuidas (iid) como: $X_i\sim .(E[X_i]=\mu_x,V(X_i)=\sigma_x^2)$, entonces, si:

$$\bar{X}=\frac{\sum_s x_i}{n}$$

Tenemos que 

$$E[\bar{X}]=\mu_x$$  

$$V(\bar{X})=\sigma^2_{\bar{x}}=\frac{\sigma^2_x}{n}$$

Demostración, 

$$E[\bar{X}]=E\left[\frac{\sum_s x_i}{n}\right]=\frac{1}{n}E[x_1+x_2+\ldots+x_n]=\frac{1}{n}\left(E[x_1]+E[x_2]+\ldots+E[x_n] \right)=$$
$$=\frac{1}{n}(\mu_x+\mu_x+\ldots+\mu_x)=\frac{n \mu_x}{n}=\mu_x$$

Si, $X$ e $Y$ son independientes $Cov(X,Y)=0$.

$$V(X+Y)=V(X)+V(Y)$$

$$V(\bar{X})=V\left(\frac{\sum_s x_i}{n}\right)=\frac{1}{n^2}V(x_1+x_2+\ldots+x_n)=\frac{1}{n^2}\{V(x_1)+\ldots+V(x_n)\}=$$
$$=\frac{1}{n^2}(\sigma^2_x+\sigma^2_x+\ldots+\sigma^2_x)=\frac{n \sigma_x^2}{n^2}=\frac{\sigma^2_x}{n}$$

## Teorema del límite central

> Teorema:

Si $\bar{X}$ es la media de una muestra aleatoria de tamaño $n$. Tomada de una población $U$ con media $\mu_x$ y varianza finita $\sigma^2_x$. Entonces la forma límite de la distribución de:

$$Z=\frac{\bar{X}-E[\bar{X}]}{\sqrt{V(\bar{X})}}=\frac{\bar{X}-\mu_x}{\frac{\sigma_x}{\sqrt{n}}}$$
a medida que $n \rightarrow \infty$, podemos asegurar que $Z\sim N(0,1)$, en este marco se puede decir a medida que $n$ es más grande $\bar{X}\sim N(\mu_x,\frac{\sigma^2_x}{n})$ 

Nota: esta idea de $n$ grande se usa tradicionalmente el valor de $n>30$, hay textos que plantean $n=20$.

Simulación del teorema del límite central:

```{r}
N<-1000000
x<-round(runif(N,0,10000),0)# ingresos mensuales de una población 
hist(x)
n<-30
choose(N,n)

#simular 1000 muestras distintas de tamaño n y calcular su media.
xbar<-NULL
for(i in 1:10000){
  s<-sample(x,n)
  xbar[i]<-mean(s)
}
hist(xbar)
plot(density(xbar),col="blue",lwd=2)
points(density(rnorm(10^6,mean(x),sqrt(var(x)*((n-1)/n))/sqrt(n))),type="l",col="red",lwd=2)
```

## Distribución muestral para la diferencia de medias

Sean dos poblaciones $U_1$ y $U_2$ independientes con medias y varianzas respectivamente: $\mu_{x_1}$ y $\mu_{x_2}$, $\sigma^2_{x_1}$ y $\sigma^2_{x_2}$.

> Teorema:

La distribución muestral de las diferencias de media $\bar{X_1}-\bar{X_2}$ esta tiene una distribución aproximadamente normal ($n\rightarrow \infty$) con medias y varianzas dadas por:

$$E[\bar{X_1}-\bar{X_2}]=\mu_{x_1}-\mu_{x_2}$$

$$V(\bar{X_1}-\bar{X_2})=\frac{\sigma^2_{x_1}}{n_1}+\frac{\sigma^2_{x_2}}{n_2}$$
Demostración:

$$E[\bar{X_1}-\bar{X_2}]=E[\bar{X_1}]-E[\bar{X_2}]=\mu_{x_1}-\mu_{x_2}$$

$$V(\bar{X_1}-\bar{X_2})=V(\bar{X_1})+V(\bar{X_2})=\frac{\sigma^2_{x_1}}{n_1}+\frac{\sigma^2_{x_2}}{n_2}$$

## Distribución muestral para la proporción

La proporción no es nada más que un caso especial de la media para $X$ que toma valores binarios según alguna característica de interés.

Sea $P_A=\frac{\#A}{N}=\frac{\sum_U x_i}{N}$, $x_i=1$ si  $i \in A$ $x_i=0$ eoc. la proporción de alguna característica de la población.

Así la el estimador de la proporción sera:

$$\hat{P}_A=\frac{\sum_s{x_i}}{n}=\frac{\#a}{n}$$

> Teorema: 

Para el estadístico $\hat{P}_A$ se cumple cuando $n$ tiende a infinito los siguientes resultados:

  * $E[\hat{P}_A]=P_A$
  * $V(\hat{P}_A)=\frac{\sigma^2_A}{n}$
  * $\hat{P}_A\sim N(P_A,\frac{\sigma^2_A}{n})$, cuando $n \rightarrow \infty$
  
Tarea, encontrar la forma de $\sigma^2_A$, sabiendo que $x_i$ es binaria.

$$\sigma^2_A=\frac{\sum_U(x_i-\mu_x)^2}{N}= P_A *(1-P_A)$$

## Distribución muestral para la varianza

Recordar que para una población $U$, si observamos a una variable de interés respecto sus características podemos obtener medidas de centralidad y también medidas de variabilidad, por ejemplo, sea $X$ una variables definida para toda la población, y definamos los siguientes parámetros de $X$.

$$\mu_x=\frac{\sum_U x_i}{N}$$

Esta $\mu_x$ es una medida de centralidad, normalmente conocida como media, promedio de $X$, la otra medida puede ser:

$$\sigma^2_x=\frac{\sum_U (x_i-\mu_x)^2}{N}$$

$\sigma^2_x$ es la varianza poblacional

Ejemplo,

Sea una población de $N=5$ elementos con la variable $X=\{10,15,20,20,35\}$, calcular $\mu_x$ y $\sigma^2_x$.

  * $\mu_x=20$
  * $\sigma^2_x=70$
  
Suponer que se toman muestras aleatorias de esta población de tamaño $n=3$ sin reposición. La cantidad de muestras posibles es 10.

```{r}
x<-c(10,15,20,20,35)
s<-combn(x,3)
s
#distribución muestral de la media
mean(apply(s, 2, mean)) 
```
Pensemos para el caso de la varianza en posibles estadísticos (estimadores):

$$\hat{\theta}_1=\hat{\sigma}^2_x=\frac{\sum_s (x_i-\bar{x})^2}{n}$$

$$\hat{\theta}_2=\hat{S}^2_x=\frac{\sum_s (x_i-\bar{x})^2}{n-1}$$

```{r}
x<-c(10,15,20,20,35)
n<-3;N<-5
s
var(x)*((N-1)/N)

theta1<-apply(s,2,var)*((n-1)/n)
theta2<-apply(s,2,var)


theta1
theta2
plot(density(theta1),xlim=c(-50,300))
points(density(theta2),col="red",type="l")

mean(theta1) #E[]
mean(theta2) #E[]
```

Notar que para el ejemplo $E[\hat{\theta_1}]$ ni $E[\hat{\theta_2}]$ se acercan a $\sigma^2_x$, sin embargo, $E[\theta_2]=S^2_x$.

$$S^2_x=\frac{\sum_U (x_i-\mu_x)^2}{N-1}$$
> Teorema

Sea $X_1,X_2,\ldots,X_n$ una muestra aleatoria extraída de una población Normal $N(\mu_x,\sigma^2_x)$, definamos al estadístico:

$$\hat{S}^2_x=\frac{\sum_s (x_i-\bar{x})^2}{n-1}$$

Entonces, se cumple

$$\chi^2=\frac{(n-1)\hat{S}^2_x}{\sigma^2_x}=\frac{\sum_s (x_i-\bar{x})^2}{\sigma^2_x}\sim \chi^2(n-1)$$

Simulación;

```{r}
#población de tamaño N=1000
set.seed(999)
x<-rnorm(1000,20,5)
hist(x)
#suponer que se extra una muestra de n=20 de esta población, 
choose(1000,20)
n<-20
#vamos a simular unas 1500 muestras posibles sigma=25
x2<-NULL
for(i in 1:100000){
 x2[i]<-(var(sample(x,20))*(n-1))/25
}
x2
plot(density(x2))
points(density(rchisq(1000000,19)),col="red",type="l")
```

## Distribución $\chi^2$

Se dice que una variable aleatoria $X$ tiene una distribución Chi-cuadrado $\chi^2$ con $v$ grados de libertad. Se escribe como: $X \sim \chi^2(v)$, donde el $Rx=\{x>0\}$, si su función de densidad es:

$$f(x)=\frac{1}{2^\frac{v}{2} \Gamma(\frac{v}{2})}*x^{\frac{v}{2}-1}*e^{-\frac{x}{2}}$$
```{r}
curve(dchisq(x,1),xlim=c(0,40),ylim=c(0,0.4))
for(v in 1:30){
curve(dchisq(x,v),add=T,col=v)
  }
```

Donde, 

$$E[X]=v$$
$$V(X)=2v$$

Ejercicio,

Encuentre la probabilidad de que una muestra aleatoria de $n=25$ de un población normal con varianza $\sigma_x^2=9$, tenga una varianza muestral $\hat{S^2}$ entre 4 y 15.

$$P(4<\hat{S}^2_x<15)=P(4*24/9<\frac{(n-1)\hat{S}^2_x}{\sigma^2_x}<15*24/9)=P(10.66<\chi^2 <40)$$
$$P(10.66<\chi^2 <40)=F(40)-F(10.66)=0.9786-0.0087=0.9699$$

```{r}
pchisq(40,24)#F(40) para un chi2(v=24)
pchisq(10.66,24)#F(10.66) para un chi2(v=24)
curve(dchisq(x,24),xlim=c(0,60))
abline(v=c(10.66,40),col="red")
```


Tomar en cuenta que:

$$\chi^2=\frac{(n-1)\hat{S}^2_x}{\sigma^2_x}=\frac{\sum_s (x_i-\bar{x})^2}{\sigma^2_x}\sim \chi^2(n-1)$$


```{r}
#ejemplo para usar R para calcular probabilidades de la Chi2
pchisq(4,10) # F(t)=P(X<t): F(4)
```


## Distribución t-student

> Teorema 

Sea $Z$ una variable aleatoria normal estándar y $V$ una variable aleatoria chi-cuadrado con $v$ grados de libertad. Si $Z$ y $V$ son independientes, entonces la distribución de la variable aleatoria $X$, donde:

$$X=\frac{Z}{\sqrt{V/v}}$$
Se comporta como una distribución $t$ con $v$ grados de libertad. En notación, decimos $X\sim t(v)$.

$$f(x)=\frac{\Gamma(\frac{v+1}{2})}{\Gamma{(\frac{v}{2})}\sqrt{v\pi} }\left(1+\frac{x^2}{v} \right)^{-(\frac{v+1}{2})}, \quad -\infty<x<\infty$$

Al igual que la distribución normal estándar, la $t$ es simétrica al rededor del cero. Y levemente más plana que una normal. 

Nota: Cuando $v\rightarrow \infty$ la $t \sim N(\mu=0,\sigma^2=\frac{v}{v-2})$

```{r}
v<-5
curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4))
curve(dt(x,10),xlim=c(-5,5),main="t-student (v=10)",col="red",add=T)
```

> Corolario

Sean $X_1, X_2, \ldots, X_n$ variables aleatorias e independientes e idénticamente distribuidas (iid) $X_i\sim N(\mu,\sigma^2_x), \quad i=\{1,\ldots,n\}$. Sean los estimadores:

$$\bar{X}=\frac{\sum_s x_i}{n} \quad y \quad \hat{S}^2_x=\frac{\sum_s (x_i-\bar{X})^2}{n-1}$$

Entonces,

$$
\frac{\bar{X}-\mu}{\hat{S}/\sqrt{n}}\sim t(v=n-1)
$$
Apariencia de la $t$

```{r}
curve(dt(x,2),xlim=c(-5,5),ylim=c(0,0.4))
for(v in 3:30){
  curve(dt(x,v),xlim=c(-5,5),col=v,add=T)
}
```


En R, para obtener $P(X<t)=F(t)$ con $X\sim t(v)$.

```{r}
# P(X<2)=F(2), X ~ t(v=10)
pt(2,10)
```

Ejercicio, Sea $X\sim t(v=14)$. Calcular:

  * $P(X>0.67)=1-P(X\leq 0.67)=1-F(0.67)=1-0.7431=0.2569$
  * $P(X<0.5)=F(0.5)=0.6875$
  * $P(-1.96<X<1.96)=F(1,96)-F(-1.96)=0.9649-0.0351=0.9298$

## Distribución Fisher

> Teorema

Sean $U$ y $V$ dos variables aleatorias independientes, con $U\sim \chi^2(v_1)$ y $V \sim \chi^2(v_2)$. Y sea la variable $X$ definida como:

$$X=\frac{U/v_1}{V/v_2}$$
Así, decimos que $X$ se distribuye como una Fisher, $X\sim F(v_1,v_2)$, donde estas $v_1$ y $v_2$ son los grados de libertad de la Fisher. La forma de la distribución $f(x)$ es:


$$
f(x)=\frac{\left(\frac{v_1}{v_2} \right)^{v_1/2} x^{v_1/2-1}\Gamma{(\frac{v_1+v_2}{2})} }{\Gamma{(\frac{v_1}{2})} \Gamma{(\frac{v_2}{2})}\left(1+\frac{v_1}{v_2}x \right)^{(v_1+v_2)/2}}, \quad x>0
$$

### Para las varianzas muestrales

Suponga que las mnuetsras aleatorias de tamaños $n_1$ y $n_2$ se selecciona de 2 poblaciones normales con varianzas $\sigma^2_1$ y $\sigma^2_2$ respectivamente. Sabemos:

$$\chi^2_1=\frac{(n_1-1)\hat{S}_1}{\sigma^2_1}\sim \chi^2(v_1=n_1-1)$$

$$\chi^2_2=\frac{(n_2-1)\hat{S}_2}{\sigma^2_2}\sim \chi^2(v_2=n_2-1)$$
> Teorema:

Si $\hat{S_1}$ y $\hat{S_2}$ son los estimadores de la varianza de muestras aleatorias independientes entre ellas de tamaño $n_1$ y $n_2$, tomadas de poblaciones normales con varianzas $\sigma^2_1$ y $\sigma^2_2$ entonces:

$$\frac{\hat{S_1}/\sigma^2_1}{\hat{S_2}/\sigma^2_2}=\frac{\hat{S}_1*\sigma^2_2}{\hat{S}_2*\sigma^2_1}\sim F(v_1=n_1-1,v_2=n_2-1)$$

```{r}
curve(df(x,10,10),xlim=c(0,6))
```

En R

```{r}
#P(X<t)=F(t), donde  X ~ F(v1,v2). F(10,10), P(X<1)=F(1)
pf(1,10,10)
```


## Ejercicios 

1. La capacidad máxima de un ascensor es de 500 kilos. Si la distribución $X$ de los pesos de los usuarios es 

$$X\sim N(\mu=70,\sigma^2=100)$$
  
  * Cuál es la probabilidad de que 8 pasajeros sobrepasen ese límite
  * Cuál es la probabilidad de que 7 pasajeros sobrepasen ese límite
  * Cuál es la probabilidad de que 6 pasajeros sobrepasen ese límite

Solución,   

Sean $X_1, X_2, \ldots,X_p$ con $p$ la cantidad de pasajeros en el ascensor, suponemos que estas $X_i$ son iid $X_i\sim N(\mu=70,\sigma=10)$. Se pide:

$$P(Y=X_1+X_2+\ldots+X_p>500)$$
Notar que la suma de variables normales es también normal. $Y\sim N(\mu_y=p*\mu_x,\sigma^2_y=p*\sigma^2_x )$  

$$E[Y]=E[X_1+\ldots+X_p]=E[X_1]+\ldots+E[X_p]=\mu_x+\ldots+\mu_x=p*\mu_x$$
$$V(Y)=V(X_1+\ldots+X_p)=V(X_1)+\ldots+V(X_p)=\sigma^2_x+\ldots+\sigma^2_x=p*\sigma^2_x$$

  * Cuál es la probabilidad de que 8 pasajeros sobrepasen ese límite; $Y\sim N(\mu_y=8*70=560,\sigma^2_y=8*100=800)$

  $$P(Y>500)=P(Z>\frac{500-560}{\sqrt{800}})=P(Z>-2.12)=1-P(Z\leq -2.12)=1-\phi(-2.12)=$$
  
  $$=1-0.017=0.983$$
  
  * Cuál es la probabilidad de que 7 pasajeros sobrepasen ese límite, $Y\sim N(\mu_y=490,\sigma^2_y=700)$
  
  $$P(Y>500)=P(Z>0.3779)=1-\phi(0.3779)=1-0.647=0.353$$
  
  * Cuál es la probabilidad de que 6 pasajeros sobrepasen ese límite. (Ejercicio)
  
2. El viaje en un autobús especial para ir de un campus de una universidad al campus de otra en una ciudad toma, en promedio, 28 minutos, con una desviación estándar de 5 minutos. En cierta semana un autobús hizo el viaje 40 veces. ¿Cuál es la probabilidad de que el tiempo promedio del viaje sea mayor a 30 minutos? 

Solución, $n=40$, Sea $X$ una va. que explica el tiempo de viaje entre los dos campus. $X \sim .(\mu_x=28,\sigma=5)$, nos pide:

$$P(\bar{X}>30)$$

Recordar por el teorema del límite central que $\bar{X} \sim N(\mu_{\bar{x}}=28,\sigma^2_{\bar{x}}= \sigma^2_x/n=25/40)$ cuando $n>30$.

$$P(\bar{X}>30)=P\left(\frac{\bar{X}-\mu_x}{\sigma_x/\sqrt{n}} >\frac{30-28}{5/\sqrt{40}}\right)=P(Z>2.52)=1-P(Z\leq 2.52)\approx 1-\phi(2.52)=$$

$$=1-0.9941=0.0059$$

3. La calificación promedio de los estudiantes de primer año en un examen de aptitudes en cierta universidad es 540, con una desviación estándar de 50. Suponga que las medias se miden con cualquier grado de precisión. ¿Cuál es la probabilidad de que dos grupos seleccionados al azar, que constan de 32 y 50 estudiantes, respectivamente, difieran en sus calificaciones promedio por:

  * más de 20 puntos?
  * una cantidad entre 5 y 10 puntos?
  _
Solución, como información se tienen 2 muestras, $n_1=32$, $n_2=50$. Sea $X\sim . (\mu=540,\sigma=50)$, nos piden analizar la diferencia de las medias de los dos grupos, $\bar{X_1}$, $\bar{X}_2$:

  * más de 20 puntos?
  
  $$P( |\bar{X_1}-\bar{X}_2| >20)=1-P( |\bar{X_1}-\bar{X}_2| \leq 20)=1-P( -20\leq \bar{X_1}-\bar{X}_2 \leq 20)$$

Para la diferencia de medias $\bar{X_1}-\bar{X}_2\sim N(\mu_{\bar{X_1}-\bar{X}_2}=\mu_x-\mu_x=0,\sigma^2_{\bar{X_1}-\bar{X}_2}=\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}=50^2/32+50^2/50)$, entonces, $Y=\bar{X_1}-\bar{X}_2\sim N(\mu_y=0,\sigma^2_y=128.125)$, bajo el supuesto que las $\bar{X}_1$ y $\bar{X}_2$ tienden a ser normales por el teorema del límite central.

$$P( -20\leq \bar{X_1}-\bar{X}_2 \leq 20)=P(-20/11.31\leq Z \leq 20/11.31)\approx \phi(1.77)-\phi(-1.77)=$$  
  
  $$=0.962-0.038=0.9232$$
$$P( |\bar{X_1}-\bar{X}_2| >20)=1-0.9232=0.0768$$  
  
  * una cantidad entre 5 y 10 puntos? (ejercicio)
  
4. Suponga que las varianzas muestrales son mediciones continuas. Calcule la probabilidad de que una muestra aleatoria de 25 observaciones, de una población normal con varianza $\sigma^2 = 6$, tenga una varianza muestral $\hat{S}^2$

  * mayor que 9.1
  * entre 3.462 y 10.745.
  
$$P(\hat{S}^2>9.1)=P\left(\frac{(n-1)\hat{S^2}}{\sigma^2_x}> 24*9.1/6 \right)=P(\chi^2>36.4)=1-P(\chi^2\leq36.4)=$$  

$$=1-0.9498=0.0502$$

Teniendo en cuenta que $\chi^2\sim \chi^2(v=24)$

5. Un fabricante de cierta marca de barras de cereal con bajo contenido de grasa afirma que el contenido promedio de grasa saturada en éstas es de 0.5 gramos. En una muestra aleatoria de 8 barras de cereal de esta marca
se encontró que su contenido de grasa saturada era de 0.6, 0.7, 0.7, 0.3, 0.4, 0.5, 0.4 y 0.2. ¿Estaría de acuerdo con tal afirmación? Suponga una distribución normal. ($t$)

