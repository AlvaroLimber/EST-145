[["index.html", "Estadística II EST-145 Prefacio Audiencia Estructura del libro Software y acuerdos Bases de datos Agradecimiento", " Estadística II EST-145 Alvaro Chirino Gutierrez 2022-11-24 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Estadística II, una materia del pregrado de la carrera de Informática de la Universidad Mayor de San Ándres. Estructura del libro El libro incluye 6 capítulos, estos son: Distribuciones de probabilidad bivariada Distribuciones muestrales Distribuciones Chi cuadrado, t-student y Fisher Estimación puntual de parámetros Estimación de parámetros por intervalos de confianza Pruebas de hipótesis Software y acuerdos sessionInfo() ## R version 4.2.2 (2022-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages: ## [1] stats graphics grDevices utils ## [5] datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.27 digest_0.6.29 ## [3] R6_2.5.1 jsonlite_1.8.0 ## [5] magrittr_2.0.3 evaluate_0.15 ## [7] highr_0.9 stringi_1.7.6 ## [9] rlang_1.0.2 cli_3.3.0 ## [11] rstudioapi_0.14 jquerylib_0.1.4 ## [13] bslib_0.3.1 rmarkdown_2.14 ## [15] tools_4.2.2 stringr_1.4.0 ## [17] xfun_0.31 yaml_2.3.5 ## [19] fastmap_1.1.0 compiler_4.2.2 ## [21] htmltools_0.5.2 knitr_1.39 ## [23] sass_0.4.1 Bases de datos En este documento se emplearan 2 bases de datos del contexto Boliviano: Encuesta a Hogares 2019 y 2019. Vivienda y Personas Computo oficial de las elecciones del 18 de Octubre de 2020 Estas bases de datos se encuentran disponibles en formato \\(.RData\\) en el repositorio de Github del texto. Agradecimiento "],["distribuciones-de-probabilidad-conjunta.html", "1 Distribuciones de probabilidad conjunta 1.1 Variables aleatorias bivariantes 1.2 Función de distribución bivariada 1.3 Distribución marginal 1.4 Independencia 1.5 Valores esperados 1.6 Distribuciones condicionales 1.7 Esperanza condicional 1.8 Medidas de relación entre dos variables 1.9 Transformaciones 1.10 Distrubuciones conjuntas de más de 2 variables aleatorias", " 1 Distribuciones de probabilidad conjunta En el caso univariado se tenia a una va \\(X\\) definida en los reales \\(\\mathbb R\\), a esta va se le asignaba una función de distribución \\(F(x)\\) y una función de densidad \\(f(x)\\). Ambas distribuciones tienen su correspondencia en lo discreto y lo continuo: Caso discreto: \\[\\sum_{Rx} P(X=x)=1\\] \\[F(t)=P(X\\leq t)=\\sum_{x\\leq t} P(X=x)\\] Caso continuo: \\[\\int_{Rx}f(x)dx=1\\] \\[F(t)=P(X\\leq t)=\\int_{-\\infty}^{t}f(x)dx\\] Recordar que las funciones de probabilidad estudiadas en estadística I son funciones útiles cuando es posible vincular su comportamiento a los datos. Estas funciones son llamadas distribuciones/funciones paramétricas. Algunos ejemplos: \\(X \\sim Bernoulli(P)\\) \\(X \\sim Binomial(n,P)\\) \\(X \\sim exp(\\lambda)\\) \\(X \\sim N(\\mu,\\sigma)\\) La idea de este capitulo es ver las propiedades en el caso bivariante (\\(X_1,X_2\\)) y generalizar para el caso multivariante (\\(X_1, X_2, X_3, \\ldots, X_n\\)). 1.1 Variables aleatorias bivariantes Son un par de variables aleatorias con una distribución conjunta, son tipicamente representadas con mayúscula \\((X, Y)\\) o \\((X_1,X_2)\\), las realizaciones de estas variables aleatorias se representan como \\((x,y)\\) o \\((x_1,x_2)\\). Definición 1: Un par de variables aleatorias bivariadas es un par numérico de resultados; una función definida en \\(\\mathbb R^2\\) Ejemplos: Considerar el par (edad, estatura cm): \\((23,170)\\) \\((20,172)\\) \\((20,154)\\) \\((26,159)\\) \\((19,175)\\) Considerar el par: (ingreso, años experiencia) Imaginar lanzar 2 monedas simultáneamente, \\(\\Omega=\\{CC,CS,SC,SS\\}\\), si definimos a \\(Cara=1\\) y \\(Sello=0\\), \\(R_{(X,Y)}=\\{(1,1),(1,0),(0,1),(0,0)\\}\\). 1.2 Función de distribución bivariada Definición 2. La función de distribución conjunta de \\((X,Y)\\) es \\[F(x,y)=P(X\\leq x,Y\\leq y)=P\\left[\\{X\\leq x\\} \\cap \\{Y\\leq y\\} \\right]\\] Las propiedades de \\(F\\) son similares al caso univariante, \\(0\\leq F(x,y)\\leq 1\\). \\(F(-\\infty,-\\infty)=0\\), \\(F(\\infty,\\infty)=1\\) Funciones de densidad/probabilidad bivariadas Discreto: \\(p(x_1,x_2)=P(X_1=x_1,X_2=x_2)=\\pi(x_1,x_2)\\) \\[0 \\leq \\pi(x_1,x_2) \\leq 1\\] \\[\\sum_{Rx_1}\\sum_{Rx_2} \\pi(x_1,x_2)=1\\] Continuo: \\(f(x_1,x_2)\\) \\[\\int_{Rx_1}\\int_{Rx_2} f(x_1,x_2)dx_2 dx_1=1\\] Ejercicio 1: Sea la siguiente función de densidad: \\[f(x_1,x_2)=\\frac{1}{500}; \\quad 0&lt;x_1&lt;0.25 \\quad 0 &lt; x_2&lt;2000\\] + Verificar si es una función de densidad + Encontrar la función de distribución (\\(F\\)) \\[F(x_1,x_2)=\\frac{x_1 x_2}{500}\\] Calcular la probabilidad de: \\[P(x_1&gt;0.1,x_2&lt;1000)=0.3\\] \\[P(x_1=0.1,x_2&lt;500)=0\\] Ejercicio. Sea la función de distribución: \\[F(x,y)=(1-e^{-x})(1-e^{-y});\\hspace{2cm} x,y\\geq0 \\] Se pide: Verificar si es una función de distribución \\(P(X&lt;5,Y&lt;10)\\) \\(P(X&gt;100,Y&lt;50)\\) Solución: Verificar si es una función de distribución \\[F(X=0,Y=0)=F(0,0)=(1-e^{-0})(1-e^{-0})=0\\] \\[\\lim_{t_1,t_2 \\rightarrow \\infty} F(t_1,t_2)=(1-e^{-\\infty})(1-e^{-\\infty})=1\\] \\(P(X&lt;5,Y&lt;10)\\) \\[F(x,y)=P(X&lt;x,Y&lt;y)=F(5,10)=(1-e^{-5})(1-e^{-10})= 0.993217\\] Fxy&lt;-function(x,y){ pp&lt;-(1-exp(-x))*(1-exp(-y)) return(pp) } Fxy(5,10) ## [1] 0.993217 \\(P(X&gt;100,Y&lt;50)=¿?\\) La distribución conjunta satisface: \\[P(a&lt;X\\leq b,c&lt; Y\\leq d)=F(b,d)-F(b,c)-F(a,d)+F(a,c)\\] Para \\(a&lt;b\\) y \\(c&lt;d\\) Ejemplo: \\(P(X&gt;100,Y&lt;50)=¿?\\) \\[P(X&gt;100,Y&lt;50)=F(\\infty,50)-F(\\infty,0)-F(100,50)+F(100,0)\\] \\[=1-0-1+0=0\\] Definición 3 \\[f(x,y)=\\frac{\\partial^2}{\\partial x \\partial y}F(x,y)\\] \\[\\int_{Rx}\\int_{Ry}f(x,y)dydx=1\\] Ejercicio Encontrar la función de densidad de: \\[F(x,y)=(1-e^{-x})(1-e^{-y});\\hspace{2cm} x,y\\geq0\\] Solución \\[f(x,y)=\\frac{\\partial^2}{\\partial x \\partial y}\\left[(1-e^{-x})(1-e^{-y})\\right]=e^{-x}e^{-y}\\] Verificar si efectivamente es una función de densidad \\[\\int_0^{\\infty}\\int_0^{\\infty} e^{-x}e^{-y} dx dy=1\\] Ejercicio Probar que la siguiente función es una función de densidad \\[f(x,y)=x+y \\quad ;0 \\leq x \\leq 1 \\quad 0\\leq y \\leq 1\\] \\[\\int_0^1 \\int_0^1 (x+y) dydx=\\int_0^1 \\left(x*y/_0^1+\\frac{y^2}{2}/_0^1 \\right)dx =\\int_0^1 x+\\frac{1}{2} dx\\] \\[=\\frac{x^2}{2}/_0^1+\\frac{1}{2}=\\frac{1}{2}+\\frac{1}{2}=1\\] Para el ejercicio: \\[\\int_0^{\\infty} \\int_0^{\\infty}e^{-x}e^{-y} dx dy=\\int_0^{\\infty} e^{-y} \\left[-e^{-x}/_0^{\\infty} \\right] dy =1\\] \\[P(a&lt;X\\leq b,c&lt; Y\\leq d)=\\int_a^b \\int_c^d f(x,y)dxdy\\] Para el caso discreto podemos definirlo de la siguiente forma: \\[f(x,y)=\\pi(x,y)=P(X=x,Y=y)\\] \\[\\sum_{Rx}\\sum_{Ry}\\pi(x,y)=1\\] &gt; Ejercicio Sea la función de probabilidad discreta: \\[\\pi(x,y)=\\frac{x+y}{30}, \\quad x=0,1,2,3; y=0,1,2\\] Verificar que se trata de una función de probabilidad y luego encontrar: \\[P(X\\leq 2, Y=1)\\] \\[P(X&gt; 2, Y\\leq 1)\\] Solución \\[1=\\sum_{x=0}^3 \\sum_{y=0}^2 \\pi(x,y)=\\sum_{x=0}^3 \\sum_{y=0}^2\\frac{x+y}{30}=1\\] \\[P(X\\leq 2, Y=1)=\\pi_{0,1}+\\pi_{1,1}+\\pi_{2,1}=0.2\\] \\[P(X&gt; 2, Y\\leq 1)=\\pi_{3,0}+\\pi_{3,1}=\\frac{3}{30}+\\frac{4}{30}=\\frac{7}{30}\\] Solución en R. #crear la función pxy&lt;-function(x,y){ return((x+y)/30) } aux&lt;-0 for(x in 0:3){ for(y in 0:2){ aux&lt;-aux+pxy(x,y) } } sum(pxy(0:2,1)) ## [1] 0.2 pxy(3,0)+pxy(3,1) ## [1] 0.2333333 Ejemplo Sea la función de densidad: \\[f(x,y)=\\frac{x}{8}; \\quad 0\\leq x\\leq 2;0\\leq y\\leq 4 \\] Verificar si es una función de densidad y calcular: \\[P(X&gt;1,Y&gt;2)\\] Obtener la función de distribución (\\(F\\)) Solución \\[\\int_0^2 \\int_0^4 \\frac{x}{8}dy dx=\\int_0^2 \\frac{x}{8} y/_0^4=\\int_0^2\\frac{x}{2}dx=\\frac{x^2}{4}/_0^2=1\\] \\[P(X\\geq1,Y\\geq2)=P(X&gt;1,Y&gt;2)=\\int_1^2 \\int_2^4 \\frac{x}{8}dy dx=\\] \\[=\\int_1^2 \\frac{x}{8} y/_2^4 dx=\\int_1^2 \\frac{x}{4} dx= \\frac{x^2}{8}/_1^2=\\frac{4-1}{8}=\\frac{3}{8}\\] Tarea: \\[F(x,y)=F(t,r)=\\int_0^t \\int_0^r \\frac{x}{8} dy dx\\] 1.3 Distribución marginal La distribución conjunta del vector aleatorio \\((X,Y)\\) describe la distribución del vector aleatorio, sin embargo, es posible a partir de la distribución conjunta, generar las distribuciones para cada componente del vector aleatorio. Definición 4, la distribución marginal de X es: \\[F_X(x)=P(X\\leq x)=P(X\\leq x, Y\\leq \\infty)=lim_{y\\rightarrow \\infty} F(x,y)\\] \\[F_Y(y)=P(Y\\leq y)=P(X\\leq \\infty, Y\\leq y)=lim_{x\\rightarrow \\infty} F(x,y)\\] De manera más usual se tiene: Para el caso discreto: \\[P(X=x)=\\pi(x)=\\sum_{Ry} \\pi(x,y)\\] \\[P(Y=y)=\\pi(y)=\\sum_{Rx} \\pi(x,y)\\] Para el caso continuo: \\[f(x)=\\int_{Ry} f(x,y)dy\\] \\[f(y)=\\int_{Rx} f(x,y)dx\\] Ejercicio Encontrar las distribuciones marginales de la siguiente función de densidad conjunta: \\[f(x,y)=x^2+\\frac{xy}{3}; \\quad 0&lt;x\\leq 1 \\quad 0\\leq y \\leq2\\] \\[f(y)=\\int_0^1 x^2+\\frac{xy}{3} dx= \\frac{x^3}{3}/_0^1+ \\frac{yx^2}{6}/_0^1=\\frac{1}{3}+\\frac{y}{6}\\] \\[f(x)=\\int_0^2 x^2+\\frac{xy}{3} dy=x^2 y/_0^2 + \\frac{x y^2}{6}/_0^2=2x^2+\\frac{2x}{3}\\] Ejemplo Encontrar las distribuciones marginales de la función \\[f(x,y)=\\frac{x}{8}; \\quad 0\\leq x\\leq 2 \\quad 0\\leq y\\leq 4\\] Solución: \\[f(x)=\\int_0^4 \\frac{x}{8} dy= \\frac{x}{2}; \\quad 0\\leq x \\leq2\\] \\[f(y)=\\int_0^2 \\frac{x}{8} dx=\\frac{x^2}{16}/_0^2=\\frac{1}{4}; \\quad 0\\leq y \\leq 4\\] 1.4 Independencia Definición 6, dos variables aleatorias son independientes si: Se dice que dos variables aleatorias son independientes si sus funciones de densidad o probabilidad cumplen los siguiente: \\[f(x,y)=f(x)f(y)\\] \\[\\pi(x,y)=\\pi(x)\\pi(y)\\] La independencia de dos variables aleatorias implica que el comportamiento de una variable no afecta a la otra. Por ejemplo en el ejercicio con la función: \\[f(x,y)=x^2+\\frac{xy}{3}; \\quad 0&lt;x\\leq 1 \\quad 0\\leq y \\leq2\\] Las variables no son independientes. Para el otro ejemplo: \\[f(x,y)=\\frac{x}{8}; \\quad 0\\leq x\\leq 2 \\quad 0\\leq y\\leq 4\\] Efectivamente las variables son independientes. Nota: Cuando definimos la independencia, estamos interesados en entender el nivel de relación que existe en las variables. Ejercicio: \\[f(x,y)=e^{-x}e^{-y}\\] Con \\(x,y\\geq 0\\), encontrar las marginales de \\(f(x)\\) y \\(f(y)\\). Solución: \\[f(x)=\\int_0^{\\infty}e^{-x}e^{-y} dy=e^{-x}\\] \\[f(y)=\\int_0^{\\infty}e^{-x}e^{-y} dx=e^{-y}\\] Nota: las marginales deben estar en función de su propia variable aleatoria y no contener otras variables, dado que son marginales. Notar que en este ejercicio: \\[f(x,y)=e^{-x}e^{-y}=f(x)*f(y)\\] Esto no siempre sucede, este caso se da cuando las variables son independientes. Ejercicio, Sea la función conjunta \\[f(x,y)=\\frac{1}{500}\\] Para \\(0&lt;X&lt;0.25\\) y \\(0&lt;y&lt;2000\\). Verificar que es una función de probabilidad Encontrar la marginal de \\(x\\) Encontrar la marginal de \\(y\\) \\[\\int_0^{0.25}\\int_0^{2000}\\frac{1}{500}dydx=1\\] \\[f(x)=\\int_{Ry} f(x,y) dy=\\int_0^{2000}\\frac{1}{500}dy=4\\] \\[f(y)=\\int_{Rx} f(x,y) dx=\\int_0^{0.25}\\frac{1}{500}dx=\\frac{1}{2000}\\] Ejercicio Dada la siguiente función de densidad conjunta: \\[f(x,y)=ke^{-2x-3y}; \\quad x,y\\geq 0\\] Encontrar el valor de \\(k\\) para que la función se una función de densidad Encontrar las marginales Verificar si \\(X\\), \\(Y\\) son independientes Calcular las esperanzas y las varianzas a partir de las marginales Solución \\[1=\\int_0^\\infty \\int_0^\\infty ke^{-2x-3y}dx dy=k\\int_0^\\infty e^{-2x} dx \\int_0^\\infty e^{-3y} dy=k*\\frac{1}{2}*\\frac{1}{3}\\] Siendo \\(k=6\\), de esta forma la función queda como: \\[f(x,y)=6e^{-2x-3y}; \\quad x,y\\geq 0\\] Para las marginales: \\[f(x)=\\int_0^\\infty 6e^{-2x-3y}dy=6 e^{-2x} \\left(-\\frac{e^{-3y}}{3} \\right)/_0^\\infty=2e^{-2x}\\] \\[f(y)=\\int_0^\\infty 6e^{-2x-3y}dx=3 e^{-3y}\\] Evidentemente las variables son independientes, ya que: \\[f(x)f(y)=2e^{-2x}3e^{-3y}=6e^{-2x-3y}=f(x,y)\\] Ejemplo, para la distribución conjunta: \\[f(x,y)=\\frac{1}{500}\\] Si \\(X\\) e \\(Y\\) son independientes, se debe cumplir: \\[f(x,y)=f(x)*f(y)=4*\\frac{1}{2000}=\\frac{1}{500}\\] Ejercicio, verificar si la siguiente función esta bien definida y si las variables son independientes. \\[f(x,y)=\\frac{1}{4}(x+y)*xy*e^{-x-y}; \\hspace{2cm} x,y&gt;0\\] Solución, Si esta bien definida, esto significa: \\[\\int_0^{\\infty}\\int_0^{\\infty}\\frac{1}{4}(x+y)*xy*e^{-x-y}=1\\] \\[f(x)=\\int_{Ry}f(x,y)dy=\\frac{x^2+2x}{4} ( e^{-x})\\] Ejercicio, Sea la función conjunta: \\[f(x,y)=k (6-x-y) \\quad 0&lt;x&lt;2, 2&lt;y&lt;4\\] Encontrar el valor de \\(k\\) para que \\(f\\) sea una función de densidad Encontrar las marginales de \\(x\\) e \\(y\\) ¿Son independientes? \\[\\int_{0}^2 \\int_2^4 k (6-x-y) dydx=1\\] Así \\(k=\\frac{1}{8}\\), de esta forma: \\[f(x,y)=\\frac{6-x-y}{8} \\quad 0&lt;x&lt;2, 2&lt;y&lt;4\\] \\[f(x)=\\int_2^4 \\frac{6-x-y}{8} dy=\\frac{3-x}{4}\\] \\[f(y)=\\int_0^2 \\frac{6-x-y}{8} dx=\\frac{5-y}{4}\\] \\[f(x)*f(y)=\\frac{3-x}{4}*\\frac{5-y}{4}\\neq \\frac{6-x-y}{8}\\] Por lo tanto \\(x\\) e \\(y\\) no son independientes. 1.5 Valores esperados En el caso univariado, sea \\(X\\) una variable aleatoria con función de probabilidad \\(\\pi(x)\\) para el caso discreto o \\(f(x)\\) para el caso continuo, el operador matemático esperanza se define como: Para el caso discreto, \\[E[g(X)]=\\sum_{Rx}g(x)P(X=x)\\] Para el caso continuo, \\[E[g(X)]=\\int_{Rx}g(x)f(x)dx\\] Definición 6, el valor esperado para la función \\(g(X,Y)\\), se define como: Para el caso discreto: \\[E[g(X,Y)]=\\sum_{Rx}\\sum_{Ry}g(x,y)\\pi(x,y)\\] Para el caso continuo: \\[E[g(X,Y)]=\\int_{Rx}\\int_{Ry}g(x,y)f(x.y)dydx\\] Nota, hay valores esperados más usuales que otros, Por ejemplo, las varianzas para cada variable \\[E[(X-E[X])^2]=V(X)\\] \\[E[(Y-E[Y])^2]=V(Y)\\] Otras medidas son \\(E[X]\\), \\(E[Y]\\) que son referencias muy similares a un promedio aritmético. Otra valor esperado bastante usado en los casos bivariados es: \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx\\] Encontrar la forma de \\(E[X]\\) usando la definición anterior. \\[E[X]=\\int_{Rx}\\int_{Ry} x f(x,y) dy dx=\\int_{Rx}xf(x) dx\\] \\[E[X]=\\sum_{Rx}\\sum_{Ry} x \\pi(x,y) =\\sum_{Rx}x \\pi(x) dx\\] Ejercicio Sea la función de densidad: \\[f(x,y)=6 e^{-2x-3y}; \\quad x,y \\geq 0\\] Encontrar: Las esperanzas: \\(E[X]\\), \\(E[Y]\\) \\(E[XY]\\) \\(E[XY]-E[X]E[Y]\\) \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx\\] Solución: \\[E[XY]=\\int_0^\\infty \\int_0^\\infty xy 6 e^{-2x-3y}dxdy=6 \\int_0^\\infty x e^{-2x}dx\\int_0^\\infty y e^{-3y}dy= \\alpha \\] Usando la función gamma. \\[t=2x\\quad x=\\frac{t}{2} \\quad \\frac{dx}{dt}=\\frac{1}{2}\\quad \\] \\[\\int_0^\\infty x e^{-2x}dx=\\int_0^\\infty\\frac{t}{2}e^{-t}\\frac{dt}{2}=\\frac{1}{4}\\int_0^\\infty t^{2-1}e^{-t}dt=\\frac{\\Gamma(2)}{4}=\\frac{1!}{4}=\\frac{1}{4}\\] \\[t=3y\\quad y=\\frac{t}{3} \\quad \\frac{dy}{dt}=\\frac{1}{3}\\quad \\] \\[\\int_0^\\infty y e^{-3x}dx=\\int_0^\\infty\\frac{t}{3}e^{-t}\\frac{dt}{3}=\\frac{1}{9}\\int_0^\\infty t^{2-1}e^{-t}dt=\\frac{\\Gamma(2)}{9}=\\frac{1!}{9}=\\frac{1}{9}\\] Retomando \\[\\alpha=6\\frac{1}{4}\\frac{1}{9}=\\frac{1}{6}=E[XY]\\] \\[E[X]=\\int_0^\\infty x 2e^{-2x}dx=2\\int_0^\\infty x e^{-2x}dx=\\frac{2}{4}=\\frac{1}{2}\\] \\[E[X^2]=\\int_0^\\infty x^2 2e^{-2x}dx=2\\int_0^\\infty x^2 e^{-2x}dx=Tarea\\] \\[E[Y]=3*\\frac{1}{9}=\\frac{1}{3}\\] \\[E[Y^2]=tarea\\] Notar que \\[E[X]E[Y]=\\frac{1}{2}\\frac{1}{3}=\\frac{1}{6}=E[XY]\\] \\[E[XY]-E[X]E[Y]=\\frac{1}{6}-\\frac{1}{2}\\frac{1}{3}=0\\] Nota: Cuando dos variables aleatorias son independientes: \\[E[XY]=E[X]E[Y]\\] &gt; Ejercicio, Sea la función conjunta \\[f(x,y)=\\frac{1}{500}, \\quad 0&lt;X&lt;0.25 \\quad 0&lt;y&lt;2000\\] Encontrar: \\(E[X]=1/8\\), \\(V[X]=\\frac{0.25^2}{12}=\\frac{1}{192}\\), \\(E[Y]=1000\\), \\(V[Y]=\\frac{2000^2}{12}=\\frac{1000000}{3}\\) \\(E[XY]\\) \\[E[X]=\\int_0^{0.25}\\int_0^{2000} x \\frac{1}{500}dy dx=\\int_0^{0.25}x\\int_0^{2000} \\frac{1}{500}dy dx=4\\int_0^{0.25}x dx=4*\\frac{(x^2)_0^{0.25}}{2}=\\frac{1}{8}\\] \\[E[XY]=125\\] Nota: Si dos variables aleatorias son independientes: \\[E[XY]=E[X]E[Y]\\] Demostración, \\[E[XY]=\\int_{Rx}\\int_{Ry} xyf(x,y)dydx=\\int_{Rx}\\int_{Ry} xyf(x)f(y)dydx=\\int_{Rx}x f(x)\\left(\\int_{Ry} yf(y)dy\\right)dx=E[X]*E[Y]\\] Para el caso discreto: https://docs.google.com/spreadsheets/d/1yJlHbrmKeJ5z4X0YlLayAxOtCWqG6uqQqIgku-aCXEI/edit?usp=sharing 1.6 Distribuciones condicionales Recordar de estadística I: \\[P(B/A)=\\frac{P(A\\cap B)}{P(A)}\\] Estas distribuciones nos ayudan a entender el comportamiento de una variable, cuando fijamos a otra. Definición, una distribución condicional se define como: Caso discreto, \\[\\pi_{x/y}(X/Y=y)=\\frac{\\pi(x,y)}{\\pi(y)}\\] \\[\\pi_{y/x}(Y/X=x)=\\frac{\\pi(x,y)}{\\pi(x)}\\] Caso continuo, \\[f_{X/Y}(x/y)=\\frac{f(x,y)}{f(y)}\\] \\[f_{Y/X}(y/x)=\\frac{f(x,y)}{f(x)}\\] Estas funciones condicionales cumplen todas las propiedades de una función de probabilidad. Demostrar que: \\[\\int_{Rx} f_{X/Y}(x/y) dx=1\\] \\[\\int_{Rx} f_{X/Y}(x/y) dx=\\int_{Rx} \\frac{f(x,y)}{f(y)} dx=\\frac{1}{f(y)}\\int_{Rx} f(x,y)dx=\\frac{f(y)}{f(y)}=1\\] Que sucede si \\(X\\) e \\(Y\\) son variables independientes: \\[f_{X/Y}(x/y)=\\frac{f(x,y)}{f(y)}=\\frac{f(x)f(y)}{f(y)}=f(x)\\] \\[f_{Y/X}(y/x)=\\frac{f(x,y)}{f(x)}=\\frac{f(x)f(y)}{f(x)}=f(y)\\] Ejemplo, caso discreto (Ver excel compartido) Ejercicio, Sea la densidad conjunta de \\(X,Y\\): \\[f(x,y)=x^2+\\frac{xy}{3}, \\quad 0&lt;x\\leq 1, \\quad 0\\leq y \\leq2\\] Se pide: Verificar que es una función de probabilidad Encontrar las marginales Encontrar las condicionales (\\(X/Y\\), \\(Y/X\\)) Encontrar \\(E[X]\\), \\(E[Y]\\), \\(E[XY]\\), \\(E[X/Y]\\), \\(E[Y/X]\\) ¿Son independientes? \\[\\int_{Rx} \\int_{Ry}f(x,y)dydx=\\int_0^1 \\int_0^2 x^2+\\frac{xy}{3} dy dx=1\\] \\[f(x)=\\int_{Ry}f(x,y)dy=2x^2 +\\frac{2x}{3}\\] \\[f(y)=\\int_{Rx}f(x,y)dx=\\frac{1}{3}+\\frac{y}{6}\\] \\[f_{X/Y}(x/y)=\\frac{x^2+\\frac{xy}{3}}{\\frac{1}{3}+\\frac{y}{6}}=\\frac{x(3x+y)}{1+\\frac{y}{2}}\\] \\[f_{Y/X}(y/x)=\\frac{x^2+\\frac{xy}{3}}{2x^2 +\\frac{2x}{3}}=\\frac{3x^2+xy}{6x^2+2x}\\] \\[E[X]=\\int_0^1 x\\left( 2x^2 +\\frac{2x}{3} \\right)dx=\\frac{13}{18}\\] \\[E[Y]=\\int_0^2 y\\left( \\frac{1}{3}+\\frac{y}{6} \\right)dy=\\frac{10}{9}\\] \\[E[XY]=\\int_{Rx} \\int_{Ry} xy f(x,y)dydx=\\int_0^1 \\int_0^2 xy \\left(x^2+\\frac{xy}{3}\\right) dy dx=\\frac{43}{54}\\] 1.7 Esperanza condicional Se refiere a calcular el valor esperado sobre la función condicional. \\[E[Y/X]=\\int_{Ry} y* f_{Y/X}(y/x)dy\\] \\[E[X/Y]=\\int_{Rx} x* f_{X/Y}(x/y)dx\\] Para el ejercicio anterior, \\[E[Y/X]=\\int_0^2 y* \\left(\\frac{3x^2+xy}{6x^2+2x}\\right) dy=\\frac{9x+4}{9x+3}\\] Calcular: \\[E[Y/X=0.5]=\\frac{9*0.5+4}{9*0.5+3}=\\frac{8.5}{7.5}=1.13\\] \\[E[Y/X=1]=\\frac{9*1+4}{9*1+3}=\\frac{13}{12}=1.083\\] \\[E[Y/X=0.5]=\\frac{17}{15}\\] \\[E[X/Y]=\\frac{9+4y}{12+6y}\\] Nota, si \\(X\\) e \\(Y\\) son independientes, \\[E[X/Y=y]=E[X] \\quad;\\quad E[Y/X=x]=E[Y] \\quad ; \\quad E[XY]=E[X]E[Y]\\] Tarea, demostrar lo anterior. 1.8 Medidas de relación entre dos variables Estas medidas nos ayudan a conocer si dos variables están relacionadas y nos permite saber el tipo de relación (directa, inversa) y también podemos saber la intensidad de la relación. Las medidas son: La Covarianza \\(cov(X,Y)\\) es una medida absoluta de relación: \\[cov(X,Y)=E\\left\\{(X-E[X])(Y-E[Y]) \\right\\}\\] Una alternativa a esta formula (versión corta). \\[cov(X,Y)=E[XY]-E[X]*E[Y]\\] Tarea, demostrar lo anterior. Nota: La covarianza es una medida absoluta, nos ayuda a conocer el tipo de relación entre las variables, pero no es útil para conocer la intensidad de la relación Otra medida importantes es la correlación ($corr(X,Y)=_{xy} $) entre \\(X\\) e \\(Y\\), esta es una medida relativa, que cumple la propiedad: \\(-1 \\leq corr(X,Y) \\leq 1\\), esta se define como: \\[\\rho_{xy}=corr(X,Y)=\\frac{cov(X,Y)}{\\sqrt{V(X)V(Y)}}=\\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}\\] La correlación esta definida en: \\[-1 \\leq \\rho_{xy} \\leq 1\\] Si \\(cov_{xy}\\) o \\(corr_{xy}\\) son distintas de 0, podemos afirmar que existe relación (lineal) Si \\(cov_{xy}&gt;0\\) o \\(corr_{xy}&gt;0\\) la relación entre \\(X\\) e \\(Y\\) es directa Si \\(cov_{xy}&lt;0\\) o \\(corr_{xy}&lt;0\\) la relación entre \\(X\\) e \\(Y\\) es inversa La intensidad de la dirección de la relación nos la da \\(corr_{xy}\\), mientras más cercana a \\(corr_{xy}\\rightarrow 1\\) la relación directa es más fuerte, \\(corr_{xy}\\rightarrow -1\\) la relación inversa es más fuerte Si \\(corr_{xy}\\rightarrow 0\\) podemos decir que las variables no están relacionadas (“cuasi-independencia”) la correlación mide principalmente relaciones lineales. Nota: * Si las variables \\(X,Y\\) son independientes, entonces, siempre se cumple que la covarianza y la correlación es igual a 0. * Si la covarianza o la correlación es igual a 0, eso no implica independencia. Alerta de baja relación o una relación distinta a la lineal. Ejercicio: Sea la función de densidad: \\[f(x,y)=x+y; \\quad 0&lt;x&lt;1 \\quad 0&lt;y&lt;1\\] Encontrar el coeficiente de correlación \\[E[XY]=\\int_0^1 \\int_0^1 xy (x+y)dxdy= \\int_0^1 \\int_0^1 x^2y +xy^2dxdy=\\] \\[=\\int_0^1 \\frac{x^3 y}{3}/_0^1+\\frac{x^2 y^2}{2}/_0^1 dy=\\int_0^1 \\frac{y}{3}+\\frac{y^2}{2}dy=\\frac{y^2}{6}/_0^1+\\frac{y^3}{6}/_0^1=\\frac{1}{3}\\] \\[f(x)=\\int_0^1 (x+y) dy=x*y/_0^1+\\frac{y^2}{2}/_0^1=x+\\frac{1}{2}\\] \\[f(y)=y+\\frac{1}{2}\\] \\[E[X]=\\int_0^1 x^2+\\frac{x}{2}dx=\\frac{1}{3}+\\frac{1}{4}=\\frac{7}{12}=E[Y]\\] \\[cov(X,Y)=E[XY]-E[X]*E[Y]=\\frac{1}{3}-\\frac{7}{12}\\frac{7}{12}=-\\frac{1}{144}\\] \\[E[X^2]=\\int_0^1 x^3+\\frac{x^2}{2}dx=\\frac{1}{4}+\\frac{1}{6}=\\frac{5}{12}=E[Y^2]\\] \\[V(X)=E[X^2]-E[X]^2=\\frac{5}{12}-\\left(\\frac{7}{12}\\right)^2=\\frac{11}{144}=V(Y)\\] \\[\\sigma_X=\\sigma_Y=\\sqrt{V(X)}=\\sqrt{\\frac{11}{144}}\\] \\[\\rho_{xy}=\\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}=\\frac{-\\frac{1}{144}}{\\sqrt{\\frac{11}{144}}\\sqrt{\\frac{11}{144}}}=-\\frac{144}{11*144}=-\\frac{1}{11}=-0.0909\\] Calcular la Covarianza y la correlación de las variables aleatorias \\(X,Y\\). \\[cov(X,Y)=\\frac{43}{54}-\\frac{13}{18}*\\frac{10}{9}=-\\frac{1}{162}\\] &gt; Nota Demostrar que si \\(X\\) y \\(Y\\) son independientes entonces: \\[E[XY]=E[X]*E[Y]\\] Demostración, \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx=\\int_{Rx}\\int_{Ry} xy f(x)f(y) dy dx\\] \\[=\\int_{Rx} x f(x)\\left( \\int_{Ry} y f(y) dy \\right) dx=E[Y] \\int_{Rx} xf(x) dx=E[X]E[Y] \\] Como resultado de lo anterior, si \\(X\\) e \\(Y\\) son independientes: \\[cov(X,Y)=E[XY]-E[X]*E[Y]=E[X]E[Y]-E[X]E[Y]=0\\] Si dos variables son independientes la covarianza y la correlación son iguales a cero, el inverso de esta afirmación no necesariamente es cierta. 1.8.1 Ejemplo Sea la función de densidad: \\[f(x,y)=\\frac{3}{2}(x^2+y^2); \\quad 0&lt;x&lt;1;\\quad 0&lt;y&lt;1\\] Se pide: Verificar que es una función de densidad Encontrar las marginales Encontrar las distribuciones condicionales Calcular la covarianza y correlación Verificar si son independientes Ejercicio Para \\[f(x,y)=\\frac{x(1+3y^2)}{4} \\quad 0&lt;x&lt;2, \\quad 0&lt;y&lt;1\\] Calcular \\[P(1/4&lt;X&lt;1/2 | Y=1/3)\\] Solución Se debe encontrar \\(f_{X|Y}(X|Y)=\\frac{f(x,y)}{f(y)}\\) \\[f(y)=\\int_0^2 \\frac{x(1+3y^2)}{4} dx=\\frac{(1+3y^2)}{4} \\int_0^2 x dx=\\frac{(1+3y^2)}{4} \\frac{x^2}{2}/_0^2=\\] \\[=\\frac{(1+3y^2)}{4}(2)=\\frac{(1+3y^2)}{2}\\] \\[f_{X|Y}(X|Y)=\\frac{\\frac{x(1+3y^2)}{4}}{\\frac{(1+3y^2)}{2}}=\\frac{x}{2}\\] Verificando que sea correcta \\[\\int_0^2 \\frac{x}{2}dx=\\frac{x^2}{4}/_0^2=1\\] \\[P(1/4&lt;X&lt;1/2 | Y=1/3)=\\int_{1/4}^{1/2} f_{X|Y}(X|Y) dx=\\int_{1/4}^{1/2} \\frac{x}{2} dx=\\frac{x^2}{4}/_{0.25}^{0.5}=\\] \\[=\\frac{0.5^2}{4}-\\frac{0.25^2}{4}=\\frac{3}{64}\\] Ejercicio de práctica, Sea la función de densidad: \\[f(x,y)=24xy, \\quad x&gt;0 \\quad y&gt;0 \\quad x+y&lt;1\\] Verificar si es una función de probabilidad Encontrar la función de distribución \\(F\\) Calcular la probabilidad que \\[P(X&gt;0.3 , Y&lt;0.5 )\\] Recordar; \\[F(x,y)=P(X&lt;x,Y&lt;y)\\] Hay que tomar al menos 2 casos: \\(0&lt;x&lt;1\\), \\(x+y&lt;1\\) \\(0&lt;x&lt;1\\), \\(1-x\\leq y\\leq1\\) Para el primer caso: \\[\\int_0^y \\int_0^x 24xy dxdy=6x^2 y^2\\] Para el segundo caso: \\[\\int_0^y \\int_0^{1-y} 24xy dxdy=\\] Ejercicio de práctica, Sea la función de densidad \\[f(x,y)=4 e^{-2(x+y)} \\quad x&gt;0 \\quad y&gt;0\\] * Verificar si es una función de probabilidad * Suponer que estamos interesados en el comportamiento de la variable: \\[Z=\\frac{X}{Y}\\] encontrar la distribución de \\(Z\\). Definimos a \\(z=\\frac{x}{y}\\) y \\(w=x+y\\). Ahora encontramos las funciones \\(G\\) \\[x=\\frac{zw}{1+z}; \\quad y=\\frac{w}{1+z}\\] \\[l(w,z)=f(X=G_1(Z,W),Y=G_2(Z,W))*|J(Z,W)|=4 e^{-2(\\frac{zw}{1+z}+\\frac{w}{1+z})}*\\frac{w}{(1+z)^2}\\] \\[l(z,w)=\\frac{4w e^{-2w}}{(1+z)^2}, \\quad w&gt;0, \\quad z&gt;0\\] finalmente: \\[f(z)=\\int_0^{\\infty} \\frac{4w e^{-2w}}{(1+z)^2} dw=\\frac{1}{(1+z)^2}\\] 1.9 Transformaciones Supongamos que se busca encontrar a partir de la función de densidad \\(f(X,Y)\\) de dos variables X, Y una tercera variable \\(Z=H_1(X,Y)\\). La estrategia para resolver este tipo de problemas es: Definir una cuarta variable \\(W=H_2(X,Y)\\), es elegida por conveniencia Se encuentran \\(X=G_1(Z,W)\\), \\(Y=G_2(Z,W)\\) Una vez con estos valores la función conjunta de \\(Z,W\\) es: \\[l(Z,W)=f(X=G_1(Z,W),Y=G_2(Z,W))*|J(Z,W)|\\] Donde \\(J(Z,W)\\) se llama jacobiano de la transformación y es dado por: \\[J(Z,W)=\\left| \\begin{array} &amp; \\frac{\\partial X }{\\partial Z} &amp; \\frac{\\partial X }{\\partial W} \\\\ \\frac{\\partial Y }{\\partial Z} &amp; \\frac{\\partial Y }{\\partial W} \\end{array} \\right| \\] Ejemplo, Suponer que tenemos una función de densidad conjunto \\(f(x,y)\\), ahora estamos interesados en encontrar la función de densidad de la variable z, definida como: \\[Z=X+Y=H_1(X,Y)\\] \\[Z=XY=H_1(X,Y)\\] Ejemplo Sea la función de densidad conjunta definida como: \\[f(x,y)=4 e^{-2(x+y)} \\quad x&gt;0 \\quad y&gt;0\\] Encontrar la función de densidad de la variable \\(Z=X+Y\\) \\[Z=X+Y=H_1(X,Y); \\quad W=Y=H_2(X,Y)\\] \\[X=Z-W=G_1(Z,W) \\quad Y=W=G_2(Z,W)\\] \\[J(Z,W)=\\left| \\begin{array} &amp; \\frac{\\partial X }{\\partial Z} &amp; \\frac{\\partial X }{\\partial W} \\\\ \\frac{\\partial Y }{\\partial Z} &amp; \\frac{\\partial Y }{\\partial W} \\end{array} \\right|=\\left| \\begin{array} &amp; 1 &amp; -1 \\\\ 0 &amp; 1 \\end{array} \\right|=1 \\] \\[l(z,w)=4 e^{-2(z-w+w)}|1|=4e^{-2z}; z&gt;0,w&gt;0\\] \\[l(z)=\\int_0^\\infty 4e^{-2z}dw=4e^{-2z} w/_0^\\infty=\\] Se debe volver a plantear una nueva forma de \\(W\\) 1.10 Distrubuciones conjuntas de más de 2 variables aleatorias Estamos introduciendo ahora \\(n\\) variables aleatorias, que se puede denotar por vector \\[\\left[X_1,X_2,\\ldots,X_n \\right]\\] Se puede definir su función de densidad conjunta: \\[f(X_1,X_2,\\ldots,X_n)\\] Evidentemente esta función cumple con: \\[\\int_{Rx_1}\\int_{Rx_2}\\ldots \\int_{Rx_n}f(X_1,X_2,\\ldots,X_n)dx_n\\ldots dx_2dx_1=1\\] \\[f(X_1,X_2,\\ldots,X_n)\\geq 0\\] Las marginales de cada variables se obtienen integrando la función de densidad sobre todas las variables aleatorias excepto la variable de interés. \\[f(x_i)=\\int \\ldots\\int_{Rx_{j\\neq i}} f(X_1,X_2,\\ldots,X_n)dx\\ldots dx_{j \\neq i}\\] Notar también que si las \\(n\\) variables aleatorias son independientes entre ellas: \\[f(X_1,X_2,\\ldots,X_n)=f(X_1)*f(X_2)*\\ldots f(X_n)\\] Si estamos interesados en encontrar la covarianza entre dos variables aleatorias de estas \\(n\\) va. \\[cov(X_i,X_j)=E[X_i X_j]-E[X_i]E[X_j] \\quad i \\neq j\\] \\[f(x_i,x_j)=\\int \\ldots\\int_{k\\neq i,j} f(x_1,x_2,\\ldots,x_n)dx_{k\\neq i,j}\\] "],["tema-2-distribuciones-muestrales.html", "2 Tema 2: Distribuciones muestrales 2.1 Muestras y población 2.2 Parámetros, estadísticas y estimadores. 2.3 Distribución muestral 2.4 Propiedades de los estimadores 2.5 Distribución muestral para la media 2.6 Teorema del límite central 2.7 Distribución muestral para la diferencia de medias 2.8 Distribución muestral para la proporción 2.9 Distribución muestral para la varianza 2.10 Distribución \\(\\chi^2\\) 2.11 Distribución t-student 2.12 Distribución Fisher (F)", " 2 Tema 2: Distribuciones muestrales A partir de este tema la estadística esta vinculada con la inferencia sobre los parámetros de la información/datos. 2.1 Muestras y población Definición: Una población es una colección “completa” de objetos, estos objetos tienen variables. Sea nuestra población \\(U\\), esta población puede ser finita o infinita \\[U=\\{u_1, u_2, \\ldots , u_i,...,u_N \\}\\] \\[U=\\{u_1, u_2, \\ldots , u_i,... \\}\\] Cada elemento de \\(U\\) tiene variables o características asociadas: \\[u_i=\\{X_{i1}, X_{i2}, \\ldots, X_{iP} \\}\\] \\[u_j=\\{X_{j1}, X_{j2}, \\ldots, X_{jP} \\}\\] Definición, Muestra: Una muestra es un subconjunto de U. \\[s \\subset U ,\\quad s \\in U \\quad\\] Donde \\(s\\) representa al conjunto denominado “muestra” Normalmente una muestra tiene un tamaño \\(n\\) (puede ser fijo y aleatorio), el mecanismo para obtener la muestra de \\(U\\) puede ser con reposición o sin reposición, en cualquier caso podemos anotar esto de la siguiente forma, sea \\(s\\) una muestra: \\[s=\\{u_{1}^*,u_2^*, \\ldots, u_n^*\\}\\] En un caso extremo para el muestreo con reposición \\[u_i^*=u_j^* \\quad i,j=1, \\ldots ,n\\] Note que los elementos \\(u_1\\) y \\(u_1^*\\) no necesariamente son los mismos. Ejemplo de un tamaño de muestra aleatorio: Se tiene una población de 53 elementos, para decidir quien es parte de la muestra se lanza un dado para cada elemento. Si el dado sale 1 el elemento es parte de la muestra, si sale cualquier otro número, ele elemento no es parte de la muestra. Una característica ideal al momento de obtener la muestra, es que todos los elementos de la población tengan alguna probabilidad de ser parte de la muestra. (Esto depende del esquema de selección) El subconjunto \\(s\\) no es único y en realidad existen muchas muestras posibles, según el contexto, esto depende: Del tamaño de \\(N\\), \\(n\\) Del mecanismo s/rep, c/rep. Problema: Sea el conjunto \\(U={a,b,c,d,e,f,g,h,i,j}\\), se pide calcular para ambos mecanismos de selección, la cantidad de muestras posibles; cuando \\(n=3\\), \\(n=4\\) y \\(n=7\\). Sin reposición: \\(n=3\\), 120 \\(n=4\\), 210 \\(n=7\\), 120 Con reposición: \\(n=3\\), \\(1000\\) \\(n=4\\), \\(10^4\\) \\(n=7\\), \\(10^7\\) Ejemplo, Sea la población \\(U=\\{a,b,c,d,e,f\\}\\), se define una muestra de \\(n=3\\), escriba todas las muestras posibles según ambos mecanismos de reposición. Solución, (s/rep), 20: \\(s_1=\\{a,b,c\\}\\), \\(s_2=\\{a,b,d \\}\\), \\(\\ldots\\) ,\\(s_{20}=\\{d,e,f\\}\\) (c/rep), 216: \\(s_1=\\{a,a,a\\}\\), \\(s_2=\\{a,a,b\\}\\), \\(\\ldots\\), \\(s_{216}=\\{f,f,f\\}\\) N&lt;-6;n&lt;-3 choose(N,n) ## [1] 20 U&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;) combn(U,n) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; ## [2,] &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; &quot;d&quot; &quot;d&quot; ## [3,] &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;e&quot; &quot;f&quot; ## [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## [1,] &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; ## [2,] &quot;e&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; &quot;d&quot; &quot;d&quot; &quot;e&quot; ## [3,] &quot;f&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;e&quot; &quot;f&quot; &quot;f&quot; ## [,17] [,18] [,19] [,20] ## [1,] &quot;c&quot; &quot;c&quot; &quot;c&quot; &quot;d&quot; ## [2,] &quot;d&quot; &quot;d&quot; &quot;e&quot; &quot;e&quot; ## [3,] &quot;e&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; N^n ## [1] 216 En una población de 58 estudiantes, si se define una muestra de 15 estudiantes, según ambos mecanismos de selección ¿Cuántas muestras se pueden armar? (s/r) 2.9752626^{13} (c/r) 2.8276126^{26} Doscientos ochenta y dos cuatrillones setecientos sesenta y un mil doscientos cincuenta y seis trillones ochocientos ochenta y un mil doscientos noventa y siete billones trescientos tres mil setecientos seis millones seiscientos sesenta y seis mil cuatrocientos cuatro En la carrera de informática de la UMSA se planea realizar una encuesta de opinión con una muestra de 200 estudiantes, según ambos mecanismos de selección ¿Cuántas muestras se pueden armar? (Suponer que \\(N=2500\\)) s/r: 13750504940386732907426482044088242840008688608422088462248622008668200400608248460880626448288804800424086882008020260604628484282664848664888482402462868880466408484286482882842486466444284028422444028868608862606880888224440880420680442862806886006626280828468462800008284000620266020286044062620664 c/r, \\(2500^{200}\\) Sin reposición: \\[Muestras_{Posibles}=\\binom{N}{n}=\\frac{N!}{(N-n)!n!}\\] Con reposición (el orden importa): \\[Muestras_{Posibles}=N^n\\] Imaginemos a la primera variable de interés \\(X_1\\), para el universo esta variable tiene los elementos: \\[X_1=\\{X_{11}, X_{21}, X_{31}, \\ldots, X_{N1}\\}\\] Imaginemos que observamos a \\(X_1\\), para la muestra. \\[X_1^*=\\{X_{11}^*, X_{21}^*, X_{31}^*, \\ldots, X_{n1}^* \\}\\] Estos \\(X_{i1}^*\\) para los \\(i=1,\\ldots,n\\) son variables aleatorias. Por lo tanto \\(X_1^*\\) es un vector aleatorio de tamaño \\(n\\). De ahora en adelante vamos a trabajar con un solo vector aleatorio denominado \\(X\\), de tal forma que este sea la colección de \\(n\\) variables aleatorias. \\[X=\\{X_1,X_2,\\ldots,X_n \\}\\] Definición. La colección del vector aleatorio \\(X=\\{X_1,X_2,\\ldots,X_n \\}\\), son independientes e idénticamente distribuidas (iid) si la distribución conjunta de las \\(n\\) variables puede ser escrita como: \\[f(x_1,x_2,\\ldots,x_n)=f(x_1)*f(x_2)*\\ldots*f(x_n)\\] y además todas las \\(x_i\\) tienen la misma función de distribución \\(F(x)\\). Nota: El criterio iid se cumple si: Se asume un muestreo con reposición (el orden importa) Se asume un muestreo sin reposición pero la muestra tiende al infinito Definición Sea \\(N\\) el tamaño de la población y \\(n\\) el tamaño de la muestra, ambos valores para fines de este capítulo son constantes. 2.2 Parámetros, estadísticas y estimadores. El objetivo de la estadística es aprender acerca de las características de una población. Estas características las vamos a llamar parámetros. Definición, Un parámetro \\(\\theta\\) es una función sobre la población \\(U\\). \\[\\theta=f(U,X,Y,Z,\\ldots)\\] Nota: Los parámetros de una población son constantes. Los parámetros más usuales son, el total, la media, la proporción, la varianza, las razones. Ejemplo, Sea el universo los 10 primeros números naturales y sus valores. \\(Y=\\{1,2,3,4,5,6,7,8,9,10\\}\\). Sobre estos valores de esta población de \\(N=10\\) se pueden calcular los siguientes parámetros. Total \\[\\theta_1=t_y=\\sum_U y_i=55\\] Media \\[\\theta_2=\\mu_y=\\frac{t_y}{N}=\\frac{55}{10}=5.5\\] Máximo: \\(\\theta_3=max(y)=10\\) Mínimo: \\(\\theta_4=min(y)=1\\) Proporción \\[P_{pares}=\\frac{5}{10}=0.5\\] Es posibles hacer transformaciones sobre \\(Y\\), sea \\(Z\\) una variables binaria que identifique a los números primos de \\(Y\\); \\(1=primo\\), \\(0=\\sim primo\\). \\[Z=\\{1,1,1,0,1,0,1,0,0,0 \\}\\] Calcular el promedio de \\(Z\\) \\[\\theta_5=\\mu_z=\\frac{5}{10}=0.5\\] Cuando obtenemos la media de un vector binario, obtenemos lo que se denomina una proporción de la característica que define el valor de 1. \\[\\theta_5=P_a=\\frac{\\#A}{N}\\] \\[\\theta_5=P_{primos}=\\frac{\\#primos}{N}\\] Diferencias de medias: asume que tenemos a 2 poblaciones de interés, de tamaño \\(N_1\\) y \\(N_2\\) \\[\\theta_6=\\mu_1-\\mu_2\\] Diferencias de proporciones: asume que tenemos a 2 poblaciones de interés, de tamaño \\(N_1\\) y \\(N_2\\) \\[\\theta_7=P_1-P_2\\] Definición, estadística Se denomina estadística a una función sobre la muestra. Definición, estimador Un estimador \\(\\hat{\\theta}\\) para el parámetro \\(\\theta\\) es una estadística que busca aproximar/adivinar el valor de \\(\\theta\\) \\[Aleatorio\\quad:f(s,X)=\\hat{\\theta} \\rightarrow \\theta=f(U,X): \\quad fijo\\] Notar que el estimador es una variable aleatoria, ya que depende de las distribuciones muestrales. Cuando se aplica la función \\(f(s,X)=\\hat{\\theta}\\) sobre una única muestra seleccionada, se tiene la estimación. Ejemplo Usando los datos recolectados de un grupo de 44 (\\(N=44\\)) estudiantes respecto sus horas de sueño: 8, 9, 6, 8, 8, 7, 6, 2, 7, 7, 7, 6, 7, 8, 8, 5, 7, 7, 5, 8, 4, 6, 8, 8, 8, 7, 7, 6, 4, 8, 8, 6, 8, 4, 10, 7, 6, 7, 6, 8, 5, 6, 8, 6 Se calculó el total de horas de sueño: \\[\\theta=t_{sueño}=297\\] Supongamos que no tenemos conocimiento del 297, pero sí tenemos acceso a una muestra de 8 estudiantes de estos 44, a los cuales se les pregunto sus horas de sueño, las respuestas dadas fueron: 7, 8, 6, 5, 8, 8, 7, 6. x&lt;-c(8, 9, 6, 8, 8, 7, 6, 2, 7, 7, 7, 6, 7, 8, 8, 5, 7, 7, 5, 8, 4, 6, 8, 8, 8, 7, 7, 6, 4, 8, 8, 6, 8, 4, 10, 7, 6, 7, 6, 8, 5, 6, 8, 6) set.seed(999) s&lt;-sample(x,8) set.seed(9999) s2&lt;-sample(x,8) Proponer algún estimador (función) que use estos 8 datos de la muestra y que aproxime el valor del parámetro. \\[\\hat{\\theta}_1=\\bar{x}_{mod}*\\bar{x}_{med}*min(x)\\] \\[\\hat{\\theta}_2=min(x)*\\sum_s x_i+\\frac{max(x)*min(x)}{2}\\] \\[\\hat{\\theta}_3=max(x)*min(x)^2+97\\] \\[\\hat{\\theta}_4= min(x)*\\sum_s x_i+\\frac{\\sum_s x_i}{max(x)-min(x)}\\] \\[\\hat{\\theta}_5=\\frac{\\sum_s x_i}{n}*N\\] Veamos las estimaciones para cada estimador con la muestra selecciona. \\[\\hat{\\theta}_1=280\\] \\[\\hat{\\theta}_2=295\\] \\[\\hat{\\theta}_3=297\\] \\[\\hat{\\theta}_4=293.33\\] \\[\\hat{\\theta}_5=302.5\\] Ahora imaginemos que tenemos una segunda muestra: 6, 7, 5, 6, 6, 7, 9, 2. Aplicar sobre esta muestra los 5 estimadores antes definidos. \\(\\hat{\\theta}_1=72\\) \\(\\hat{\\theta}_2=105\\) \\(\\hat{\\theta}_3=133\\) \\(\\hat{\\theta}_4=102.9\\) \\(\\hat{\\theta}_5=264\\) ¿Cómo evaluamos la calidad de un estimador? Es importante conocer las propiedades asociadas a los estimadores, es decir como se comportan con las distintas muestras posibles. sample(x,8) ## [1] 8 7 6 10 7 8 5 7 tt4&lt;-function(y){ yy&lt;-min(y)*sum(y)+(sum(y)/(max(y)-min(y))) return(yy) } tt5&lt;-function(y){ yy&lt;-mean(y)*44 return(yy) } k&lt;-1000 theta4&lt;-NULL theta5&lt;-NULL for(i in 1:k){ ss&lt;-sample(x,8) print(ss) theta4[i]&lt;-tt4(ss) theta5[i]&lt;-tt5(ss) } ## [1] 7 8 5 8 8 6 4 8 ## [1] 7 8 5 6 6 10 8 8 ## [1] 8 7 10 8 6 6 6 7 ## [1] 6 10 8 9 6 6 2 4 ## [1] 5 8 6 7 6 6 8 5 ## [1] 8 6 6 8 8 8 6 6 ## [1] 5 5 8 6 6 7 6 7 ## [1] 8 6 6 7 4 8 2 8 ## [1] 5 8 4 8 4 7 7 6 ## [1] 6 6 8 6 6 7 5 6 ## [1] 6 10 7 9 6 8 7 7 ## [1] 8 7 6 8 8 7 6 4 ## [1] 2 8 8 8 6 6 4 8 ## [1] 8 4 6 7 7 6 7 8 ## [1] 7 8 6 6 6 2 6 5 ## [1] 8 6 8 6 8 8 6 8 ## [1] 7 8 7 8 6 7 6 6 ## [1] 8 7 7 7 6 8 6 10 ## [1] 8 7 6 4 6 8 4 7 ## [1] 5 6 9 7 7 2 4 8 ## [1] 4 7 4 7 6 6 7 5 ## [1] 6 4 4 7 8 6 2 6 ## [1] 6 6 8 8 5 6 5 8 ## [1] 7 8 6 7 6 7 4 6 ## [1] 7 7 9 6 8 7 8 8 ## [1] 8 7 6 8 5 7 4 4 ## [1] 6 8 7 4 5 2 7 6 ## [1] 5 4 10 7 7 8 6 7 ## [1] 8 4 2 6 7 6 6 8 ## [1] 4 6 8 6 4 8 5 8 ## [1] 9 7 8 8 8 8 4 8 ## [1] 8 6 4 8 6 4 7 5 ## [1] 10 7 5 4 8 7 4 7 ## [1] 2 6 6 8 4 8 7 8 ## [1] 7 8 5 7 8 6 8 6 ## [1] 8 6 4 7 6 8 6 7 ## [1] 4 8 8 2 6 7 7 7 ## [1] 2 6 8 8 6 8 6 7 ## [1] 8 8 7 8 8 8 8 6 ## [1] 8 7 5 6 8 2 8 7 ## [1] 8 4 8 8 8 6 10 8 ## [1] 5 7 6 8 7 6 2 8 ## [1] 4 8 7 7 6 7 8 8 ## [1] 7 8 7 6 8 6 6 7 ## [1] 7 8 7 7 6 7 6 6 ## [1] 8 5 8 8 6 6 8 6 ## [1] 9 5 6 8 7 7 7 8 ## [1] 8 7 4 7 6 7 8 6 ## [1] 7 4 8 6 8 7 8 6 ## [1] 6 4 6 7 8 7 6 8 ## [1] 7 2 6 8 6 7 8 6 ## [1] 7 7 7 6 4 6 5 6 ## [1] 8 7 8 8 7 4 7 10 ## [1] 8 8 9 5 6 2 8 4 ## [1] 8 7 6 7 8 5 8 6 ## [1] 8 6 7 6 7 8 4 6 ## [1] 8 5 9 8 8 7 6 7 ## [1] 8 6 6 7 7 8 2 7 ## [1] 10 7 8 7 7 7 6 7 ## [1] 6 7 6 6 7 7 6 6 ## [1] 8 7 8 7 10 9 8 8 ## [1] 8 10 6 6 2 5 6 8 ## [1] 6 8 7 8 6 7 4 5 ## [1] 7 6 6 4 6 8 8 9 ## [1] 6 8 8 6 4 7 9 4 ## [1] 7 8 8 7 4 8 8 5 ## [1] 7 7 6 8 6 8 5 8 ## [1] 7 6 7 8 7 9 10 8 ## [1] 8 2 6 7 8 7 8 7 ## [1] 8 7 5 6 8 7 8 6 ## [1] 7 5 6 7 6 8 6 8 ## [1] 7 8 6 7 2 7 8 10 ## [1] 7 8 6 6 7 6 8 6 ## [1] 6 8 7 6 8 7 7 8 ## [1] 8 8 6 7 7 7 8 7 ## [1] 6 5 6 8 6 7 8 7 ## [1] 8 8 8 8 8 6 8 6 ## [1] 8 4 7 5 8 8 8 8 ## [1] 7 7 8 6 7 4 8 6 ## [1] 6 2 9 6 7 6 8 6 ## [1] 7 8 8 5 8 4 8 10 ## [1] 4 7 5 7 9 7 8 8 ## [1] 7 8 8 8 6 6 8 4 ## [1] 5 7 7 6 7 6 7 7 ## [1] 6 8 5 7 8 6 4 8 ## [1] 7 2 6 8 8 8 6 10 ## [1] 6 6 7 8 4 7 8 6 ## [1] 2 7 8 8 6 6 6 4 ## [1] 8 7 8 8 6 6 5 9 ## [1] 6 10 8 6 4 8 7 8 ## [1] 9 7 8 8 10 8 6 4 ## [1] 8 8 7 7 8 7 8 6 ## [1] 6 6 8 8 6 8 8 8 ## [1] 6 8 5 8 7 7 6 8 ## [1] 8 8 6 4 6 8 8 7 ## [1] 8 8 7 8 6 8 6 6 ## [1] 8 4 6 2 8 8 8 7 ## [1] 4 5 6 8 8 5 6 8 ## [1] 8 8 6 8 8 8 8 6 ## [1] 6 8 6 5 7 6 7 8 ## [1] 4 7 6 8 8 7 6 7 ## [1] 8 5 6 6 8 8 6 6 ## [1] 7 6 5 8 7 8 7 7 ## [1] 8 6 6 10 8 8 6 7 ## [1] 8 8 10 8 7 7 7 5 ## [1] 6 6 7 8 4 6 7 5 ## [1] 6 8 7 7 5 8 8 6 ## [1] 6 7 7 6 8 7 2 7 ## [1] 8 7 8 6 7 7 8 10 ## [1] 8 7 6 7 7 7 6 4 ## [1] 4 9 7 6 4 6 4 7 ## [1] 7 8 7 6 8 6 6 5 ## [1] 10 8 8 8 8 4 2 7 ## [1] 6 7 6 10 7 6 8 4 ## [1] 6 7 8 6 6 8 8 7 ## [1] 6 6 8 8 7 4 7 7 ## [1] 8 8 6 8 6 8 7 8 ## [1] 8 7 2 6 5 8 6 4 ## [1] 7 8 6 6 7 6 6 9 ## [1] 6 6 5 8 7 6 6 8 ## [1] 6 6 5 9 7 8 6 8 ## [1] 6 8 7 7 7 7 8 8 ## [1] 7 8 8 10 7 4 6 8 ## [1] 8 7 10 8 8 7 8 7 ## [1] 4 6 8 8 7 6 6 7 ## [1] 2 6 7 8 7 9 8 6 ## [1] 8 5 6 5 8 8 7 7 ## [1] 7 4 7 5 7 6 7 5 ## [1] 7 8 5 8 4 7 6 8 ## [1] 7 8 2 5 8 6 7 8 ## [1] 5 8 5 6 4 6 8 6 ## [1] 6 8 6 5 6 7 5 10 ## [1] 8 8 8 7 6 6 8 2 ## [1] 6 5 7 4 7 8 8 8 ## [1] 8 8 6 7 8 5 7 6 ## [1] 8 6 8 8 6 7 2 6 ## [1] 5 8 6 4 8 9 6 4 ## [1] 7 7 5 6 8 8 7 8 ## [1] 7 7 8 6 6 5 8 7 ## [1] 4 6 7 7 8 9 7 6 ## [1] 8 5 8 7 8 2 8 4 ## [1] 7 6 10 8 8 7 6 8 ## [1] 8 7 8 4 6 2 6 4 ## [1] 6 6 9 4 8 7 6 6 ## [1] 8 8 7 7 8 6 6 6 ## [1] 7 2 10 6 8 8 6 5 ## [1] 8 8 6 8 6 7 4 8 ## [1] 5 8 6 8 7 6 6 8 ## [1] 7 5 6 8 7 8 8 6 ## [1] 7 8 4 7 8 7 6 8 ## [1] 6 7 8 2 8 5 8 7 ## [1] 6 6 7 6 8 8 8 5 ## [1] 6 7 7 8 6 7 8 9 ## [1] 7 6 8 8 7 8 8 8 ## [1] 6 7 6 8 7 8 8 9 ## [1] 6 8 7 8 7 8 7 5 ## [1] 4 8 5 4 8 6 8 6 ## [1] 6 7 7 7 7 5 6 8 ## [1] 8 7 8 8 6 7 7 8 ## [1] 6 8 8 7 5 8 7 8 ## [1] 2 6 7 7 7 5 8 6 ## [1] 10 6 8 6 7 8 6 8 ## [1] 8 7 8 10 7 6 8 8 ## [1] 8 8 8 8 7 4 8 7 ## [1] 5 6 8 8 7 6 8 6 ## [1] 4 6 8 8 9 8 7 5 ## [1] 7 6 4 8 6 7 8 8 ## [1] 7 8 4 5 8 6 6 6 ## [1] 8 5 8 4 8 7 7 6 ## [1] 8 8 7 5 8 8 6 2 ## [1] 4 8 8 8 6 8 7 6 ## [1] 6 7 8 8 6 5 2 6 ## [1] 8 5 10 5 7 6 6 7 ## [1] 7 7 6 7 4 8 8 8 ## [1] 7 6 6 4 4 6 8 6 ## [1] 6 8 10 8 6 6 8 5 ## [1] 5 6 6 6 10 7 6 7 ## [1] 8 6 8 5 6 8 7 4 ## [1] 7 5 7 6 2 5 7 7 ## [1] 8 7 8 7 7 6 7 6 ## [1] 8 8 10 7 7 4 7 4 ## [1] 6 5 7 6 6 8 5 6 ## [1] 8 7 6 5 7 2 7 9 ## [1] 6 8 7 6 6 6 8 6 ## [1] 6 4 6 8 7 6 5 6 ## [1] 7 7 4 7 8 6 8 5 ## [1] 8 8 6 8 6 8 8 8 ## [1] 7 6 10 8 8 7 7 8 ## [1] 6 6 7 8 8 8 7 8 ## [1] 7 8 2 5 5 8 8 8 ## [1] 8 6 2 7 8 8 6 7 ## [1] 6 8 6 4 7 6 6 7 ## [1] 6 2 9 4 8 7 8 4 ## [1] 8 8 6 7 7 8 5 8 ## [1] 7 8 6 7 7 8 7 8 ## [1] 8 5 8 6 6 9 6 4 ## [1] 8 8 6 9 8 8 8 7 ## [1] 6 6 5 8 4 8 7 8 ## [1] 6 8 9 7 7 8 8 7 ## [1] 7 10 8 8 6 6 6 8 ## [1] 4 8 8 7 6 7 4 5 ## [1] 7 7 7 7 8 8 4 8 ## [1] 8 6 8 8 8 6 7 7 ## [1] 8 8 8 8 7 10 8 8 ## [1] 8 8 8 7 6 7 8 2 ## [1] 8 6 8 5 8 7 7 7 ## [1] 8 8 6 7 8 6 5 6 ## [1] 7 6 6 7 7 4 7 6 ## [1] 10 5 8 6 7 6 7 8 ## [1] 8 9 8 6 8 6 8 8 ## [1] 8 5 7 10 6 7 7 7 ## [1] 4 8 10 8 9 7 7 7 ## [1] 4 5 7 8 6 2 8 7 ## [1] 6 7 8 9 6 8 2 8 ## [1] 10 4 6 8 8 6 8 8 ## [1] 5 8 5 8 7 7 6 8 ## [1] 4 5 7 7 8 4 4 8 ## [1] 4 7 6 8 6 7 7 6 ## [1] 8 5 6 6 7 8 9 7 ## [1] 8 6 5 7 8 7 8 9 ## [1] 7 4 7 7 7 8 8 7 ## [1] 8 6 6 8 8 7 8 8 ## [1] 8 7 7 6 6 8 8 7 ## [1] 10 8 6 6 8 7 6 8 ## [1] 6 8 7 6 7 4 8 6 ## [1] 6 7 6 7 7 8 6 8 ## [1] 6 4 7 9 7 8 7 6 ## [1] 8 8 8 6 7 8 4 9 ## [1] 8 7 4 5 8 8 8 2 ## [1] 6 8 8 6 7 7 6 8 ## [1] 8 7 8 7 7 7 6 4 ## [1] 7 8 6 8 8 8 8 7 ## [1] 7 7 6 7 7 5 6 7 ## [1] 7 6 7 8 6 7 8 10 ## [1] 6 6 7 10 7 6 8 8 ## [1] 8 8 7 7 5 8 8 8 ## [1] 8 6 7 8 6 8 6 6 ## [1] 7 8 7 7 7 8 8 9 ## [1] 6 4 7 8 7 8 6 7 ## [1] 4 2 6 9 7 8 7 6 ## [1] 8 5 7 7 7 6 4 6 ## [1] 7 6 6 8 8 8 7 8 ## [1] 8 8 8 6 6 8 8 7 ## [1] 2 8 8 5 9 7 6 8 ## [1] 5 4 6 9 8 6 7 8 ## [1] 8 8 8 8 8 8 7 4 ## [1] 7 9 5 8 6 2 5 7 ## [1] 7 6 7 7 7 8 6 8 ## [1] 8 9 7 5 8 8 7 8 ## [1] 7 8 4 7 5 6 8 8 ## [1] 4 6 7 6 6 8 8 8 ## [1] 5 8 8 7 8 8 8 8 ## [1] 6 8 8 6 7 8 6 2 ## [1] 4 8 2 7 7 8 8 5 ## [1] 7 5 8 8 7 8 4 8 ## [1] 6 7 5 7 8 6 6 7 ## [1] 4 9 7 7 8 4 5 7 ## [1] 5 8 8 7 8 8 6 6 ## [1] 6 6 8 7 8 4 8 7 ## [1] 5 7 6 5 8 8 7 7 ## [1] 8 6 7 7 7 6 8 6 ## [1] 6 7 8 7 8 8 8 6 ## [1] 8 8 8 6 5 8 10 7 ## [1] 7 10 8 7 6 5 8 8 ## [1] 5 8 8 6 2 7 6 6 ## [1] 7 7 8 5 8 6 8 6 ## [1] 6 7 6 9 4 6 5 6 ## [1] 8 8 9 4 10 7 5 6 ## [1] 6 8 5 4 6 7 6 7 ## [1] 8 8 2 8 7 8 5 8 ## [1] 8 7 4 8 2 7 6 4 ## [1] 8 8 9 10 6 6 7 6 ## [1] 8 9 7 5 6 10 8 7 ## [1] 7 4 6 7 6 8 8 8 ## [1] 8 4 7 6 7 5 8 6 ## [1] 9 6 4 6 8 5 2 7 ## [1] 7 6 7 6 8 7 8 5 ## [1] 5 8 6 7 7 8 6 6 ## [1] 7 8 6 4 8 8 6 5 ## [1] 6 6 5 10 8 7 6 4 ## [1] 2 7 6 7 5 8 8 4 ## [1] 6 7 6 5 6 7 8 7 ## [1] 6 7 7 9 2 10 7 4 ## [1] 8 4 8 6 6 7 8 6 ## [1] 7 6 2 6 6 7 6 7 ## [1] 8 6 4 7 6 4 6 8 ## [1] 4 8 8 6 8 5 9 6 ## [1] 8 7 7 6 6 5 4 7 ## [1] 7 8 5 4 6 10 4 5 ## [1] 4 2 8 7 8 5 7 6 ## [1] 7 8 4 6 5 8 8 7 ## [1] 4 9 5 7 7 6 8 10 ## [1] 6 6 7 4 5 4 8 9 ## [1] 8 4 8 7 6 2 6 8 ## [1] 6 2 5 10 4 6 9 8 ## [1] 6 6 8 8 6 7 7 5 ## [1] 7 6 7 5 8 10 4 7 ## [1] 8 6 7 8 6 6 6 8 ## [1] 7 6 6 4 7 9 8 4 ## [1] 10 6 6 5 6 5 8 8 ## [1] 8 6 9 8 7 7 8 6 ## [1] 6 6 6 8 7 7 5 8 ## [1] 8 7 8 5 7 8 8 6 ## [1] 7 7 8 8 7 8 5 9 ## [1] 6 7 8 8 7 6 6 6 ## [1] 8 8 10 7 6 6 8 7 ## [1] 6 7 7 6 5 8 4 7 ## [1] 5 8 8 5 7 10 8 6 ## [1] 6 7 7 8 6 8 7 8 ## [1] 7 8 6 8 6 8 8 8 ## [1] 7 8 6 5 7 6 7 9 ## [1] 7 6 8 7 7 7 7 8 ## [1] 8 8 7 8 7 6 8 6 ## [1] 5 7 8 4 8 8 6 8 ## [1] 7 8 7 8 6 8 8 6 ## [1] 7 5 8 10 7 7 6 8 ## [1] 8 7 7 5 4 6 8 2 ## [1] 10 7 6 7 5 8 8 7 ## [1] 8 7 7 8 6 8 4 8 ## [1] 8 8 6 4 7 2 7 8 ## [1] 7 8 6 8 7 6 7 8 ## [1] 5 2 7 7 6 7 8 7 ## [1] 2 8 8 7 8 6 8 8 ## [1] 7 6 6 8 6 8 4 8 ## [1] 9 6 10 8 7 8 6 6 ## [1] 8 8 7 7 6 6 8 8 ## [1] 7 8 8 6 6 2 4 10 ## [1] 6 10 8 5 8 6 6 6 ## [1] 6 6 8 6 7 7 6 7 ## [1] 7 8 8 6 6 7 7 7 ## [1] 6 4 7 7 8 7 8 6 ## [1] 10 4 6 7 8 6 5 7 ## [1] 7 6 6 6 5 7 9 5 ## [1] 7 7 6 8 6 7 7 7 ## [1] 9 7 6 8 8 6 5 8 ## [1] 8 6 8 7 7 8 7 4 ## [1] 7 7 6 6 4 6 6 8 ## [1] 6 7 5 4 7 8 6 2 ## [1] 7 2 8 5 7 7 6 7 ## [1] 4 8 2 7 4 5 8 6 ## [1] 6 6 7 5 8 4 4 7 ## [1] 6 7 4 6 9 8 8 6 ## [1] 8 8 6 7 6 7 8 7 ## [1] 7 6 7 6 8 8 7 6 ## [1] 6 8 8 8 8 7 8 6 ## [1] 8 6 8 7 4 7 7 8 ## [1] 7 8 5 8 8 4 7 8 ## [1] 8 8 7 6 2 7 7 6 ## [1] 8 8 8 8 5 8 6 5 ## [1] 4 7 5 8 2 5 6 4 ## [1] 7 6 8 6 6 6 4 9 ## [1] 8 8 6 7 8 7 8 7 ## [1] 8 8 7 8 7 8 5 7 ## [1] 5 5 8 4 6 8 8 8 ## [1] 6 8 6 6 8 6 4 6 ## [1] 6 6 7 8 6 7 8 8 ## [1] 8 8 4 7 8 7 7 6 ## [1] 7 6 6 7 5 8 6 8 ## [1] 8 2 8 4 6 8 7 6 ## [1] 7 7 7 8 8 8 6 7 ## [1] 7 7 7 4 8 8 9 5 ## [1] 8 4 9 5 6 6 6 8 ## [1] 6 8 10 8 6 2 5 8 ## [1] 6 8 8 8 5 10 7 8 ## [1] 10 7 8 6 7 4 8 8 ## [1] 6 7 8 8 8 8 2 7 ## [1] 7 8 8 7 8 6 5 8 ## [1] 5 6 8 7 8 6 7 4 ## [1] 6 7 8 8 6 4 7 8 ## [1] 8 6 5 8 8 6 5 8 ## [1] 6 7 8 7 7 7 7 7 ## [1] 6 8 7 8 8 4 6 4 ## [1] 8 7 7 7 7 8 7 7 ## [1] 7 4 4 8 6 7 9 6 ## [1] 6 7 8 7 7 7 10 8 ## [1] 7 8 6 7 7 7 6 4 ## [1] 6 8 10 7 7 6 7 8 ## [1] 6 8 7 6 6 8 4 5 ## [1] 5 6 5 6 7 8 8 6 ## [1] 6 8 6 7 8 5 7 4 ## [1] 7 5 8 8 4 5 2 5 ## [1] 5 8 8 9 6 7 8 8 ## [1] 8 6 6 7 7 8 6 5 ## [1] 7 8 8 8 10 6 6 5 ## [1] 7 7 8 8 7 6 6 6 ## [1] 7 5 7 8 8 7 8 6 ## [1] 7 10 7 5 6 4 4 7 ## [1] 8 4 8 7 7 8 6 8 ## [1] 6 6 7 9 5 8 10 6 ## [1] 7 7 8 8 4 7 6 5 ## [1] 8 7 7 8 7 8 5 9 ## [1] 5 6 6 8 8 8 8 6 ## [1] 8 7 8 7 8 7 4 7 ## [1] 7 5 4 7 7 7 8 6 ## [1] 6 8 7 6 7 5 7 8 ## [1] 6 5 7 7 2 8 8 6 ## [1] 7 7 6 8 9 6 8 7 ## [1] 8 6 6 2 4 7 8 7 ## [1] 6 8 6 7 5 6 4 7 ## [1] 5 5 6 6 6 8 10 6 ## [1] 7 8 6 4 4 6 7 7 ## [1] 7 7 6 6 6 8 8 8 ## [1] 7 8 5 7 4 7 8 7 ## [1] 7 8 8 6 8 5 6 7 ## [1] 5 8 6 6 8 7 7 4 ## [1] 7 8 6 7 6 8 8 6 ## [1] 7 5 8 7 2 6 8 8 ## [1] 5 9 6 7 7 8 8 6 ## [1] 8 6 8 6 8 10 8 7 ## [1] 7 8 7 7 8 5 9 6 ## [1] 7 8 4 6 10 6 8 8 ## [1] 8 7 7 7 10 7 9 8 ## [1] 6 8 7 7 2 8 8 8 ## [1] 8 6 5 8 4 7 7 7 ## [1] 8 7 6 7 10 7 6 9 ## [1] 6 8 8 7 6 7 6 7 ## [1] 8 10 8 6 7 8 8 7 ## [1] 6 6 7 8 2 8 5 7 ## [1] 7 4 8 6 8 8 8 6 ## [1] 8 8 6 5 7 7 7 6 ## [1] 5 6 8 7 7 8 8 2 ## [1] 8 8 4 8 5 8 6 5 ## [1] 8 6 7 7 8 8 7 8 ## [1] 8 8 6 8 7 6 7 5 ## [1] 10 4 8 9 4 6 6 7 ## [1] 7 6 8 6 7 8 8 7 ## [1] 6 8 7 8 8 10 4 7 ## [1] 6 8 6 5 6 6 7 6 ## [1] 6 7 8 7 6 8 6 4 ## [1] 8 6 10 7 4 7 6 8 ## [1] 5 8 7 8 7 8 7 6 ## [1] 8 8 8 8 9 5 8 7 ## [1] 7 8 7 5 5 2 7 7 ## [1] 4 6 6 7 8 7 5 7 ## [1] 7 8 6 7 8 6 7 6 ## [1] 6 6 6 6 7 7 8 9 ## [1] 8 6 5 2 6 8 6 8 ## [1] 4 7 8 8 5 5 7 8 ## [1] 8 8 7 6 7 4 7 6 ## [1] 8 7 5 7 8 7 4 8 ## [1] 8 8 7 7 6 8 6 7 ## [1] 8 4 7 7 8 7 7 8 ## [1] 6 7 6 7 6 5 8 7 ## [1] 4 6 9 8 5 5 8 8 ## [1] 7 8 10 7 6 8 7 8 ## [1] 6 8 8 2 7 6 4 8 ## [1] 7 8 2 7 8 8 6 8 ## [1] 5 4 5 8 8 7 6 7 ## [1] 8 8 8 2 8 7 7 4 ## [1] 6 6 8 5 7 7 6 7 ## [1] 8 7 4 7 7 7 8 8 ## [1] 6 8 4 8 8 6 6 6 ## [1] 5 7 4 8 5 5 7 8 ## [1] 2 6 8 6 6 8 8 5 ## [1] 5 6 6 5 7 8 8 6 ## [1] 6 5 6 7 8 8 5 6 ## [1] 6 5 6 9 6 7 8 4 ## [1] 5 7 8 7 8 8 5 7 ## [1] 7 7 8 8 10 8 4 8 ## [1] 8 2 6 8 7 4 8 8 ## [1] 7 8 8 9 6 8 8 6 ## [1] 8 7 7 7 8 7 5 7 ## [1] 7 7 7 6 8 6 6 7 ## [1] 6 2 8 8 6 6 10 8 ## [1] 6 6 5 6 7 7 8 6 ## [1] 6 8 9 8 6 4 8 8 ## [1] 6 5 7 4 5 6 7 7 ## [1] 8 5 8 6 7 2 8 4 ## [1] 8 6 6 7 4 8 7 6 ## [1] 8 7 8 6 8 8 7 8 ## [1] 4 9 8 8 8 5 7 5 ## [1] 8 8 7 7 5 6 6 7 ## [1] 6 7 6 4 6 4 7 8 ## [1] 8 8 6 8 5 4 2 7 ## [1] 2 4 5 8 5 6 8 7 ## [1] 6 7 8 10 7 8 8 8 ## [1] 9 8 2 6 4 6 6 5 ## [1] 6 7 10 8 8 8 6 8 ## [1] 6 7 8 8 6 7 8 10 ## [1] 8 5 8 8 7 9 7 6 ## [1] 5 9 7 7 7 8 6 6 ## [1] 7 8 6 8 8 7 5 8 ## [1] 7 6 5 7 4 9 6 6 ## [1] 7 5 6 6 2 8 7 8 ## [1] 10 6 8 5 8 8 8 8 ## [1] 8 6 8 8 6 6 2 7 ## [1] 4 6 8 2 6 7 8 6 ## [1] 7 6 6 7 8 8 7 8 ## [1] 4 8 4 8 7 8 7 7 ## [1] 8 6 4 8 7 6 6 8 ## [1] 8 7 7 6 7 4 5 10 ## [1] 5 7 6 8 4 8 7 7 ## [1] 6 8 8 8 8 8 6 9 ## [1] 8 8 9 7 6 5 8 7 ## [1] 10 5 7 7 8 7 8 7 ## [1] 4 6 6 7 7 4 8 6 ## [1] 7 7 8 8 8 8 7 6 ## [1] 8 6 7 8 6 7 8 6 ## [1] 7 7 8 7 8 2 6 8 ## [1] 6 8 7 7 7 8 8 8 ## [1] 6 7 6 8 9 6 7 5 ## [1] 8 7 4 6 5 8 6 6 ## [1] 6 8 8 4 7 6 2 7 ## [1] 6 8 4 6 6 8 6 6 ## [1] 8 5 2 8 6 8 8 6 ## [1] 6 7 8 8 8 8 6 10 ## [1] 5 7 7 10 6 2 8 4 ## [1] 7 6 8 8 7 7 8 8 ## [1] 9 6 4 7 7 2 4 8 ## [1] 8 6 6 6 8 6 8 6 ## [1] 8 6 7 6 7 8 7 5 ## [1] 4 9 8 8 6 5 7 8 ## [1] 8 7 6 7 6 7 6 8 ## [1] 8 6 4 8 8 4 7 8 ## [1] 6 10 7 6 4 8 6 6 ## [1] 6 8 8 8 7 8 8 5 ## [1] 7 6 7 8 7 2 6 8 ## [1] 7 8 8 7 4 8 6 6 ## [1] 6 5 7 5 8 7 8 8 ## [1] 8 7 8 8 2 8 7 8 ## [1] 8 8 6 7 6 7 6 6 ## [1] 4 7 10 6 7 7 8 4 ## [1] 8 8 4 8 8 7 7 8 ## [1] 7 7 4 8 6 8 8 8 ## [1] 8 7 7 10 6 5 7 8 ## [1] 8 6 8 8 4 7 8 5 ## [1] 7 8 7 7 8 6 4 5 ## [1] 8 8 8 7 8 8 6 5 ## [1] 7 8 7 8 9 10 8 2 ## [1] 7 8 4 6 8 8 8 6 ## [1] 6 7 8 7 5 8 7 7 ## [1] 8 6 7 8 6 4 7 6 ## [1] 4 6 7 7 8 5 7 7 ## [1] 8 8 6 7 6 8 6 6 ## [1] 6 8 9 7 8 8 6 4 ## [1] 5 5 5 8 6 9 7 7 ## [1] 2 8 8 8 6 6 6 4 ## [1] 6 5 7 6 6 7 7 6 ## [1] 6 8 7 6 7 8 7 6 ## [1] 4 7 7 2 7 10 5 8 ## [1] 7 4 8 2 7 8 8 6 ## [1] 10 7 8 8 6 5 4 8 ## [1] 6 6 7 8 9 6 8 7 ## [1] 7 5 7 8 5 6 7 8 ## [1] 7 8 8 7 7 6 7 2 ## [1] 8 7 6 5 6 8 7 4 ## [1] 8 6 7 6 6 7 7 8 ## [1] 8 7 6 4 6 5 7 8 ## [1] 9 8 8 5 4 6 6 8 ## [1] 7 4 8 8 7 5 8 10 ## [1] 6 5 4 8 8 8 10 8 ## [1] 6 8 8 6 8 2 9 4 ## [1] 8 6 10 7 7 8 8 6 ## [1] 7 8 8 8 8 7 6 8 ## [1] 7 8 8 8 8 6 6 8 ## [1] 6 7 6 8 8 8 8 8 ## [1] 8 10 9 7 7 2 8 6 ## [1] 2 6 8 7 8 5 6 6 ## [1] 8 4 7 8 7 7 6 8 ## [1] 8 8 7 8 5 7 8 6 ## [1] 8 8 7 7 2 7 6 7 ## [1] 6 8 8 8 7 7 6 5 ## [1] 4 7 6 7 7 8 6 6 ## [1] 5 9 8 4 8 6 6 2 ## [1] 6 7 6 8 6 7 8 8 ## [1] 6 7 9 7 8 8 2 6 ## [1] 2 5 7 7 8 4 7 7 ## [1] 8 8 8 7 8 2 5 10 ## [1] 6 7 10 7 8 7 5 8 ## [1] 4 8 6 5 6 6 7 8 ## [1] 8 8 6 8 7 6 6 6 ## [1] 6 8 8 7 5 8 8 7 ## [1] 7 7 8 7 2 5 6 6 ## [1] 7 6 6 5 7 6 6 7 ## [1] 8 8 7 9 6 7 7 6 ## [1] 6 4 7 6 8 4 8 5 ## [1] 5 8 7 10 6 8 6 4 ## [1] 5 7 8 7 2 8 6 8 ## [1] 8 6 7 8 7 4 8 8 ## [1] 6 8 6 10 4 6 2 7 ## [1] 8 8 4 7 7 7 9 8 ## [1] 8 6 2 9 7 8 8 7 ## [1] 6 6 6 4 5 7 7 8 ## [1] 6 7 5 10 7 7 8 6 ## [1] 8 8 7 6 5 9 6 6 ## [1] 7 6 8 8 7 8 8 8 ## [1] 8 6 6 7 4 8 5 9 ## [1] 7 6 8 8 8 5 7 7 ## [1] 6 8 6 7 4 5 8 8 ## [1] 6 7 8 7 6 7 6 7 ## [1] 10 5 4 8 5 6 8 8 ## [1] 6 7 6 6 7 10 8 8 ## [1] 7 5 8 6 6 8 6 9 ## [1] 5 7 8 8 6 6 9 7 ## [1] 7 5 8 7 6 6 5 8 ## [1] 6 6 9 8 8 5 7 7 ## [1] 7 6 8 7 8 7 5 2 ## [1] 8 7 8 6 4 6 5 6 ## [1] 6 6 9 6 8 6 7 8 ## [1] 7 6 6 8 6 7 4 8 ## [1] 8 2 4 8 7 6 8 7 ## [1] 6 2 7 9 8 8 6 4 ## [1] 8 7 8 7 7 8 5 6 ## [1] 4 5 10 8 6 6 7 2 ## [1] 8 7 8 7 9 6 8 6 ## [1] 7 8 7 7 6 6 7 5 ## [1] 7 7 7 7 10 4 6 8 ## [1] 5 5 4 7 8 6 8 8 ## [1] 6 10 7 7 5 6 6 8 ## [1] 7 8 7 8 7 8 4 7 ## [1] 8 9 5 6 8 8 8 6 ## [1] 6 7 8 7 8 8 6 8 ## [1] 6 7 7 4 7 8 10 9 ## [1] 7 7 8 8 7 5 5 7 ## [1] 7 8 2 7 7 5 6 8 ## [1] 7 8 8 8 10 8 6 6 ## [1] 4 6 8 8 5 8 8 8 ## [1] 8 6 7 6 7 6 5 8 ## [1] 8 7 8 6 8 7 4 5 ## [1] 4 5 9 6 6 6 6 8 ## [1] 6 8 6 6 8 8 8 7 ## [1] 6 8 6 6 7 5 5 7 ## [1] 6 6 7 7 7 2 7 7 ## [1] 10 8 6 7 6 8 8 8 ## [1] 8 7 6 8 4 8 8 5 ## [1] 8 6 7 8 4 6 8 8 ## [1] 6 4 8 8 7 8 8 6 ## [1] 6 10 5 8 6 7 8 4 ## [1] 4 6 8 8 7 8 7 8 ## [1] 8 8 6 8 6 9 7 4 ## [1] 6 9 6 7 7 4 6 7 ## [1] 5 8 6 6 4 8 4 2 ## [1] 8 7 6 8 8 8 2 6 ## [1] 6 6 8 7 6 8 2 8 ## [1] 6 6 7 8 6 7 8 7 ## [1] 8 5 8 7 6 8 6 8 ## [1] 6 8 8 5 2 6 8 7 ## [1] 7 6 8 8 6 2 6 5 ## [1] 7 4 7 6 7 10 8 8 ## [1] 6 10 5 2 8 7 6 6 ## [1] 8 6 9 4 2 4 6 8 ## [1] 6 6 8 7 6 7 8 8 ## [1] 4 8 6 5 6 10 6 8 ## [1] 5 6 8 6 7 4 7 5 ## [1] 7 8 6 4 8 6 6 7 ## [1] 6 7 4 8 7 7 4 4 ## [1] 8 7 7 6 7 7 5 6 ## [1] 7 6 4 8 7 8 8 7 ## [1] 8 7 7 8 8 7 4 7 ## [1] 7 7 7 8 8 8 6 6 ## [1] 6 6 5 7 6 7 2 8 ## [1] 7 8 6 8 7 9 8 2 ## [1] 8 6 5 7 8 6 7 6 ## [1] 7 6 6 8 6 7 5 8 ## [1] 6 6 8 6 8 6 4 8 ## [1] 8 7 7 7 8 8 5 6 ## [1] 8 9 6 8 6 7 6 8 ## [1] 5 6 7 4 2 8 7 10 ## [1] 7 8 8 8 6 8 7 7 ## [1] 7 6 8 10 7 6 4 8 ## [1] 5 7 6 6 5 7 8 6 ## [1] 8 7 7 6 7 6 8 6 ## [1] 7 8 8 8 6 8 5 8 ## [1] 8 7 8 8 8 8 7 7 ## [1] 5 8 6 6 7 8 6 6 ## [1] 8 2 8 5 7 6 8 6 ## [1] 8 7 8 6 9 5 8 8 ## [1] 8 6 6 6 6 5 6 5 ## [1] 6 6 8 7 7 8 7 8 ## [1] 7 2 8 7 8 7 5 6 ## [1] 6 7 4 7 6 2 6 6 ## [1] 7 8 6 7 9 7 6 7 ## [1] 6 8 5 8 7 7 6 6 ## [1] 7 5 8 8 4 6 6 8 ## [1] 7 6 7 7 4 8 5 8 ## [1] 8 8 7 7 8 8 8 8 ## [1] 7 6 9 8 8 8 6 8 ## [1] 8 6 6 8 4 7 4 2 ## [1] 5 8 7 6 6 7 7 8 ## [1] 10 5 6 6 5 7 6 2 ## [1] 8 6 9 8 6 6 7 7 ## [1] 7 7 9 7 7 4 6 7 ## [1] 8 7 9 4 8 6 5 5 ## [1] 6 9 7 8 8 8 6 7 ## [1] 7 8 6 7 8 5 8 4 ## [1] 7 6 4 6 7 7 8 7 ## [1] 6 7 7 6 5 8 7 5 ## [1] 7 8 7 8 6 7 8 7 ## [1] 8 9 6 4 6 6 8 6 ## [1] 5 6 7 8 9 8 2 10 ## [1] 8 2 7 6 4 6 8 6 ## [1] 6 7 8 8 5 6 8 6 ## [1] 8 8 8 8 7 6 7 7 ## [1] 4 4 6 6 7 6 5 8 ## [1] 6 8 6 5 7 8 6 7 ## [1] 6 10 7 7 8 7 8 8 ## [1] 8 6 5 6 6 8 4 8 ## [1] 7 4 6 6 6 5 5 5 ## [1] 8 8 4 7 6 7 8 6 ## [1] 6 6 8 8 6 5 7 8 ## [1] 6 8 8 7 7 7 5 8 ## [1] 6 8 5 6 6 8 8 7 ## [1] 10 8 8 7 8 8 6 7 ## [1] 8 7 7 7 8 7 7 6 ## [1] 4 8 8 8 7 7 6 8 ## [1] 7 5 6 2 6 7 7 4 ## [1] 6 6 9 7 8 8 7 8 ## [1] 9 4 7 8 5 8 6 7 ## [1] 8 8 6 8 7 9 5 7 ## [1] 5 6 7 7 8 5 7 7 ## [1] 7 7 6 7 8 2 6 8 ## [1] 8 5 7 5 7 8 6 7 ## [1] 8 6 8 2 6 7 8 8 ## [1] 5 4 6 6 6 7 8 7 ## [1] 8 7 7 9 8 6 6 6 ## [1] 6 8 7 6 8 7 8 5 ## [1] 7 4 8 4 8 10 4 7 ## [1] 8 8 6 8 2 8 6 8 ## [1] 5 10 2 8 6 7 4 7 ## [1] 7 6 7 5 4 4 5 8 ## [1] 8 7 10 8 6 7 8 4 ## [1] 4 6 8 8 6 8 7 6 ## [1] 8 7 6 5 8 6 10 8 ## [1] 6 4 8 7 8 7 8 5 ## [1] 7 6 4 5 8 8 7 8 ## [1] 6 6 8 7 2 4 8 6 ## [1] 4 6 7 8 4 8 2 8 ## [1] 8 4 7 6 5 8 4 6 ## [1] 7 6 8 6 10 6 4 7 ## [1] 8 6 10 8 8 8 2 8 ## [1] 4 6 7 6 8 5 5 7 ## [1] 2 6 5 7 7 5 7 8 ## [1] 8 8 5 6 7 7 6 7 ## [1] 8 7 5 8 8 4 6 7 ## [1] 4 7 6 8 6 8 8 8 ## [1] 6 5 6 7 7 6 8 7 ## [1] 8 7 4 7 6 7 10 4 ## [1] 6 8 7 7 9 8 8 8 ## [1] 8 6 4 8 7 7 8 6 ## [1] 7 6 8 5 6 7 5 8 ## [1] 6 8 8 8 8 8 2 6 ## [1] 7 6 6 8 8 5 10 6 ## [1] 4 6 5 8 7 2 6 8 ## [1] 8 8 7 6 4 6 8 8 ## [1] 7 2 10 4 9 7 7 7 ## [1] 8 7 7 2 5 10 8 7 ## [1] 6 6 8 8 8 8 6 5 ## [1] 7 8 7 8 4 8 9 2 ## [1] 7 4 8 6 5 6 8 7 ## [1] 8 8 6 6 6 7 8 5 ## [1] 9 6 8 7 8 8 6 6 ## [1] 8 5 10 8 5 6 4 9 ## [1] 8 8 6 6 8 6 4 6 ## [1] 6 8 8 2 6 7 5 6 ## [1] 8 7 6 10 8 8 7 6 ## [1] 4 7 8 8 8 6 8 10 ## [1] 2 8 7 7 6 8 5 8 ## [1] 4 8 7 9 8 7 8 2 ## [1] 8 7 7 7 5 8 6 7 ## [1] 7 8 6 6 4 8 9 6 ## [1] 6 6 5 8 6 8 7 7 ## [1] 7 4 7 8 7 8 2 8 ## [1] 6 7 8 10 8 8 8 8 ## [1] 7 6 7 8 8 6 8 2 ## [1] 4 2 8 7 6 6 9 8 ## [1] 8 8 8 6 8 8 8 8 ## [1] 6 8 8 6 6 7 8 8 ## [1] 8 7 6 8 8 8 8 8 ## [1] 8 8 5 7 6 7 7 8 ## [1] 8 6 7 5 6 8 8 8 ## [1] 6 6 6 7 8 8 8 8 ## [1] 6 7 2 6 6 6 6 7 ## [1] 6 10 8 8 7 8 8 8 ## [1] 10 5 7 7 8 6 8 2 ## [1] 5 8 8 7 6 7 8 7 ## [1] 8 6 8 9 4 6 4 2 ## [1] 7 5 7 8 10 8 7 4 ## [1] 8 7 7 9 8 8 6 6 ## [1] 6 9 5 8 8 8 8 4 ## [1] 8 5 6 8 8 7 7 8 ## [1] 8 7 5 5 8 7 8 8 ## [1] 7 8 6 8 7 7 7 6 ## [1] 7 6 6 8 7 8 8 6 ## [1] 6 5 8 7 5 8 6 6 ## [1] 4 6 4 5 6 10 8 7 ## [1] 8 6 8 8 7 6 4 5 ## [1] 5 8 8 7 8 4 8 8 ## [1] 8 7 7 7 6 8 8 4 ## [1] 6 6 8 6 7 7 6 6 ## [1] 7 5 6 8 8 7 9 6 ## [1] 7 10 5 6 8 7 7 7 ## [1] 8 8 6 5 8 5 7 8 ## [1] 6 7 8 8 8 4 9 8 ## [1] 8 8 4 4 7 6 7 6 ## [1] 5 8 5 5 6 10 6 6 ## [1] 8 8 5 7 6 10 8 7 ## [1] 8 8 7 8 6 7 8 4 ## [1] 6 5 6 8 7 4 6 8 ## [1] 8 6 5 9 6 7 7 8 ## [1] 7 7 5 5 2 6 7 4 ## [1] 7 6 8 8 7 7 6 8 ## [1] 8 8 8 6 6 6 7 6 ## [1] 7 8 6 6 8 8 6 4 ## [1] 8 8 10 5 7 6 4 7 ## [1] 6 4 6 8 5 5 8 8 ## [1] 7 4 8 8 7 6 7 6 ## [1] 5 9 7 7 7 4 7 7 ## [1] 7 4 7 7 6 8 8 4 ## [1] 7 8 8 6 7 7 5 7 ## [1] 6 7 8 6 7 4 7 6 ## [1] 8 4 9 7 7 8 5 7 ## [1] 7 8 5 10 7 7 5 2 ## [1] 6 7 9 8 4 6 5 8 ## [1] 6 8 2 8 8 6 5 7 ## [1] 7 8 9 8 6 8 6 7 ## [1] 6 8 6 5 7 8 7 5 ## [1] 6 8 7 7 6 8 8 6 ## [1] 8 8 8 6 6 7 5 9 ## [1] 6 6 6 4 8 8 6 10 ## [1] 9 6 7 7 7 6 8 8 ## [1] 4 8 7 8 6 2 8 7 ## [1] 4 6 6 7 8 6 8 9 ## [1] 7 8 6 7 7 4 6 9 ## [1] 8 8 6 8 7 6 7 6 ## [1] 6 7 8 8 6 2 7 8 ## [1] 8 6 8 7 5 7 7 8 ## [1] 6 4 8 10 8 8 9 7 ## [1] 7 10 7 7 7 6 8 7 ## [1] 6 8 6 6 4 8 8 7 ## [1] 8 7 8 8 5 10 7 8 ## [1] 6 4 7 8 8 8 4 7 ## [1] 8 6 5 6 5 8 7 8 ## [1] 7 8 7 5 7 4 8 8 ## [1] 8 6 8 8 8 8 2 6 ## [1] 8 7 5 7 8 8 7 4 ## [1] 7 8 8 6 8 8 7 10 ## [1] 4 8 8 5 7 7 8 7 ## [1] 7 7 8 8 5 8 4 6 ## [1] 7 5 2 8 8 4 6 7 ## [1] 8 6 6 6 8 8 7 6 ## [1] 8 8 8 7 7 6 5 8 ## [1] 6 9 8 5 7 6 8 5 ## [1] 6 6 7 8 10 5 8 8 ## [1] 7 8 6 2 8 4 6 6 ## [1] 6 6 4 6 7 7 7 8 ## [1] 9 5 8 7 5 6 10 7 ## [1] 7 8 8 7 7 8 8 4 ## [1] 6 8 8 8 7 6 4 7 ## [1] 7 8 8 8 4 7 4 8 ## [1] 7 8 2 8 8 8 8 9 ## [1] 7 4 6 8 7 8 7 6 ## [1] 7 8 7 5 7 5 8 6 ## [1] 8 6 4 8 10 8 7 6 ## [1] 8 8 8 7 6 4 4 8 ## [1] 8 8 8 6 6 8 7 6 ## [1] 5 7 8 8 4 9 6 7 ## [1] 7 8 9 7 6 8 5 6 ## [1] 8 2 7 7 8 6 8 9 ## [1] 8 6 7 6 5 8 2 6 ## [1] 4 5 6 6 6 8 4 7 ## [1] 8 4 8 7 5 7 8 5 ## [1] 7 6 8 8 8 7 5 8 ## [1] 6 8 8 7 5 6 7 7 ## [1] 7 6 8 8 2 8 7 7 ## [1] 7 9 7 7 8 4 7 8 ## [1] 8 6 10 8 8 2 8 8 ## [1] 5 7 7 6 8 8 6 7 ## [1] 8 7 8 7 8 6 5 5 ## [1] 8 8 5 7 4 7 10 8 ## [1] 8 7 7 7 5 8 6 6 ## [1] 8 6 7 8 7 6 8 8 ## [1] 8 5 8 7 8 8 6 2 ## [1] 8 7 8 6 8 4 2 6 ## [1] 8 6 6 8 6 8 8 8 ## [1] 4 7 6 7 7 7 8 8 ## [1] 7 5 7 7 8 8 6 10 ## [1] 9 7 8 5 7 8 6 8 ## [1] 6 6 6 8 7 7 5 8 ## [1] 6 4 6 8 6 8 6 8 ## [1] 6 8 8 7 7 9 6 4 ## [1] 7 7 7 4 8 10 7 6 ## [1] 5 6 6 8 4 5 6 6 ## [1] 6 7 6 5 8 6 8 7 ## [1] 6 7 8 4 8 6 7 8 ## [1] 8 5 6 7 8 6 8 6 ## [1] 2 4 7 6 8 8 6 7 ## [1] 6 7 7 6 5 8 8 9 ## [1] 6 6 6 5 6 6 6 8 ## [1] 8 5 8 8 8 6 4 7 ## [1] 7 7 4 8 8 7 7 8 ## [1] 6 7 8 9 7 7 8 4 ## [1] 7 7 8 6 6 8 4 6 ## [1] 8 7 6 5 8 7 8 5 ## [1] 7 6 4 5 6 8 8 4 ## [1] 7 6 4 8 6 7 8 5 ## [1] 8 8 6 6 5 7 5 8 ## [1] 7 6 7 6 6 7 8 7 ## [1] 6 5 7 6 7 8 8 2 ## [1] 5 2 6 7 8 8 7 7 ## [1] 6 6 2 5 7 8 4 8 ## [1] 5 8 5 8 7 7 6 8 ## [1] 8 9 8 7 8 8 8 6 ## [1] 7 4 6 6 8 6 8 7 ## [1] 8 8 6 6 8 8 7 5 ## [1] 8 8 8 2 6 7 5 5 ## [1] 6 8 4 7 8 7 6 8 ## [1] 7 7 8 8 8 6 8 5 ## [1] 6 9 7 10 7 8 6 7 ## [1] 8 5 8 8 7 6 6 8 ## [1] 8 8 8 6 6 8 4 5 ## [1] 7 8 7 6 7 8 6 7 ## [1] 8 8 4 7 7 8 6 8 ## [1] 6 8 5 7 7 7 4 6 ## [1] 7 7 4 7 4 7 6 8 ## [1] 8 5 4 9 8 8 8 6 ## [1] 7 6 8 8 7 8 7 8 ## [1] 6 7 7 6 6 8 6 8 ## [1] 9 6 8 2 7 8 6 7 ## [1] 8 8 9 8 6 6 5 6 ## [1] 8 8 5 7 8 6 7 2 ## [1] 6 9 7 8 7 4 8 6 ## [1] 8 8 4 6 6 8 6 5 ## [1] 6 6 6 8 8 8 7 5 ## [1] 10 8 7 5 5 8 6 6 ## [1] 2 8 8 6 6 9 8 6 ## [1] 8 8 7 9 8 4 7 6 ## [1] 7 8 7 7 8 8 6 6 ## [1] 6 7 8 8 6 8 6 7 ## [1] 7 6 6 7 6 9 2 8 ## [1] 8 8 6 6 8 6 8 6 ## [1] 7 6 8 8 8 7 6 7 ## [1] 7 10 5 8 7 4 7 6 ## [1] 8 7 8 5 5 7 8 7 ## [1] 5 6 6 4 8 7 7 7 ## [1] 6 6 8 6 7 7 6 5 ## [1] 6 8 8 4 8 8 6 6 ## [1] 6 5 7 8 8 7 4 4 ## [1] 7 6 6 8 8 5 7 8 ## [1] 8 8 8 6 7 8 4 8 ## [1] 9 8 5 8 7 2 6 8 ## [1] 7 8 7 4 5 6 8 7 ## [1] 8 8 7 7 10 5 6 9 ## [1] 4 4 8 8 6 6 2 6 ## [1] 6 8 6 8 8 7 2 7 ## [1] 8 8 8 8 9 6 8 7 ## [1] 8 8 7 7 6 7 7 5 ## [1] 7 8 7 7 7 10 6 6 ## [1] 8 6 2 7 8 7 10 4 ## [1] 6 7 5 6 7 8 8 10 ## [1] 10 8 7 8 8 4 7 8 ## [1] 8 8 8 7 8 5 7 7 ## [1] 2 7 6 8 6 5 5 8 ## [1] 8 6 4 7 7 6 8 8 ## [1] 6 8 9 6 7 7 7 6 ## [1] 8 7 7 8 8 6 7 7 ## [1] 8 6 6 4 7 6 6 7 ## [1] 8 7 8 8 4 4 9 7 ## [1] 7 6 7 5 8 10 4 8 ## [1] 8 7 5 4 9 5 6 6 ## [1] 8 7 8 7 7 7 9 7 ## [1] 7 6 9 6 7 6 4 8 ## [1] 8 6 5 9 8 8 6 8 ## [1] 6 4 7 8 8 7 7 8 ## [1] 8 8 8 8 7 4 6 8 ## [1] 2 6 8 8 7 6 7 8 ## [1] 7 8 6 10 8 7 6 7 ## [1] 7 8 7 6 7 7 7 4 ## [1] 7 6 8 2 8 6 4 7 ## [1] 8 7 8 8 8 10 7 7 ## [1] 8 8 7 6 7 8 6 8 ## [1] 8 4 8 7 7 6 8 6 ## [1] 8 5 8 2 5 7 4 5 ## [1] 8 6 6 6 6 6 8 6 ## [1] 8 7 7 10 4 8 9 4 ## [1] 10 7 5 6 6 6 8 4 ## [1] 8 8 5 8 6 6 7 6 ## [1] 7 6 8 5 7 8 7 8 ## [1] 6 5 7 5 6 8 8 7 ## [1] 6 7 7 4 4 8 8 8 ## [1] 7 8 7 8 7 8 5 8 ## [1] 7 6 10 7 8 4 8 6 ## [1] 10 8 7 4 8 8 8 6 ## [1] 6 8 5 7 6 8 6 7 ## [1] 8 6 4 6 8 6 8 7 ## [1] 6 9 8 8 8 10 4 7 ## [1] 4 8 6 7 6 6 5 7 ## [1] 6 6 8 8 7 6 8 7 ## [1] 10 6 7 6 8 6 2 8 ## [1] 6 4 8 7 6 7 5 7 ## [1] 7 4 6 8 6 7 7 8 ## [1] 8 8 5 8 6 2 8 6 ## [1] 2 7 7 8 4 8 8 6 ## [1] 6 8 7 7 8 6 7 8 ## [1] 8 8 7 4 6 6 8 8 ## [1] 6 8 4 8 8 8 8 6 ## [1] 8 4 5 8 8 6 7 7 ## [1] 6 8 7 8 8 6 6 6 ## [1] 7 5 7 8 7 6 7 8 ## [1] 7 5 6 8 8 10 4 7 ## [1] 7 8 8 6 7 8 6 8 hist(theta4) hist(theta5) par(mfrow=c(1,2)) plot(density(theta4)) abline(v=297,col=&quot;red&quot;) plot(density(theta5)) abline(v=297,col=&quot;red&quot;) dev.off() ## null device ## 1 Ejemplo Imaginemos una población de 10 estudiantes sobre la cuales nos interesa conocer el gasto diario en pasajes a la universidad (pasaje ida y vuelta desde su casa al monoblock), para ello se planea tomar una muestra de 3 estudiantes. El objetivo de este estudio es conocer el gasto total de estos 10 estudiantes. Sea \\(y_i\\) la variable gasto es pasajes del estudiante \\(i\\). \\[\\theta=t_y=\\sum_U y_i=\\sum_{i=1}^{10} y_i\\] N&lt;-10 y&lt;-c(5.2,6,8,10,3,4,7,4,4,10.5) ty&lt;-sum(y)#parámetro Ahora vamos a obtener una muestra de tamaño 3 de esta población. (s/rep) n&lt;-3 choose(10,3)#muestra posibles ## [1] 120 set.seed(1439)#semilla ys&lt;-sample(y,n) ys ## [1] 10.5 3.0 5.2 \\[\\hat{\\theta}_1=\\hat{t}_y=\\frac{\\prod_s y_i}{n}=54.6\\] \\[\\hat{\\theta}_2=N*\\frac{\\sum_s y_i}{n}=62.3\\] \\[\\hat{\\theta}_3=y_{min}+\\prod_{s(-y_{min})}y_i=57.6\\] \\[\\hat{\\theta}_4=n\\sum_s y_i=56.1\\] Nota, el estimador hace referencia a la función matemática, mientras que una estimación es una evaluación de esa función matemática usando la muestra seleccionada. s&lt;-combn(y,n)#muestras posibles #theta1 et1&lt;-apply(s,2,prod)/n #theta2 et2&lt;-N*apply(s, 2,mean) #theta4 et4&lt;-n*apply(s, 2,sum) par(mfrow=c(2,2)) plot(density(et1),&quot;theta1&quot;,xlim=c(0,150)) abline(v=ty,col=&quot;red&quot;) plot(density(et2),&quot;theta2&quot;,xlim=c(0,150)) abline(v=ty,col=&quot;red&quot;) plot(density(et4),&quot;theta4&quot;,xlim=c(0,150)) abline(v=ty,col=&quot;red&quot;) dev.off() ## null device ## 1 e1&lt;-sum(et1*(1/120)) e2&lt;-sum(et2*(1/120)) e4&lt;-sum(et4*(1/120)) sum(((et1-e1)^2)*(1/120)) ## [1] 2348.766 sum(((et2-e2)^2)*(1/120)) ## [1] 162.2989 sum(((et4-e4)^2)*(1/120)) ## [1] 131.4621 \\[E[X]=\\sum_{Rx}x*P(X=x)\\] \\[E[\\hat{\\theta}]=\\theta\\] 2.3 Distribución muestral Recordar que una estadística es una función sobre la muestra y sobre los valores que toman las variables aleatorias vinculadas a esta. Como la estadística es una función sobre las muestras aleatorias (muestras posibles) las evaluaciones que se realizan para cada una de las muestras posibles (estimaciones) conforman lo que vamos a denominar una distribución muestral. Por ejemplo si planteamos al estimador del parámetro del total, recordar: \\[\\theta=t_y=\\sum_U y_i\\] Un estimador para este parámetro será: \\[\\hat{\\theta}=\\hat{t}_y=\\frac{N}{n} \\sum_s y_i=N \\bar{y}\\] Este \\(\\hat{\\theta}\\) es una estadística sobre las muestras aleatorias, por lo tanto podemos decir que existe una distribución de probabilidad para este estimador, a esa distribución de probabilidad se conoce como distribución muestral. Ejemplo práctico. Supongamos que de una población de 6 personas tenemos la información de sus ingresos mensuales. \\[Y_{Ingresos}=\\{2000,3000,3500,0,6000,4500\\}\\] Supongamos que seleccionamos una muestra de tamaño \\(n=3\\) de esta población, para ambos mecanismos de selección (s/rep, c/rep), se pide para ambos mecanismos: Conocer la cantidad de muestras posibles y mostrar estas. Para el estimador \\[\\hat{\\bar{Y}}=\\frac{1}{n}\\sum_s y_i\\] construir su distribución muestral y calcular su esperanza y su varianza Para el estimador; \\[\\hat{t}_y=\\frac{N}{n}\\sum_s y_i\\] construir su distribución muestral y calcular su esperanza y su varianza Respuesta, (S/rep) Las muestras posibles son 20, estas muestras posibles son: Y&lt;-c(2000,3000,3500,0,6000,4500) s&lt;-combn(Y,3) s ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 2000 2000 2000 2000 2000 2000 2000 2000 2000 ## [2,] 3000 3000 3000 3000 3500 3500 3500 0 0 ## [3,] 3500 0 6000 4500 0 6000 4500 6000 4500 ## [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## [1,] 2000 3000 3000 3000 3000 3000 3000 ## [2,] 6000 3500 3500 3500 0 0 6000 ## [3,] 4500 0 6000 4500 6000 4500 4500 ## [,17] [,18] [,19] [,20] ## [1,] 3500 3500 3500 0 ## [2,] 0 0 6000 6000 ## [3,] 6000 4500 4500 4500 Para el estimador de la media; Tomar en cuenta que el valor del parámetro de la media poblacional es: \\(\\mu_y=\\sum_U y_i /N=3166.667\\) y&lt;-apply(s,2,sum)/3 #Distribución muestral para el estimador de la media hist(y) abline(v=mean(Y),col=&quot;red&quot;,lwd=3) # calcular la esperanza y la varianza uy&lt;-sum(y*(1/20)) # esperanza del estimador de la media sum((y-uy)^2*(1/20)) # varianza de la media muestral ## [1] 711111.1 \\[E[\\hat{\\theta}]=\\sum_{Rs} \\hat{\\theta_s} P(\\hat{\\theta}=\\hat{\\theta_s})\\] \\[V(\\hat{\\theta})=E[(\\hat{\\theta}-E[\\hat{\\theta}])^2]=\\sum_{s}(\\hat{\\theta_s}-E[\\hat{\\theta}])^2*P(\\hat{\\theta}=\\hat{\\theta})\\] Nota: Si, \\[E[\\hat{\\theta}]=\\theta\\] decimos que el estimador \\(\\hat{\\theta}\\) es un estimador insesgado (sin sesgo) El estimador de la media muestral, es un estimador insesgado de la media poblacional. Para el estimador del total; Tomar en cuenta que el valor del parámetro del total poblacional es: \\(t_y=\\sum_U y_i=19000\\) ty&lt;-apply(s,2,sum)*(6/3) #Distribución muestral para el estimador del total hist(ty) abline(v=sum(Y),col=&quot;red&quot;,lwd=3) pty&lt;-sum(ty*(1/20)) # esperanza sum((ty-pty)^2*(1/20)) # varianza de la media muestral ## [1] 25600000 \\[E[\\hat{t}_y]=E[N*\\bar{Y}]=N E[\\bar{Y}]=N*u_y=N*\\frac{\\sum_U y_i}{N}=\\sum_U {y_i}=t_y\\] Repetir los cálculos para un muestreo con reposición. Muestras probables \\(6^3=N^n=216\\). Y&lt;-round(rnorm(25,30,5)) s&lt;-combn(Y,10) y&lt;-apply(s,2,sum)/10 hist(y) abline(v=mean(Y),col=&quot;red&quot;,lwd=2) 2.4 Propiedades de los estimadores Se busca que un estimador cumpla al menos 2 de las siguientes propiedades: 2.4.1 Estimador insesgado \\[E[\\hat{\\theta}]=\\theta\\] 2.4.2 Estimador eficiente \\[V[\\hat{\\theta}_1]&lt;V[\\hat{\\theta}_2]\\] Se dice que \\(\\hat{\\theta}_1\\) es más eficiente que \\(\\hat{\\theta}_2\\) si tiene una menor varianza 2.5 Distribución muestral para la media Recordar que para una población (\\(U\\)) con alguna variable \\(X\\) de tipo cuantitativa se puede obtener el parámetro de la media, definido como: \\[\\mu_x=\\frac{\\sum_U x_i}{N}\\] Esta variable \\(X\\) en la población por lo tanto tiene su media \\(\\mu_x\\) y también tiene su varianza, denotada por: \\[V(X)=\\sigma_x^2=\\frac{\\sum_U (x_i-\\mu_x)^2}{N}\\] Teorema: Sean \\(X_1,X_2,\\ldots,X_n\\) variables aleatorias para una muestra de tamaño \\(n\\) extraída de la población \\(U\\), donde estas \\(X_i\\) independientes e idénticamente distribuidas (iid) como: \\[X_i\\sim .(E[X_i]=\\mu_x,V(X_i)=\\sigma_x^2)\\] entonces, si: \\[\\bar{x}=\\frac{\\sum_s x_i}{n}\\] Tenemos que \\[E[\\bar{x}]=\\mu_x\\] \\[V(\\bar{x})=\\sigma^2_{\\bar{x}}=\\frac{\\sigma^2_x}{n}\\] Demostración, \\[E[\\bar{X}]=E\\left[\\frac{\\sum_s x_i}{n}\\right]=\\frac{1}{n}E[x_1+x_2+\\ldots+x_n]=\\frac{1}{n}\\left(E[x_1]+E[x_2]+\\ldots+E[x_n] \\right)=\\] \\[=\\frac{1}{n}(\\mu_x+\\mu_x+\\ldots+\\mu_x)=\\frac{n \\mu_x}{n}=\\mu_x\\] Si, \\(X\\) e \\(Y\\) son independientes \\(Cov(X,Y)=0\\). \\[V(X+Y)=V(X)+V(Y)\\] \\[V(\\bar{X})=V\\left(\\frac{\\sum_s x_i}{n}\\right)=\\frac{1}{n^2}V(x_1+x_2+\\ldots+x_n)=\\frac{1}{n^2}\\{V(x_1)+\\ldots+V(x_n)\\}=\\] \\[=\\frac{1}{n^2}(\\sigma^2_x+\\sigma^2_x+\\ldots+\\sigma^2_x)=\\frac{n \\sigma_x^2}{n^2}=\\frac{\\sigma^2_x}{n}\\] Nota: Cuando no es posible tener acceso al valor de \\(\\sigma^2_x\\) se puede estimar este parámetro, mediante la muestra usando en su lugar a la varianza muestral: \\[s^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] Ejemplo numérico N&lt;-10000 set.seed(1234) edad&lt;-round(runif(N,15,55),0) head(edad) ## [1] 20 40 39 40 49 41 n&lt;-100 100/10000 ## [1] 0.01 format(choose(N,n),scientific = F) ## [1] &quot;65208469245476314532868462062420402886440220844086882000668242466420284888666444448266406288646844404802644648042082422284860680880048646820626840404688288424002408048406662408080424660200808402206460028082808420460084402868046268040884042420&quot; #parámetros ux&lt;-mean(edad) ux ## [1] 35.0144 #Vamos a simular algunas de las muestras posibles k&lt;-100000 xbar&lt;-NULL for(i in 1:k){ xbar[i]&lt;-mean(sample(edad,n)) } mean(xbar)#E[xbar]=ux ## [1] 35.02042 #varianza teórica sigma2x&lt;-sum((edad-mean(edad))^2)/N var_xbar&lt;-sigma2x/n var_xbar ## [1] 1.3216 #con la simulación E[X^2]-E[X]^2 mean(xbar^2)-mean(xbar)^2 ## [1] 1.308612 2.6 Teorema del límite central Teorema: Si \\(\\bar{x}\\) es la media de una muestra aleatoria de tamaño \\(n\\). Tomada de una población \\(U\\) con media \\(\\mu_x\\) y varianza finita \\(\\sigma^2_x\\). Entonces la forma límite de la distribución de: \\[Z=\\frac{\\bar{x}-E[\\bar{x}]}{\\sqrt{V(\\bar{x})}}=\\frac{\\bar{x}-\\mu_x}{\\frac{\\sigma_x}{\\sqrt{n}}}\\] a medida que \\(n \\rightarrow \\infty\\), podemos asegurar que \\(Z\\sim N(0,1)\\), en este marco se puede decir a medida que \\(n\\) es más grande: \\[\\bar{x}\\sim N\\left(\\mu_x,\\frac{\\sigma^2_x}{n}\\right)\\] Nota: esta idea de \\(n\\) grande se usa tradicionalmente el valor de \\(n&gt;30\\), hay textos que plantean \\(n=20\\). Este teorema nos permite indicar que la distribución de la media se puede parametrizar a una Norma. 2.6.1 Simulación del teorema del límite central Vamos a suponer los siguiente: \\(N=1000000\\) Vamos a simular a una variable de edad \\(X:Edad\\) \\(Rx \\in [0,100]\\) Vamos tener dos comportamientos de \\(X\\), Volátil (uniforme) Concentrada (Normal) N&lt;-1000000 set.seed(999) x1&lt;-round(runif(N,0,100),0) set.seed(999) x2&lt;-round(abs(rnorm(N,50,15)),0) par(mfrow=c(1,2)) hist(x1,xlim=c(0,100),main=&quot;Uniforme&quot;) hist(x2,xlim=c(0,100),main=&quot;Normal&quot;) dev.off() ## null device ## 1 # Veamos sus parámetros mean(x1)#media poblacional ## [1] 50.01653 mean(x2)#media poblacional ## [1] 50.00974 var(x1) ## [1] 833.7201 var(x2) ## [1] 224.7493 #### Teorema del limite central #n=30 ,100, 500 n&lt;-30 format(choose(N,n),scientific = F) ## [1] &quot;3768348024714503837228662244820288262846862426804084842200866866628242624608408262040682806208086200444462662662060844642062864868204860466460404466&quot; k&lt;-10000 (k/choose(N,n))*100 ## [1] 2.653683e-142 #simular 10000 muestras distintas de tamaño n y calcular su media. xbar1&lt;-NULL xbar2&lt;-NULL for(i in 1:k){ s1&lt;-sample(x1,n) s2&lt;-sample(x2,n) xbar1[i]&lt;-mean(s1) xbar2[i]&lt;-mean(s2) } #teorema 1 abs(mean(x1)-mean(xbar1))#esperanza ## [1] 0.036567 sigma2_x1&lt;-sum((x1-mean(x1))^2)/N # parámetros sigma2_x1 sigma2_x2&lt;-sum((x2-mean(x2))^2)/N # parámetros sigma2_x2 abs(sum((xbar1^2)*(1/k))-mean(xbar1)^2-sigma2_x1/n) ## [1] 0.2868185 abs(sum((xbar2^2)*(1/k))-mean(xbar2)^2-sigma2_x2/n) ## [1] 0.02380241 #teorema del limite central par(mfrow=c(1,2)) plot(density(xbar1)) plot(density(xbar2)) dev.off() ## null device ## 1 curve(dnorm(x,mean(x1),sqrt(sigma2_x1/n)),xlim = c(30,70),main=&quot;Uniforme&quot;) points(density(xbar1),col=&quot;blue&quot;,type = &quot;l&quot;,lwd=2) curve(dnorm(x,mean(x2),sqrt(sigma2_x2/n)),xlim = c(30,70),main=&quot;Normal&quot;) points(density(xbar2),col=&quot;blue&quot;,type = &quot;l&quot;,lwd=2) Ejemplo, Se tiene una muestra de 35 personas de una población, sobre la cual se mide su estatura en centímetros, se calculó la media muestral de 167.6 cm. Suponiendo un varianza poblacional de 44. Calcular la probabilidad que el estimador de la media sea mayor a 170 cm. Solución. Se pide: \\[P(\\bar{x}&gt;170)\\] Por el teorema del limite central ya que \\(n=35\\) \\[\\bar{x}\\sim N(\\hat{\\mu}_{\\bar{x}}=167.6,\\sigma_{\\bar{x}}=\\sqrt{\\frac{44}{35}})=N(167.6,1.12)\\] \\[P(\\bar{x}&gt;170)=P(Z&gt;3.93)=1-P(Z\\leq 3.93 )=1-\\phi(3.93)=\\] \\[=1-0.9999575=0.0000425\\] Ejercicio, se conoce la media de gastos en transporte de una muestra aleatoria de estudiantes de la UMSA en un día normal, el valor es de 9.5 Bs. Suponer que la varianza del gasto es de 4, calcular las siguientes probabilidades suponiendo un \\(n=20\\), \\(n=40\\), \\(n=1000\\). \\[P(\\bar{x}&lt;5)\\approx 0\\] \\[P(7&lt;\\bar{x}&lt;10)=\\phi(\\frac{10-9.5}{\\sqrt{4/n}})-\\phi(\\frac{7-9.5}{\\sqrt{4/n}})=\\] \\[\\bar{x}\\sim N\\left(\\mu_x,\\frac{\\sigma^2_x}{n}\\right)\\] # pnorm((5-9.5)/sqrt(4/c(20,40,100))) ## [1] 4.053836e-24 2.973462e-46 2.075311e-112 curve(dnorm(x,9.5,0.45),xlim = c(7,12),ylim=c(0,2.5)) curve(dnorm(x,9.5,0.32),xlim = c(7,12),add = T, col=&quot;red&quot;) curve(dnorm(x,9.5,0.06),xlim = c(7,12),add = T, col=&quot;blue&quot;) 2.7 Distribución muestral para la diferencia de medias Sean dos poblaciones \\(U_1\\) y \\(U_2\\) independientes con medias y varianzas respectivamente: \\(\\mu_{x_1}\\) y \\(\\mu_{x_2}\\), \\(\\sigma^2_{x_1}\\) y \\(\\sigma^2_{x_2}\\). Teorema: La distribución muestral de las diferencias de media \\(\\bar{X_1}-\\bar{X_2}\\) esta tiene una distribución aproximadamente normal (\\(n\\rightarrow \\infty\\)) con medias y varianzas dadas por: \\[E[\\bar{X_1}-\\bar{X_2}]=\\mu_{x_1}-\\mu_{x_2}\\] \\[V(\\bar{X_1}-\\bar{X_2})=\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\] Demostración: \\[E[\\bar{X_1}-\\bar{X_2}]=E[\\bar{X_1}]-E[\\bar{X_2}]=\\mu_{x_1}-\\mu_{x_2}\\] \\[V(\\bar{X_1}-\\bar{X_2})=V(\\bar{X_1})+V(\\bar{X_2})=\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\] Por el teorema del limite central con \\(n\\geq30\\): \\[\\bar{X_1}-\\bar{X_2}\\sim N\\left(\\mu_{x_1}-\\mu_{x_2},\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\right)\\] Problema: Existen 2 paralelos en la materia estadística 2, se extrae una muestra en ambos grupos de tamaño 35. Con los datos de la muestra se encontro que el promedio de edad del paralelo A es de 21.5 y del paralelo B de 20.7. Si suponemos que las varianzas de ambos grupos son iguales y el valor es de 1.9. Calcular la probabilidad que la diferencia de edades entre el paralelo A respecto el B sea mayor a un año. Solución, por el teorema del limite central ya que el tamaño de muestra en mayor a 30, se tiene: \\[\\bar{x}_a-\\bar{x}_b \\sim N(21.5-20.7,1.9/35+1.9/35)=N(0.8,0.1086)\\] \\[P(\\bar{x}_a-\\bar{x}_b&gt;1)=P(Z&gt;(1-0.8)/\\sqrt{0.1086})=\\] \\[=P(Z&gt;0.61)=1-\\phi(0.61)=1-0.7291=0.2709\\] 1-pnorm((1-0.8)/sqrt(2*1.9/35)) ## [1] 0.2719331 Ejercicio de clase. Se tienen 2 paralelos de la materia de estadística 2, se obtuvo una muestra de ambos paralelos tal que \\(n_1=40\\), \\(n_2=25\\), las medias muestrales de las notas sobre un examen similar fueron de 15.9 y 17.1, suponer que la varianza poblacional es igual en ambos paralelos y es de 27. Calcular la probabilidad que el rendimiento de ambos paralelos sea el mismo. Solución, por el teorema del limite central \\[\\bar{X_1}-\\bar{X_2}\\sim N\\left(15.9-17.1,\\frac{27}{40}+\\frac{27}{25}\\right)=N(-1.2,\\sigma=1.32)\\] \\[P(\\bar{X_1}-\\bar{X_2}=0)=0\\] \\[P(|\\bar{X_1}-\\bar{X_2}|&lt; \\epsilon)=P(|\\bar{X_1}-\\bar{X_2}|&lt; 0.5)=P(-0.5&lt;\\bar{X_1}-\\bar{X_2}&lt;0.5)=\\] \\[=P(-1.29&lt;\\bar{X_1}-\\bar{X_2}&lt;-0.53)=\\phi(-0.53)-\\phi(-1.29)=\\] \\[=0.2980-0.0985=0.199\\] 2.8 Distribución muestral para la proporción La proporción no es nada más que un caso especial de la media para \\(X\\) que toma valores binarios según alguna característica de interés. Sea \\[P_A=\\frac{\\#A}{N}=\\frac{\\sum_U x_i}{N}, \\quad x_i=1, i \\in A, \\quad x_i=0\\text{ } eoc\\] la proporción de alguna característica de la población. Así la el estimador de la proporción sera: \\[\\hat{P}_A=\\frac{\\sum_s{x_i}}{n}=\\frac{\\#a}{n}\\] Teorema: Para el estadístico \\(\\hat{P}_A\\) se cumple: \\[E[\\hat{P}_A]=P_A\\] \\[V(\\hat{P}_A)=\\frac{\\sigma^2_A}{n}\\] Cuando n tiene al infinito \\[\\hat{P}_A\\sim N\\left(P_A,\\frac{\\sigma^2_A}{n}\\right)\\] Donde \\(\\sigma^2_A\\), sabiendo que \\(x_i\\) es binaria. \\[\\sigma^2_A=\\frac{\\sum_U(x_i-\\mu_x)^2}{N}= P_A *(1-P_A)\\] Demostración \\[\\mu_x=\\frac{\\sum_U{x_i}}{N}=\\frac{\\#A}{N}=P_A\\] \\[\\sigma^2_A=\\frac{\\sum_U(x_i-P_A)^2}{N}=\\frac{\\sum_U x_i^2-2P_A \\sum_U{x_i}+NP_A^2}{N}=\\] \\[=\\frac{NP_A-2P_A NP_A+NP_A^2}{N}=P_A-P_A^2=P_A(1-P_A)\\] Entonces: \\[\\hat{P}_A \\sim N\\left(P_A,\\frac{P_A (1-P_A)}{n}\\right)\\] Problema: Se tiene una muestra aleatoria de tamaño 50 de un grupo de estudiantes, donde se verifico que 42 de ellos cuentan con acceso a internet mediante un plan mensual en sus casas. ¿Cuál es la probabilidad que más del 95% de estos estudiantes tengan acceso a internet mediante un plan mensual en sus casas? Solución, el estimador de la proporción es: \\[\\hat{P}_{internet}=\\frac{42}{50}=0.84 \\rightarrow 84\\%\\] \\[\\hat{P}_{internet} \\sim N\\left(0.84,\\frac{0.84 (1-0.84)}{50}\\right)=N(0.84,0.002688)\\] \\[P(\\hat{P}_{internet}&gt;0.95)=P\\left(Z&gt;\\frac{0.95-0.84}{\\sqrt{0.002688}}\\right)=P(Z&gt;2.12)=\\] \\[=1-\\phi(2.12)=1-0.9830=0.017\\] ### Simulación en R del teorema del limite central para una proporción. N&lt;-100000 x&lt;-round(runif(N,18,60)) a&lt;-(x&gt;30)*1 mean(a)#P_A ## [1] 0.70348 n&lt;-100 r&lt;-10000 pa&lt;-NULL for(i in 1:r){ s&lt;-sample(a,n) pa[i]&lt;-mean(s) } PA&lt;-mean(a) plot(density(pa),col=&quot;blue&quot;,lwd=1.5) points(density(rnorm(10^6,PA,sqrt(PA*(1-PA)/n))),col=&quot;red&quot;,type=&quot;l&quot;,lwd=2) Ejercicio: De una población de 150 estudiantes de la materia de estadística I se toma una muestra de 40 estudiantes, sobre los cuales se realiza un test sobre 100 puntos. Con los siguientes resultados: 59 54 61 58 66 56 35 42 49 65 61 61 58 57 57 60 55 63 66 48 52 54 49 54 55 32 60 40 53 67 54 60 44 74 55 24 43 56 62 50 Calcule: la probabilidad que el promedio de nota sea menor a 50, la probabilidad que el promedio de nota sea mayor a 60 la probabilidad que el promedio de nota se encuentre entre 50 y 55 Solución, Como información \\(N=150\\), \\(n=40\\), \\[\\bar{X}=54.225\\] \\[\\hat{S}^2_x=99.35833\\] Usando el teorema del limite central, podemos decir (aproximar) que: \\[\\bar{X}\\sim N\\left(\\mu_x\\approx\\bar{x},\\frac{\\sigma^2_x}{n}\\approx \\frac{\\hat{S}^2_x}{n} \\right)=N(54.225,2.484)\\] La probabilidad que el promedio de nota sea menor a 50, \\[P(\\bar{X}&lt;50)=P\\left(\\frac{\\bar{X}-\\mu_x}{\\frac{\\sigma_x}{\\sqrt{n}}} &lt;\\frac{50-54.225}{\\sqrt{2.484}}\\right)=P(Z&lt;-2.68)=\\phi(-2.68)=\\] \\[=0.00368\\] la probabilidad que el promedio de nota sea mayor a 60 \\[P(\\bar{X}&gt;60)=P(Z&gt;3.66)=1-P(Z\\leq 3.66)=1-\\phi(3.66)=0.00013\\] la probabilidad que el promedio de nota se encuentre entre 50 y 55 2.9 Distribución muestral para la varianza Recordar que para una población \\(U\\), si observamos a una variable de interés respecto sus características podemos obtener medidas de centralidad y también medidas de variabilidad, por ejemplo, sea \\(X\\) una variables definida para toda la población, y definamos los siguientes parámetros de \\(X\\). \\[\\mu_x=\\frac{\\sum_U x_i}{N}\\] Esta \\(\\mu_x\\) es una medida de centralidad, normalmente conocida como media, promedio de \\(X\\), la otra medida puede ser: \\[\\sigma^2_x=\\frac{\\sum_U (x_i-\\mu_x)^2}{N}\\] \\(\\sigma^2_x\\) es la varianza poblacional Ejemplo, Sea una población de \\(N=5\\) elementos con la variable \\(X=\\{10,15,20,20,35\\}\\), calcular \\(\\mu_x\\) y \\(\\sigma^2_x\\). \\(\\mu_x=20\\) \\(\\sigma^2_x=70\\) Un estimador candidato a ser estimador de la varianza podría ser: \\[\\hat{\\sigma}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n}\\] Sin embargo, este estimador no es un estimador insesgado como se lo estudiara en el próximo capitulo. Ahora existe un estimador que si logra ser insesgado de la varianza poblacional, este se conoce como la varianza muestral. \\[S_x^2=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] Teniendo esto en cuenta, es importante conocer la distribución muestral del estimador de la varianza muestral. #Supongamos que tenemos una población de 10000 personas y se sacan muestras de tamaño 20. Simular el comportamiento de la distribución muestral de la varianza. x&lt;-round(runif(10000,10,70)) svar&lt;-NULL k&lt;-10000 for(i in 1:k){ aux&lt;-sample(x,20) svar[i]&lt;-var(aux) } plot(density(svar)) abline(v=var(x)*(9999/10000),col=&quot;red&quot;) abline(v=var(x)*(9999/10000)+c(-1,1)*sd(svar),col=&quot;red&quot;,lty=2) abline(v=var(x)*(9999/10000)+c(-1,1)*2*sd(svar),col=&quot;red&quot;,lty=3) La teoría estadística para estudiar la distribución muestral de la varianza define el estudio de una nueva distribución paramétrica, esta distribución que ayuda a entender a la distribución de la varianza muestral es la distribución \\(\\chi^2\\) curve(dchisq(x,100),xlim=c(0,200)) Teorema Sea \\(X_1,X_2,\\ldots,X_n\\) una muestra aleatoria extraída de una población Normal \\(N(\\mu_x,\\sigma^2_x)\\), definamos al estadístico (varianza muestral): \\[S^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] Entonces, se cumple \\[\\chi^2=\\frac{(n-1){S}^2_x}{\\sigma^2_x}=\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}\\sim \\chi^2(n-1)\\] 2.10 Distribución \\(\\chi^2\\) Se dice que una variable aleatoria \\(X\\) tiene una distribución Chi-cuadrado \\(\\chi^2\\) con \\(v\\) grados de libertad. Se escribe como: \\(X \\sim \\chi^2(v)\\), donde el \\(Rx=\\{x&gt;0\\}\\), si su función de densidad es: \\[f(x)=\\frac{1}{2^\\frac{v}{2} \\Gamma(\\frac{v}{2})}*x^{\\frac{v}{2}-1}*e^{-\\frac{x}{2}};\\quad x&gt;0\\] curve(dchisq(x,1),xlim=c(0,60),ylim=c(0,0.4)) curve(dchisq(x,2),col=2,add=T) for(v in 2:50){ curve(dchisq(x,v),add=T,col=v) } curve(dchisq(x,10),xlim=c(0,60),main=&quot;v=10&quot;) abline(v=10) curve(dchisq(x,100),xlim=c(0,300),main=&quot;v=100&quot;) abline(v=100) curve(dchisq(x,500),xlim=c(200,800),main=&quot;v=500&quot;) abline(v=500) Donde, \\[E[X]=v \\quad ; \\quad V(X)=2v\\] Relación entre la chi-cuadrado y la normal v&lt;-500 vx2&lt;-2*v curve(dchisq(x,v),xlim=v+c(-3,3)*sqrt(vx2),main=paste0(&quot;v=&quot;,v)) curve(dnorm(x,v,sqrt(vx2)),col=&quot;red&quot;,add=T) Ejercicio 1, Sea X una va, tal que: \\(X\\sim \\chi^2(35)\\). Calcular la probabilidad que: \\[P(30&lt;X&lt;40)=P(X&lt;40)-P(X&lt;30)=0.4503\\] Como se aproxima este valor con la distribución normal. Vamos a suponer que \\(X\\sim N(35,70)\\) ya que n es “grande”. \\[P(30&lt;X&lt;40)=P( -0.5976143&lt;Z&lt;0.5976143)=\\] \\[=\\phi(0.5976)-\\phi(-0.5976)=0.7257-0.2743=0.4514\\] \\[P(X&gt;35)=1-P(X\\leq 35)=0.4682\\] \\[P(X&lt;32)=0.3863\\] En el R curve(dchisq(x,35),xlim=c(0,100),main=&quot;v=35&quot;) abline(v=c(30,40)) pchisq(40,35)-pchisq(30,35) ## [1] 0.4503496 1-pchisq(35,35) ## [1] 0.4682027 pchisq(32,35) ## [1] 0.386295 Ejercicio 2, Para una muestra aleatoria de \\(n=30\\), se busca estimar la varianza poblacional, mediante la varianza muestral, suponiendo que la variable de interés \\[X\\sim N(\\mu_x,\\sigma_x^2=16)\\] Encuentre la probabilidad que la varianza muestral se encuentre entre 12 y 18. \\[P\\left(12&lt;S^2_x&lt;18\\right)=?\\] Solución, \\[P\\left(12&lt;\\hat{S}^2_x&lt;18\\right)=P\\left(12*29/16&lt;(n-1)\\hat{S}^2_x/\\sigma^2_x&lt;18*29/16\\right)=\\] \\[\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=Y\\sim \\chi^2(29)\\] \\[P(21.75&lt;Y&lt;32.625)=0.5373\\] pchisq(32.625,29)-pchisq(21.75,29) ## [1] 0.5372716 Ejercicio 3, Encuentre la probabilidad de que una muestra aleatoria de \\(n=45\\) de un población normal con varianza \\(\\sigma_x^2=14\\), tenga una varianza muestral \\(\\hat{S^2}\\) entre 15 y 22. Solución: \\[P\\left(15&lt;\\hat{S}^2_x&lt;22\\right)=P\\left(15*44/14&lt;(n-1)\\hat{S}^2_x/\\sigma^2_x&lt;22*44/14\\right)=\\] \\[\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=Y\\sim \\chi^2(44)\\] \\[P(47.14&lt;Y&lt;69.14)=0.34\\] pchisq(69.14,44)-pchisq(47.14,44) ## [1] 0.3362603 pchisq(22*44/14,44)-pchisq(15*44/14,44)# más exacto ## [1] 0.3361588 Tomar en cuenta que: \\[\\chi^2=\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}\\sim \\chi^2(n-1)\\] #ejemplo para usar R para calcular probabilidades de la Chi2 pchisq(4,10) # F(t)=P(X&lt;t): F(4) ## [1] 0.05265302 Nota, \\[\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}=\\sum_s\\left(\\frac{x_i-\\bar{x}}{\\sigma_x}\\right)^2\\] En el fondo la distribución \\(\\chi^2\\) es la suma de variables aleatorias Normales estándar al cuadrado. 2.11 Distribución t-student Teorema Sea \\(Z\\) una variable aleatoria normal estándar y \\(V\\) una variable aleatoria chi-cuadrado con \\(v\\) grados de libertad. Si \\(Z\\) y \\(V\\) son independientes, entonces la distribución de la variable aleatoria \\(X\\), donde: \\[X=\\frac{Z}{\\sqrt{V/v}}\\] Se comporta como una distribución \\(t\\) con \\(v\\) grados de libertad. En notación, decimos \\(X\\sim t(v)\\). \\[f(x)=\\frac{\\Gamma(\\frac{v+1}{2})}{\\Gamma{(\\frac{v}{2})}\\sqrt{v\\pi} }\\left(1+\\frac{x^2}{v} \\right)^{-(\\frac{v+1}{2})}; \\quad -\\infty&lt;x&lt;\\infty\\] Al igual que la distribución normal estándar, la \\(t\\) es simétrica al rededor del cero. Y levemente más plana que una normal. Apariencia de la \\(t\\) curve(dt(x,2),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;v=2&quot;) curve(dt(x,2),xlim=c(-5,5),ylim=c(0,0.4),lwd=2) for(v in 3:50){ curve(dt(x,v),xlim=c(-5,5),col=v,add=T) } curve(dt(x,20),xlim=c(-5,5),col=&quot;red&quot;,add=T,lwd=2.5) curve(dnorm(x),col=&quot;blue&quot;,lwd=2.5,add=T) Para una variable \\(X\\sim t(v)\\), se cumple: \\[E[X]=0 \\quad V(X)=\\frac{v}{v-2}; v&gt;2\\] Nota: Cuando \\(v\\rightarrow \\infty\\) \\[t \\sim N\\left(\\mu=0,\\sigma^2=\\frac{v}{v-2}\\right)\\] v&lt;-5 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;5 grados de libertad&quot;) curve(dt(x,v),xlim=c(-5,5),main=&quot;t-student (v=5)&quot;,col=&quot;red&quot;,add=T) v&lt;-30 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;30 grados de libertad&quot;) curve(dt(x,v),xlim=c(-5,5),main=&quot;t-student (v=30)&quot;,col=&quot;red&quot;,add=T) v&lt;-50 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;50 grados de libertad&quot;) curve(dt(x,v),xlim=c(-5,5),main=&quot;t-student (v=50)&quot;,col=&quot;red&quot;,add=T) v&lt;-100 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;100 grados de libertad&quot;) curve(dt(x,v),xlim=c(-5,5),main=&quot;t-student (v=100)&quot;,col=&quot;red&quot;,add=T) Ejercicio: Sea \\(X\\sim t(v=10)\\), calcular: \\[P(-1&lt;X&lt;1)=P(X&lt;1)-P(X&lt;-1)=\\] \\[=0.8295534-0.1704466=0.6591068\\] En R, para obtener \\(P(X&lt;t)=F(t)\\) con \\(X\\sim t(v)\\). curve(dt(x,df=10),xlim=c(-5,5),main=&quot;v=10&quot;) # P(X&lt;2)=F(2), X ~ t(v=10) pt(2,10) ## [1] 0.963306 pt(0,10) ## [1] 0.5 Ejercicio Calcular, si \\(X\\sim t(v=30)\\) \\(P(X&gt;0.5)=1-P(X\\leq 0.5)= 0.3103615\\) \\(P(-1&lt;X&lt;1.2)=F(1.2)-F(-1)=0.7175805\\) \\(P(X&lt;3)=F(3)=0.997305\\) curve(dt(x,df=30),xlim=c(-5,5),main=&quot;v=30&quot;) abline(v=0.5) 1-pt(0.5,30) ## [1] 0.3103615 pt(1.2,30)-pt(-1,30) ## [1] 0.7175805 pt(3,30) ## [1] 0.997305 Suponer que se puede utilizar la normal para aproximar a la variable anterior, usando esa aproximación calcular las mismas probabilidades. \\[X\\sim N\\left(0,\\sigma^2=\\frac{30}{28}\\right)\\] \\(P(X&gt;0.5)=1-P(X\\le 0.5)=0.3145316\\) \\(P(-1&lt;X&lt;1.2)=F(1.2)-F(-1)=0.709836\\) \\(P(X&lt;3)=F(3)=0.9981239\\) 1-pnorm(0.5,0,sqrt(30/28)) ## [1] 0.3145316 pnorm(1.2,0,sqrt(30/28))-pnorm(-1,0,sqrt(30/28)) ## [1] 0.709836 pnorm(3,0,sqrt(30/28)) ## [1] 0.9981239 2.12 Distribución Fisher (F) Teorema Sean \\(U\\) y \\(V\\) dos variables aleatorias independientes, con \\(U\\sim \\chi^2(v_1)\\) y \\(V \\sim \\chi^2(v_2)\\). Y sea la variable \\(X\\) definida como: \\[X=\\frac{U/v_1}{V/v_2}\\] Así, decimos que \\(X\\) se distribuye como una Fisher, \\(X\\sim F(v_1,v_2)\\), donde estas \\(v_1\\) y \\(v_2\\) son los grados de libertad de la Fisher. La forma de la distribución \\(f(x)\\) es: \\[ f(x)=\\frac{\\left(\\frac{v_1}{v_2} \\right)^{v_1/2} x^{v_1/2-1}\\Gamma{(\\frac{v_1+v_2}{2})} }{\\Gamma{(\\frac{v_1}{2})} \\Gamma{(\\frac{v_2}{2})}\\left(1+\\frac{v_1}{v_2}x \\right)^{(v_1+v_2)/2}}, \\quad x&gt;0 \\] Para la Fisher se tiene: \\[E[X]=\\frac{v_2}{v_2-2}; \\quad v_2&gt;2\\] \\[V(X)=\\frac{2 v_2^2 (v_1+v_2-2)}{v_1(v_2-2)(v_2-4)}; \\quad v_2&gt;4\\] Gráficamente curve(df(x,10,10),xlim=c(0,6),ylim=c(0,2)) curve(df(x,10,20),xlim=c(0,6),ylim=c(0,2),col=2,add = T) curve(df(x,20,10),xlim=c(0,6),ylim=c(0,2),col=3,add = T) curve(df(x,100,100),xlim=c(0,6),ylim=c(0,2),col=3,add = T) curve(df(x,100,200),xlim=c(0,6),ylim=c(0,2),col=4,add = T) curve(df(x,200,100),xlim=c(0,6),ylim=c(0,2),col=2,add = T) Ejemplo: Sea \\(x\\sim F(5,9)\\), calcular: \\[P(X&gt;1.2)=1-P(X\\leq 1.2)=1-F(1.2)=\\] \\[=1-0.6182996=0.3817\\] pf(1.2,5,9) ## [1] 0.6182996 curve(df(x,5,9),xlim=c(0,3)) abline(v=1.2) "],["estimaciones.html", "3 Estimaciones 3.1 Inferencia estadística 3.2 Estimadores puntuales 3.3 Estimación por intervalos de confianza 3.4 Ejercicios", " 3 Estimaciones 3.1 Inferencia estadística El proceso por el cual, mediante una muestra estadísticamente seleccionada se busca describir a la población/universo de la cual esta proviene. Podemos clasificar a la inferencia estadística en: Inferencia descriptiva: Tiene el único objetivo de describir a la población mediante la muestra, tradicionalmente se enfoca en estimaciones comunes como; la media, la varianza, total, un porcentaje, diferencia de medias, diferencia de proporciones. Estimación puntual: \\[\\hat{\\theta}\\] + Estimación por intervalos: \\[[ \\hat{\\theta}_{LI},\\hat{\\theta}_{LS}]\\] Pruebas de hipótesis: \\[\\theta=k, \\theta&gt;k, \\theta&lt;k\\] + Tamaño de la muestra (\\(n\\)): \\[n=f(U,V(\\hat{\\theta}),\\hat{\\theta},\\ldots)\\] Inferencia predictiva: Tiene una idea de estudiar; por un lado, la evolución de las estimaciones y sus posibles valores futuros (series de tiempo), por otro lado, le interesa conocer las relaciones (no causales) entre las variables. Series de tiempo Modelos lineales Técnicas multivariantes etc. Inferencia causal: Tiene el objetivo de medir la relación causal entre variables. \\(X \\rightarrow Y\\) Diseños experimentales Diseños cuasi-experimentales Modelos estructurales Etc. Inferencia bayesiana 3.2 Estimadores puntuales Recordemos que tenemos un universo \\(U\\) de tamaño \\(N\\). \\[U=\\{u_1, u_2,\\ldots,u_N\\}\\] Donde cada unidad del universo tiene variables (características) asociadas, pensemos en \\(p\\) características. \\[u_i=\\{X_{i1},X_{i2}, \\ldots , X_{ip}\\}\\] Un parámetro es una función sobre el universo y sus variables, lo denotamos por \\(\\theta\\) \\[\\theta=f(U,X)\\] Un estimador se construye a partir de la definición de una estadística (\\(\\Theta\\)) y tiene el objetivo de aproximar de la mejor forma a un parámetro. \\[\\hat{\\theta}\\rightarrow \\theta\\] El estimador \\(\\hat{\\theta}\\) se construye a partir de una muestra aleatoria (\\(s\\)) de tamaño \\(n\\) obtenida de \\(U\\). \\[\\hat{\\theta}=f(s(n),X)\\] Nota: Para un parámetro \\(\\theta\\), pueden existir muchos estimadores candidatos: \\[\\hat{\\Theta}=\\{\\hat{\\theta}_1,\\hat{\\theta}_2, \\hat{\\theta}_3,...\\}\\rightarrow \\theta\\] la pregunta es ¿Cuál es mejor?. Existen al menos dos criterios: 3.2.1 Estimador insesgado \\[E[\\hat{\\theta}]=\\theta\\] Nota: Un ejemplo de un estimador sesgado es el de la razón. \\[R_{y/x}=\\frac{t_y}{t_x}=\\frac{\\mu_y}{\\mu_x}\\] \\[\\hat{R}_{y/x}=\\frac{\\hat{t}_y}{\\hat{t}_x}=\\frac{\\bar{y}}{\\bar{x}}\\] \\[E[\\hat{R}_{y/x}]=E\\left[\\frac{\\hat{t}_y}{\\hat{t}_x}\\right]=E\\left[\\frac{\\bar{y}}{\\bar{x}}\\right] \\neq \\frac{t_y}{t_x}\\] 3.2.2 Estimador eficiente Supongamos que tenemos dos estimadores para \\(\\theta\\), \\(\\hat{\\theta}_1\\), \\(\\hat{\\theta}_2\\), el estimador más eficiente entre los dos será quien tenga el valor más pequeño en su varianza. \\[min(V(\\hat{\\theta_1}),V(\\hat{\\theta_2})) \\rightarrow \\hat{\\theta}\\] Ejemplo. Sea el vector \\(X=\\{10,10,20,25,30\\}\\) de una población con \\(N=5\\), se define una muestra de \\(n=3\\) y se busca estimar la media de \\(X\\): \\(\\mu_x\\). A partir de los estimadores de la media y la mediana muestral. Determinar: Son estimadores insesgados Cuál estimador es más eficiente Suponga un muestreo sin reposición. Solución. N&lt;-5 x&lt;-c(10,10,20,25,30) x ## [1] 10 10 20 25 30 mean(x)#parámetro ## [1] 19 n&lt;-3#muestra choose(5,3) ## [1] 10 s&lt;-combn(x,3) s ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 10 10 10 10 10 10 10 10 10 ## [2,] 10 10 10 20 20 25 20 20 25 ## [3,] 20 25 30 25 30 30 25 30 30 ## [,10] ## [1,] 20 ## [2,] 25 ## [3,] 30 #distribución muestral de la media muestral dmedia&lt;-apply(s,2,mean) dmedia ## [1] 13.33333 15.00000 16.66667 18.33333 20.00000 ## [6] 21.66667 18.33333 20.00000 21.66667 25.00000 #distribución muestral de la mediana muestral dmediana&lt;-apply(s,2,median) dmediana ## [1] 10 10 10 20 20 25 20 20 25 25 # Son estimadores insesgados #media sum(dmedia*(1/10)) # E[] ## [1] 19 mean(dmedia) ## [1] 19 #media sum(dmediana*(1/10)) # E[] ## [1] 18.5 mean(dmediana) ## [1] 18.5 #parámetro media poblacional mean(x) ## [1] 19 # Cuál estimador es más eficiente sum((dmedia-mean(dmedia))^2*1/10) # media muestral ## [1] 10.66667 sum((dmediana-mean(dmediana))^2*1/10) # mediana muestral ## [1] 35.25 Para este ejercicio, la media muestral es insesgado y más eficiente que la mediana muestral, como un estimador de la media poblacional. Nota Los principales problemas de estimación ocurren con frecuencia para estimar: El promedio o media de una población \\(\\mu\\) \\[\\mu_X=\\frac{\\sum_U X_i}{N}\\] La varianza poblacional \\(\\sigma^2\\) \\[\\sigma^2=\\frac{\\sum_U (X_i-\\mu_X)^2}{N}\\] La proporción de una característica en la población \\(P\\) \\[P_A=\\frac{\\#A}{N}=\\frac{\\sum_{U} X_i}{N}; \\quad \\{X_i=1, \\quad i \\in A, X_i=0,\\quad eoc\\}\\] La diferencia de medias de dos poblaciones \\[\\mu_1-\\mu_2\\] La diferencia de proporciones de dos poblaciones \\[P_1-P_2\\] - El cociente de varianzas de dos poblaciones \\[\\sigma_1/\\sigma_2\\] Estimaciones puntuales razonables de estos parámetros son las siguientes: Para \\(\\mu_x\\), la estimación es la media muestral: \\(\\hat{\\mu_x}=\\bar{X}\\) \\[\\bar{x}=\\frac{\\sum_s X_i}{n}\\] - Para \\(\\sigma^2\\), la estimación es la varianza muestral \\(S^2\\), \\[S^2=\\frac{\\sum_s (X_i-\\bar{X})^2}{n-1}\\] Para \\(P\\), la estimación es \\(\\hat{P}\\), la proporción muestral \\[\\hat{P}_A=\\frac{\\#_sA}{n}=\\frac{\\sum_{s} X_i}{n}; \\quad \\{X_i=1, \\quad i \\in A, X_i=0,\\quad eoc\\}\\] - Para \\(\\mu_1-\\mu_2\\), la diferencia entre las medias de las muestras de dos muestras aleatorias independientes. \\[\\hat{\\mu_1}-\\hat{\\mu}_2=\\bar{X}_1-\\bar{X}_2\\] - Para \\(P_1-P_2\\), la diferencia entre las proporciones de las muestras de dos muestras aleatorias independientes. \\[\\hat{P_1}-\\hat{P}_2\\] - Para el cociente de varianzas \\(\\sigma_1/\\sigma_2\\) un estimador razonables es: \\[\\hat{S}_1/\\hat{S}_2\\] Ejemplo, Suponga que \\(X\\) es una variable aleatoria con media \\(\\mu_x\\) y varianza \\(\\sigma^2_x\\). Sea \\(X_1, X_2, \\ldots, X_n\\) una muestra aleatoria de tamaño \\(n\\) de \\(X\\). DEMOSTRAR que la media muestral \\(\\bar{X}\\) y la varianza muestral \\(S^2\\) son estimadores insesgados de \\(\\mu_x\\) y \\(\\sigma^2_x\\), respectivamente. Como información; \\(E[X]=\\mu\\), \\[V(X)=\\sigma^2=E[X^2]-E[X]^2\\]. Solución, \\[E[\\bar{X}]=E\\left[\\frac{\\sum_s X_i}{n} \\right]=\\frac{1}{n}\\left(\\sum_s E[X_i]\\right)=\\frac{1}{n}\\left(\\sum_{i=1}^n \\mu\\right)=\\mu \\] Así, \\[V(\\bar{X})=\\frac{\\sigma^2}{n}=E[\\bar{X}^2]-E[\\bar{X}]^2=E[\\bar{X}^2]-\\mu^2\\] \\[E[S^2]=E\\left[\\frac{\\sum_s (X_i-\\bar{X})^2}{n-1} \\right]=\\frac{1}{n-1} E\\left[\\sum_s (X_i^2-2X_i \\bar{X}+\\bar{X}^2) \\right]=\\] \\[=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2\\bar{X} \\sum_sX_i +n\\bar{X}^2 \\right]= \\] \\[=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2\\bar{X}n\\frac{\\sum_s X_i}{n} +n\\bar{X}^2 \\right]=\\] \\[=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2n\\bar{X}^2 +n\\bar{X}^2 \\right]=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-n\\bar{X}^2\\right]=\\] \\[=\\frac{1}{n-1}\\left(\\sum_s E[X_i^2]-nE[\\bar{X}^2] \\right)= \\alpha\\] Notar \\(\\sigma^2=E[X^2]-E[X]^2=E[X^2]-\\mu^2\\) para un \\(X_i\\), \\(\\sigma^2=E[X_i^2]-E[X_i]^2=E[X_i^2]-\\mu^2\\), entonces, \\(E[X_i^2]=\\sigma^2+\\mu^2\\). Por otro lado \\(E[\\bar{X}^2]=\\frac{\\sigma^2}{n}+\\mu^2\\), Así: \\[\\alpha=\\frac{1}{n-1}\\left[\\sum_s (\\sigma^2+\\mu^2) -n \\left(\\frac{\\sigma^2}{n}+\\mu^2 \\right) \\right]=\\] \\[=\\frac{1}{n-1}\\left[ n \\sigma^2+n\\mu^2 -\\sigma^2-n\\mu^2 \\right]=\\frac{\\sigma^2(n-1)}{n-1}=\\sigma^2\\] 3.2.3 Error cuadrático medio (ECM) Este se define para un estimador como: \\[ECM(\\hat{\\theta})=E\\left[(\\hat{\\theta}-\\theta)^2\\right]\\] Recordar que \\[V(\\hat{\\theta})=E\\left[(\\hat{\\theta}-E[\\hat{\\theta}])^2\\right]\\] \\[ECM(\\hat{\\theta})=E\\left[\\left[(\\hat{\\theta}-E[\\hat{\\theta}])-(\\theta-E[\\hat\\theta]) \\right]^2\\right]=E\\left[(\\hat{\\theta}-E[\\hat{\\theta}])^2-2(\\hat{\\theta}-E[\\hat{\\theta}])(\\theta-E[\\hat\\theta])+ (\\theta-E[\\hat\\theta])^2 \\right]=\\] \\[=E[(\\hat{\\theta}-E[\\hat{\\theta}])^2]-2(\\theta-E[\\hat\\theta])E\\left[\\hat{\\theta}-E[\\hat{\\theta}]\\right]+E[(\\theta-E[\\hat\\theta])^2]=V(\\hat\\theta)=V(\\hat{\\theta})+E[(\\theta-E[\\hat\\theta])^2]=\\] \\[=V(\\hat{\\theta})+sesgo(\\hat\\theta)^2\\] 3.2.4 Cota de Cramer Rao Es posible obtener una cota inferior de la varianza de todos los estimadores (\\(\\hat{\\theta}_1, \\hat{\\theta}_2,\\ldots\\)) insesgados de \\(\\theta\\). Sea \\(\\hat{\\theta}\\) un estimador insesgado del parámetro \\(\\theta\\), basado en una muestra aleatoria de \\(n\\) observaciones, y considérese que \\(f(x,\\theta)\\) denota la función de distribución de probabilidades de una variable aleatoria \\(X\\). Entonces una cota inferior en la varianza de \\(\\hat{\\theta}\\) es: \\[V(\\hat{\\theta})\\geq\\frac{1}{nE\\left\\{ \\left[\\frac{d}{d\\theta }ln f(X,\\theta) \\right]^2 \\right\\}}\\] Esta desigualdad se denomina cota de Cramer Rao. Si un estimador insesgado \\(\\hat{\\theta}\\) satisface la desigualdad, se tratará del estimador insesgado de varianza mínima de \\(\\theta\\). Ejemplo, Demostrar que la media muestral \\(\\bar{x}\\) es el estimador insesgado de varianza mínima de la media de una distribución normal con varianza conocida. Sea \\(X\\sim N(\\mu,\\sigma^2)\\), sabemos \\(E[\\bar{x}]=\\mu\\) \\[f(X,\\mu)=\\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2}\\] \\[ln f(X,\\mu)=ln\\left( \\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2} \\right)=-ln\\left( \\sqrt{2\\pi} \\sigma \\right) -\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2\\] \\[E\\left\\{\\left[ \\frac{d}{d\\mu} ln f(X,\\mu)\\right]^2 \\right\\}=E\\left\\{ \\left[ \\frac{(X-\\mu)}{\\sigma^2} \\right]^2\\right\\} =E\\left[\\frac{(X-\\mu)^2}{\\sigma^4} \\right]=\\frac{E[(X-\\mu)^2]}{\\sigma^4}=\\] \\[=\\frac{\\sigma^2}{\\sigma^4}=\\frac{1}{\\sigma^2}\\] Finalmente, para la cota de Cramer-Rao \\[V(\\bar{x})\\geq \\frac{1}{\\frac{n}{\\sigma^2}}=\\frac{\\sigma^2}{n}=V(\\bar{x})\\] Tarea: - Para la varianza muestral \\(S^2\\), encontrar la cota de cramer Rao, suponiendo media conocida y distribución normal. - Suponer que \\(x\\sim exp(\\lambda)\\), si el objetivo es estimar \\(\\lambda\\) encontrar la cota de cramer Rao para \\(\\lambda\\). 3.2.5 Método de máxima verosimilitud Suponga que \\(X\\) es una va, con distribución \\(f(X,\\theta)\\), donde \\(\\theta\\) es un parámetro desconocido. Sean \\(X_1, X_2,\\ldots, X_n\\) va. iid. como \\(X\\), la muestra de tamaño \\(n\\). La función de probabilidad de las \\(n\\) va. se escribe como: \\[f(X_1,X_2, \\ldots,X_n,\\theta)=f(X_1,\\theta)*f(X_2,\\theta)*\\ldots*f(X_n,\\theta)=L(\\theta)\\] El estimador de máxima verosimilitud de \\(\\theta\\) es el valor que maximiza la función de probabilidad \\(L(\\theta)\\). Pasos para obtener el estimador de máxima verosimilitud para un parámetro \\(\\theta\\) Obtener \\(L(\\theta)\\) Calcular \\(ln [L(\\theta)]\\) Resolver la ecuación: \\[\\frac{d}{d\\theta} ln [L(\\theta)]=0\\] En el caso de que tengamos más de un parámetro, los pasos son: Obtener \\(L(\\theta_1,\\theta_2,\\ldots)=f(X_1,\\theta_1,\\theta_2,\\ldots)*\\ldots*f(X_n,\\theta_1,\\theta_2,\\ldots)\\) Calcular \\(ln [L(\\theta_1,\\theta_2,\\ldots)]\\) Resolver el sistema de ecuaciones: \\[\\frac{\\partial }{\\partial \\theta_1} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] \\[\\frac{\\partial }{\\partial \\theta_2} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] \\[\\vdots\\] \\[\\frac{\\partial }{\\partial \\theta_p} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] Ejemplos Para el caso discreto, sea \\(x\\) una variable aleatoria con función de probabilidad: \\[\\pi(x)=P(X=x)=\\theta (1-\\theta)^{x-1}; \\quad x=1,2,3,\\ldots; \\quad 0\\leq\\theta\\leq1\\] Encontrar al estimador de máxima verosimilitud de \\(\\theta\\) Solución, Sea \\(X_1, X_2, \\ldots, X_n\\) una muestra aleatoria, donde las \\(X_i\\) son iid. Así: \\[\\pi(x_i)=P(X_i=x_i)=\\theta (1-\\theta)^{x_i-1}; \\quad x_i=1,2,3,\\ldots; \\quad 0\\leq\\theta\\leq1\\] \\[L(\\theta)=\\prod_{i=1}^n \\theta (1-\\theta)^{x_i-1}=\\theta^n (1-\\theta)^{\\sum_s x_i - n}\\] \\[ln (L (\\theta))=ln \\left(\\theta^n (1-\\theta)^{\\sum_s x_i - n} \\right)=n ln (\\theta)+\\left(\\sum_s x_i -n\\right) ln(1-\\theta)\\] \\[\\frac{d}{d \\theta} ln(L(\\theta))=\\frac{n}{\\theta}-\\left(\\sum_s x_i -n\\right)\\frac{1}{1-\\theta}=0\\] \\[\\frac{n}{\\theta}=\\frac{\\left(\\sum_s x_i -n\\right)}{1-\\theta}\\rightarrow n-n\\theta=\\theta \\sum_s x_i-n\\theta\\] \\[n=\\theta \\sum_s x_i\\] \\[\\hat{\\theta}=\\frac{n}{\\sum_s x_i}=\\frac{1}{\\bar{x}}\\] Sea \\(X\\sim Bernoulli(p)\\), la función de probabilidad es: \\[P(X=x)=\\pi(x)=p^x (1-p)^{1-x} \\quad ; x=\\{0,1\\}\\] Si \\(p\\) es el parámetro de interés que se busca estimar, ¿qué forma tendrá el estimador de máxima verosimilitud? Solución, Supongamos que se extrae una muestra de tamaño \\(n\\), así: \\[L(p)=f(X_1,p)*f(X_2,p)*\\ldots*f(X_n,p)=p^{x_1} (1-p)^{1-x_1}*p^{x_2} (1-p)^{1-x_2}*\\ldots*p^{x_n} (1-p)^{1-x_n}\\] \\[L(p)=p^{\\sum_{i=1}^n x_i}*(1-p)^{n-\\sum_{i=1}^n x_i}\\] \\[ln[ L(p)]= \\sum_{i=1}^n x_i ln(p)+\\left(n-\\sum_{i=1}^n x_i \\right) ln(1-p) \\] \\[\\frac{d}{dp}ln[ L(p)]= \\frac{\\sum_{i=1}^n x_i}{p}-\\frac{\\left(n-\\sum_{i=1}^n x_i\\right)}{1-p}=0\\] \\[ \\frac{\\sum_{i=1}^n x_i}{p}-\\frac{\\left(n-\\sum_{i=1}^n x_i\\right)}{1-p}=0\\] \\[\\hat{p}_{mv}=\\frac{\\sum_{i=1}^n x_i}{n}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim Poisson(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de máxima verosimilitud. Solución, recordar que si \\(X\\sim Poisson(\\lambda)\\) \\[\\pi(x)=P(X=x)=\\frac{e^{-\\lambda} \\lambda ^x}{x!}; \\quad X=\\{0,1,2\\ldots\\}\\] \\[L(\\lambda)=\\pi(X_1,\\lambda)*\\pi(X_2,\\lambda)*\\ldots*\\pi(X_n,\\lambda)\\] \\[L(\\lambda)=\\frac{e^{-\\lambda} \\lambda ^{x_1}}{x_1!}*\\frac{e^{-\\lambda} \\lambda ^{x_2}}{x_2!}*\\ldots*\\frac{e^{-\\lambda} \\lambda ^{x_n}}{x_n!}\\] \\[L(\\lambda)=\\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda ^{x_i}}{x_i!}=\\frac{e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n x_i}}{\\prod_{i=1}^n x_i!}\\] \\[ln [L(\\lambda)]=-n\\lambda+\\sum_{i=1}^n x_i ln \\lambda - ln \\prod_{i=1}^n x_i!\\] \\[\\frac{d}{d\\lambda}ln [L(\\lambda)]=-n+\\frac{\\sum_{i=1}^n x_i}{\\lambda}=0\\] \\[\\hat{\\lambda}=\\frac{\\sum_{i=1}^n x_i}{n}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim exp(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de máxima verosimilitud. Solución, recordar que si \\(X\\sim exp(\\lambda)\\) su función de densidad es dada por: \\[f(x)=\\lambda e^{-\\lambda x}; \\quad x\\geq0\\] \\[L(\\lambda)=\\prod_{i=1}^n \\lambda e^{-\\lambda x_i}=\\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}\\] \\[ln [L(\\lambda)]=n ln \\lambda-\\lambda \\sum_{i=1}^n x_i\\] \\[\\frac{d}{d\\lambda}ln [L(\\lambda)]=\\frac{n}{\\lambda}-\\sum_{i=1}^n x_i=0\\] \\[\\hat{\\lambda}=\\frac{1}{\\frac{\\sum_{i=1}^n x_i}{n}}=\\frac{1}{\\bar{X}}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim N(\\mu,\\sigma^2)\\) ambos parámetros desconocidos. Encontrar los estimadores de máxima verosimilitud para \\(\\mu\\) y \\(\\sigma^2\\). Solución, recordar si \\(X\\sim N(\\mu, \\sigma^2)\\) su función de densidad es dada por: \\[f(X)=\\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{1/2} }e^{-\\frac{1}{2}\\frac{\\left(x-\\mu\\right)^2}{\\sigma^2}}\\] \\[L(\\mu,\\sigma^2)=\\prod_{i=1}^n \\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{1/2} }e^{-\\frac{1}{2}\\frac{\\left(x_i-\\mu\\right)^2}{\\sigma^2}}=\\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{n/2} }e^{-\\frac{1}{2 \\sigma^2}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2}\\] \\[ln[L(\\mu,\\sigma^2)]=-\\frac{n}{2}ln (2\\pi \\sigma^2)-\\frac{1}{2 \\sigma^2}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2\\] \\[\\frac{\\partial}{\\partial \\mu} ln[L(\\mu,\\sigma^2)]=\\frac{1}{\\sigma^2} \\sum_{i=1}^n \\left(x_i-\\mu\\right)=0\\] \\[\\frac{\\partial}{\\partial \\sigma^2} ln[L(\\mu,\\sigma^2)]=-\\frac{n}{2 \\sigma^2}+\\frac{1}{2 \\sigma^4}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2=0\\] \\[\\sum_{i=1}^n x_i - n\\mu=0\\] \\[\\hat{\\mu}=\\frac{\\sum_{i=1}^n x_i}{n}=\\bar{X}\\] \\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^n \\left(x_i-\\bar{X}\\right)^2}{n}\\] 3.2.6 Método de momentos Este método fue desarrollado por 1894 por Pearson, a diferencia del método de máxima verosimilitud que fue ampliamente utilizado por Fisher a partir 1912. Recordar que para una variable aleatoria, los momentos respecto el origen son: Primer Momento: \\(\\mu_1=E[X]=\\int x f(x) dx\\) Segundo Momento: \\(\\mu_2=E[X^2]=\\int x^2 f(x) dx\\) k-ésimo momento: \\(\\mu_k=E[X^k]\\) Sea \\(X_1, X_2, \\ldots ,X_n\\) una muestra aleatoria de tamaño \\(n\\) de una va \\(X\\), definamos los primeros \\(k\\) momentos de la muestra respecto al origen como: Primer momento: \\[m_1=\\frac{\\sum_{i=1}^n x_i}{n}\\] Segundo momento: \\[m_2=\\frac{\\sum_{i=1}^n x^2_i}{n}\\] k-ésimo momento: \\[m_k=\\frac{\\sum_{i=1}^n x^k_i}{n}; \\quad k=1,2,\\ldots \\] Los momentos \\(\\mu_k\\) de la población serán funciones de los parámetros desconocidos \\(\\theta\\). Al igualar estos momentos muestrales con los poblaciones vamos a poder construir un sistema de ecuaciones de cuantas incógnitas se defina con la distribución de \\(X\\) \\[\\mu_1=E[x]=m_1=\\frac{\\sum_s x_i}{n}\\] \\[\\mu_2=E[x^2]=m_2=\\frac{\\sum_s x_i^2}{n}\\] \\[\\mu_k=E[x^k]=m_k=\\frac{\\sum_s x_i^k}{n}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim exp(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de momentos. Solución, recordar que si \\(X\\sim exp(\\lambda)\\) su función de densidad es dada por: \\[f(x)=\\lambda e^{-\\lambda x}; \\quad x\\geq0\\] \\[E[X]=m_1\\] \\[E[X]=\\int_0^\\infty x\\lambda e^{-\\lambda x}dx=\\frac{1}{\\lambda}\\] Igualando los momentos: \\[\\frac{1}{\\lambda}=\\frac{\\sum_s x_i}{n}\\] \\[\\hat{\\lambda}= \\frac{1}{\\frac{\\sum_s x_i}{n}}=\\frac{1}{\\bar{X}}\\] Ejercicio, Sea \\(X\\) una va geométrica con parámetro \\(p\\), encuentre un estimador de \\(p\\) mediante el método de momentos y el método de máxima verosimilitud. En base a una muestra aleatoria de tamaño \\(n\\) Recordar que si \\(X\\sim G(p)\\), entonces su distribución de probabilidades es: \\[\\pi(x)=P(X=x)=(1-p)^x p; \\quad x=\\{0,1,2,\\dots\\} \\] Recordar que \\(E[X]=\\frac{1-p}{p}\\), por el método de momentos: \\[\\frac{1-p}{p}=\\bar{X}\\] \\[\\hat{p}=\\frac{1}{\\bar{X}+1}\\] Por el método de máxima verosimilitud. \\[L(p)=\\prod_{i=1}^n (1-p)^{x_i} p=(1-p)^{\\sum_s x_i} p^n\\] \\[ln [L(p)]=\\sum_s x_i ln (1-p)+n ln(p)\\] \\[\\frac{d}{dp}ln [L(p)]=-\\frac{\\sum_s x_i}{1-p}+\\frac{n}{p}=0\\] \\[\\hat{p}=\\frac{1}{\\bar{X}+1}\\] Ejercicio, Sea \\(X\\) una normal con parámetro \\(\\mu\\), \\(\\sigma^2\\), encuentre un estimador de \\(\\mu\\) y \\(\\sigma^2\\) mediante el método de momentos. En base a una muestra aleatoria de tamaño \\(n\\) \\[E[X]=m_1\\rightarrow \\mu=\\bar{X}\\] \\[E[X^2]=m_2\\rightarrow E[X^2]=\\frac{\\sum_s x_i^2}{n}\\rightarrow (\\alpha)\\] Recordar que: \\[\\sigma^2=E[X^2]-E[X]^2=E[X^2]-\\mu^2 \\approx E[X^2]-\\bar{X}^2\\] Para \\((\\alpha)\\) \\[\\sigma^2+\\bar{X}^2=\\frac{\\sum_s x_i^2}{n}\\rightarrow \\hat{\\sigma}^2=\\frac{\\sum_s x_i^2}{n}-\\bar{X}^2\\] Tarea, sea \\(X\\) una va con función de densidad: \\[f(x)=\\frac{2(\\theta-x)}{\\theta^2}; \\quad 0\\leq x\\leq \\theta; \\quad \\theta&gt;0\\] Verificar que es una función de densidad Encontrar el estimador de \\(\\theta\\) por el método de momentos y máxima verosimilitud Verificar si el estimador por método de momentos y máxima verosimilitud es insesgado. Solución Verificar que es una función de densidad \\[1=\\int_{Rx} f(x)dx=\\int_{0}^\\theta \\frac{2(\\theta-x)}{\\theta^2} dx=\\frac{2}{\\theta^2}(-1) \\frac{(\\theta-x)^2}{2}/_0^\\theta\\] \\[\\frac{1}{\\theta^2}(-1)\\left(0^2-\\theta^2 \\right)=1 \\] Encontrar el estimador de \\(\\theta\\) por el método de momentos y máxima verosimilitud Suponer que tenemos una muestra aleatoria de tamaño \\(n\\), con las va \\(x_1,x_2,\\ldots,x_n\\) siente estas variables iid. Para el método de momentos se requiere resolver: \\[E[X]=m_1; \\quad E[X]=\\bar{X}\\] Se requiere calcular \\(E[X]\\) \\[E[X]=\\int_{Rx}x f(x)dx=\\int_0^\\theta x \\frac{2(\\theta-x)}{\\theta^2} dx=\\frac{\\theta}{3}\\] Entonces, \\[\\frac{\\theta}{3}=\\bar{X}\\quad \\rightarrow \\hat{\\theta}=3\\bar{X}\\] Para el método de máxima verosimilitud, sabemos que: \\[f(x_i,\\theta)=\\frac{2(\\theta-x_i)}{\\theta^2}\\] \\[L(\\theta)=\\prod_{i=1}^n \\frac{2(\\theta-x_i)}{\\theta^2}=\\frac{2^n}{\\theta^{2n}}\\prod_{i=1}^n\\left( \\theta - x_i\\right) \\] \\[ln(L(\\theta))=ln\\left[\\frac{2^n}{\\theta^{2n}}\\prod_{i=1}^n\\left( \\theta - x_i\\right)\\right]=n ln (2)-2n ln(\\theta)+ln (\\prod_{i=1}^n\\left( \\theta - x_i\\right))\\] \\[=n ln (2)-2n ln(\\theta)+\\sum_{i=1}^n ln (\\theta-x_i)\\] \\[\\frac{d ln(L(\\theta))}{d\\theta}=-\\frac{2n}{\\theta}+\\sum_{i=1}^n \\frac{1}{\\theta-x_i}=0\\] \\[\\sum_{i=1}^n \\frac{1}{\\theta-x_i}=\\frac{2n}{\\theta} \\rightarrow\\quad \\sum_{i=1}^n \\frac{\\theta}{\\theta-x_i}=\\sum_{i=1}^n 2\\rightarrow \\frac{\\theta}{\\theta-x_i}=2\\] \\[\\theta=2x_i \\rightarrow \\sum_s \\theta = 2 \\sum_s x_i\\] \\[\\hat{\\theta}=2\\bar{X}\\] Verificar si el estimador por método de momentos y máxima verosimilitud es insesgado. Para el método de momentos: \\[E[\\hat{\\theta}]=E[3\\bar{X}]=3E\\left[\\frac{\\sum_s x_i}{n}\\right]=\\frac{3}{n}\\sum_s E[x_i]=\\frac{3}{n}\\sum_s \\frac{\\theta}{3}=\\] \\[=\\frac{3n\\theta}{n3}=\\theta\\] Para el método de máxima verosimilitud \\[E[\\hat{\\theta}]=E[2\\bar{X}]=2E\\left[\\frac{\\sum_s x_i}{n}\\right]=\\frac{2}{n}\\sum_s E[x_i]=\\frac{2}{n}\\sum_s \\frac{\\theta}{3}=\\frac{2}{3}\\theta\\] 3.3 Estimación por intervalos de confianza Para construir un intervalo de confianza del parámetro desconocido \\(\\theta\\), se debe encontrar dos estadísticas \\(L\\) y \\(U\\) tales que: \\[P(L\\leq\\theta\\leq U)=1-\\alpha\\] El intervalo \\(L\\leq\\theta\\leq U\\) se llama intervalo de confianza del \\(100*(1-\\alpha)\\) (Porcentaje de confiabilidad del intervalo). A \\(L\\) se lo conoce como límite inferior y \\(U\\) como límite superior. La interpretación del intervalo de confianza es que si se coleccionan muchas muestras aleatorias y se calcula un intervalo de confianza del \\(100*(1-\\alpha)\\) por ciento en \\(\\theta\\) de cada muestra, entonces \\(100*(1-\\alpha)\\) por ciento de estos intervalos contendrán el verdadero valor de \\(\\theta\\). 3.3.1 Intervalo de confianza para la media, asumiendo varianza conocida. Sea \\(X\\) una va con media desconocida \\(\\mu\\) y varianza conocida \\(\\sigma^2\\). Suponga que se toma una muestra aleatoria de tamaño \\(n\\), \\(X_1,X_2,\\ldots,X_n\\). Puede obtenerse un intervalo de confianza del \\(100*(1-\\alpha)\\) por ciento en \\(\\mu\\) considerando la distribución de muestreo de \\(X\\) de la media muestral \\(\\bar{x}\\). Por el teorema del límite central sabemos que: \\[\\bar{x}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\\] bajo ciertas condiciones (\\(n&gt;20\\)). Así: \\[Z=\\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\quad \\rightarrow \\quad Z\\sim N(0,1)\\] Teniendo a \\(Z\\sim N(0,1)\\), para armar el intervalo de confianza basta con trabajar sobre: \\[P(L\\leq Z\\leq U)=1-\\alpha\\] Para un intervalo de confianza \\(L \\leq \\theta \\le U\\) se debe asegurar que la precisión de los lados sea la misma, \\(\\theta-L=U-\\theta\\). curve(dnorm(x),xlim=c(-3.5,3.5),xlab=&quot;z&quot;) abline(v=0) sx&lt;-c(-3.5,seq(-3.5,qnorm(0.025),0.01),qnorm(0.025)) sy&lt;-c(0,dnorm(seq(-3.5,qnorm(0.025),0.01)),0) polygon(sx,sy,col=&quot;red&quot;) polygon(-1*sx,sy,col=&quot;red&quot;) #para encontrar los z_{alpha/2} qnorm(0.1/2)#10% ## [1] -1.644854 qnorm(0.05/2)#5% ## [1] -1.959964 qnorm(0.01/2)#1% ## [1] -2.575829 #para encontrar los z_{1-alpha/2} qnorm(1-0.1/2)#10% ## [1] 1.644854 qnorm(1-0.05/2)#5% ## [1] 1.959964 qnorm(1-0.01/2)#1% ## [1] 2.575829 Si \\(\\alpha=0.05\\) \\[P(L\\leq Z\\leq U)=0.95\\] \\[P(-Z_{\\alpha/2}\\leq Z\\leq Z_{\\alpha/2})=1-\\alpha\\] \\[P\\left(-Z_{\\alpha/2}\\leq \\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq Z_{\\alpha/2}\\right)=1-\\alpha\\] \\[P\\left(\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\leq\\mu \\leq \\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha\\] Así de esta manera tenemos identificados a \\(L\\) y \\(U\\) para \\(\\mu\\) con varianza conocida. \\[L=\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] \\[U=\\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] \\(\\alpha\\) es conocida como el nivel de significancia y \\(1-\\alpha\\) como la confiabilidad. Los valores más usuales de \\(\\alpha\\) son 0.01, 0.05 y 0.1, para estudios sobre ciencias de la salud el valor recomendado es de 0.01 o menor, para las ciencias sociales y económicas el valor recomendado es 0.05. Para la distribución normal los valores de \\(Z_{\\alpha/2}\\) son: \\(\\alpha=0.1\\), \\(Z_{\\alpha/2}=Z_{0.05}=1.64\\) (90% confiabilidad) \\(\\alpha=0.05\\), \\(Z_{\\alpha/2}=Z_{0.025}=1.96\\) (95% confiabilidad) \\(\\alpha=0.01\\), \\(Z_{\\alpha/2}=Z_{0.005}=2.58\\) (99% confiabilidad) Ejercicio, Se sabe que la vida en horas de una bombilla eléctrica de 75 watts se distribuye aproximadamente normal, con \\(\\sigma=25\\) horas. Una muestra aleatoria de 20 bombillas tiene una vida media de \\(\\bar{x}=1014\\) horas. Encontrarlos intervalos de confianza al 90, 95 y 99 % de confiabilidad. Solución, como información \\(n=20\\), para elaborar los intervalos: \\[\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\leq\\mu \\leq \\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] Al 90% \\[1014-1.64 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+1.64 \\frac{25}{\\sqrt{20}}\\] \\[1004.832 \\leq \\mu \\leq 1023.168\\] Al 95% \\[1014-1.96 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+1.96 \\frac{25}{\\sqrt{20}}\\] \\[1003.043 \\leq \\mu \\leq 1024.957\\] Al 99% \\[1014-2.58 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+2.58 \\frac{25}{\\sqrt{20}}\\] \\[999.5774 \\leq \\mu \\leq 1028.423\\] \\[|z_{\\alpha/2}|=z_{1-\\alpha/2}\\] Algunos textos como el texto guía \\[z_{\\alpha/2}=z_{1-\\alpha/2}\\] 3.3.1.1 Tamaño de muestra Definamos al margen de error absoluto como: \\[\\epsilon=Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] Notar que es posible despejar \\(n\\) y esto permitirá tener una formula para definir un tamaño de muestra condicionado a: el margen de error (\\(\\epsilon\\)), desviación de los datos (\\(\\sigma\\)) y el nivel de confiabilidad (\\(Z_{\\alpha/2}\\)) \\[n=\\frac{Z_{\\alpha/2}^2*\\sigma^2}{\\epsilon^2}=\\left(\\frac{Z_{\\alpha/2}*\\sigma}{\\epsilon} \\right)^2\\] Nota: Esta formula se puede utilizar en la medida que la muestra que se seleccione sea aleatoria simple, se refiere a la selección de unidades simples. Ejercicio: Se quiere estudiar la cantidad de dinero promedio gastado en pasajes (ida y vuelta) por los estudiantes de informática cuando van al monoblock un día cualquiera, para ello se decide hacer una encuesta aleatoria, si se fija un nivel de confiabilidad del 95% y se define que el margen de error no supere los 0.30 Bs. Calcular el tamaño de muestra necesario para esta encuesta. Solución: Ya que no se cuenta con la información respecto \\(\\sigma\\) (la variabilidad) hay varias alternativas para definir este valor; buscar información de un estudio similar, plantear un \\(\\sigma\\) con base en la experiencia y la más recomendada es realizar una prueba piloto (estudio a escala reducida). Se realizo una piloto sobre 10 estudiantes y las medidas fueron: 4, 6, 4, 4, 6, 5 , 8, 6, 2, 2. Por lo tanto: \\[n=\\left(\\frac{Z_{\\alpha/2}*\\sigma}{\\epsilon} \\right)^2=\\left(\\frac{Z_{\\alpha/2}*s}{\\epsilon} \\right)^2=\\left(\\frac{1.96*1.89}{0.3} \\right)^2=152.47\\approx 153\\] Ejemplo, se busca conocer el tiempo promedio en horas/día que pasan los estudiantes de informática de la UMSA en la computadora, para ello se planea realizar una encuesta aleatoria, que logre un 95% de confiabilidad y tenga un margen de error de 0.8 horas. Definir el tamaño de muestra necesario. Solución, como información se tiene: \\(\\epsilon=0.8\\), \\(Z_{\\alpha/2}=1.96\\), para el valor de \\(\\sigma\\) para el calculo del tamaño de muestra se realizó una piloto en la materia de estadística II a 13 estudiantes y el resultado fue \\(\\sigma=2.72\\) (varianza muestral). Así: \\[n=\\left(\\frac{1.96*2.72}{0.8} \\right)^2=44.41 \\approx 45\\] Ejercicio, se quiere obtener el tamaño de muestra para un estudio que busca conocer el promedio de gasto en educación mensual de los hogares de los estudiantes de informática. Se quiere un nivel de confiabilidad del 90%, asumiendo un margen de error de 300 Bs. Se sabe de un estudio similar que \\(\\hat{\\sigma}=650\\) Bs. Solución, \\[n=\\left(\\frac{1.64*650}{300} \\right)^2= 12.62\\approx 13 \\] Ejercicio, Se quiere hacer una encuesta sobre los ingresos de los hogares en un determinado barrio de La Paz, tomar como referencia un nivel de confiabilidad del 95% y 99%, un margen de error de 800 Bs. Suponiendo que \\(\\sigma=1300\\). 95%: \\(10.14\\approx 11\\) 99%: \\(17.58\\approx 18\\) 3.3.2 Simulación sobre la confiabilidad de los intervalos de confianza. N&lt;-200 n&lt;-25 set.seed(123) x&lt;-round(runif(N,2,15),1)#gasto en pasaje mean(x) ## [1] 8.582 sigma&lt;-sqrt(var(x)*(199/200)) # muestras choose(200,25) ## [1] 4.521713e+31 plot(0,type=&quot;n&quot;,xlim=c(2,15),ylim=c(0,100)) abline(v=mean(x),col=&quot;red&quot;) for(i in 1:100){ s&lt;-sample(x,25) xbar&lt;-mean(s) #IC 95% ic&lt;-xbar+c(-1,1)*1.96*sigma/sqrt(n) segments(ic[1],i,ic[2],i,col=&quot;blue&quot;) if(mean(x)&gt;ic[2] | mean(x)&lt;ic[1]){ segments(ic[1],i,ic[2],i,col=&quot;red&quot;) } } 3.3.3 Intervalo de confianza sobre la diferencia de dos medias, conocida la varianza Tenemos dos va independientes, \\(X_1\\) con media \\(\\mu_1\\) desconocida y varianza \\(\\sigma^2_1\\) conocida y \\(X_2\\) con media \\(\\mu_2\\) desconocida y varianza \\(\\sigma^2_2\\) conocida. El objetivo es encontrar un intervalo para \\(\\mu_1-\\mu_2\\). Sea dos muestras aleatorias recopíladas para ambas va, de tal forma que \\(n_1\\) representa el tamaño de muestra para \\(X_1\\) y \\(n_2\\) para \\(X_2\\). Recordar por el teorema del limite central, esta diferencia de medias puede ser estimada por sus medias muestrales y además se aproxima a una normal, tal que: \\[\\bar{X}_1-\\bar{X}_2 \\sim N\\left(\\mu_{\\bar{X}_1-\\bar{X}_2}=\\mu_1-\\mu_2,\\sigma^2_{\\bar{X}_1-\\bar{X}_2}=\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}\\right)\\] Ahora, \\[Z=\\frac{\\bar{X}_1-\\bar{X}_2-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}}\\sim N(0,1)\\] Dado que \\(Z\\sim N(0,1)\\), ahora lo que queda es trabajar sobre: \\[P(-Z_{\\alpha/2} \\leq Z \\leq Z_{\\alpha/2})=1-\\alpha\\] Así, el limite inferior y superior esta dado por: \\[L=\\bar{X}_1-\\bar{X}_2-Z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\] \\[U=\\bar{X}_1-\\bar{X}_2+Z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\] Notar que en general, dado un estimador \\(\\hat{\\theta}\\) para el parámetro \\(\\theta\\), su usamos el teorema del limite central su intervalo de confianza estará dado por: \\[IC(\\theta): \\quad \\hat{\\theta} \\pm Z_{\\alpha/2} \\sqrt{V(\\hat{\\theta})} \\] Ejemplo: Se lleva a cabo pruebas de resistencia a la tensión sobre diferentes clases de largueros de aluminio utilizados en la fabricación de alas de aeroplanos comerciales. De la experiencia pasada con el proceso de fabricación de largueros y del procedimiento de prueba, se supone que las desviaciones estándar de las resistencias a la tensión son conocidas, Los datos obtenidos son: Clase de larguero 1: \\(n_1=18\\), \\(\\bar{x}_1=85.9\\), \\(\\sigma_1=1\\) Clase de larguero 2: \\(n_2=16\\), \\(\\bar{x}_2=73.3\\), \\(\\sigma_2=1.5\\) Si \\(\\mu_1\\) y \\(\\mu_2\\) son las verdaderas resistencias a la tensión de ambas clases de largueros. Encuentre intervalos de confianza al 90%, 95%, 99% y 80% de confiabilidad para la diferencia de estas medias. Solución, para el 90% de confiabilidad el intervalo esta dado por: \\[IC_{90\\%}(\\mu_1-\\mu_2): 85.9-73.3 \\pm 1.64*\\sqrt{\\frac{1^2}{18}+\\frac{1.5^2}{16}}=12.6 \\pm 0.73: [11.87\\quad 13.33]\\] \\[IC_{95\\%}(\\mu_1-\\mu_2): 85.9-73.3 \\pm 1.96*\\sqrt{\\frac{1^2}{18}+\\frac{1.5^2}{16}}=12.6 \\pm 0.87: [11.73\\quad 13.47]\\] \\[IC_{99\\%}(\\mu_1-\\mu_2): 85.9-73.3 \\pm 2.58*\\sqrt{\\frac{1^2}{18}+\\frac{1.5^2}{16}}=[11.45 \\quad 13.74]\\] \\[IC_{80\\%}(\\mu_1-\\mu_2): 85.9-73.3 \\pm 1.28*\\sqrt{\\frac{1^2}{18}+\\frac{1.5^2}{16}}=[12.03 \\quad 13.17]\\] 3.3.4 Intervalo de confianza para la media y la diferencia de medias con varianza desconocida pero muestra mayor a 30. Para los 2 anteriores intervalos definidos se suponía que la varianza es conocida, cuando no sucede esto la mejor alternativa para reemplazar a \\(\\sigma^2\\) es usando \\(S^2\\), esto siempre y cuando el tamaño de muestra sea grande (\\(n&gt;30\\)), para el caso de la diferencia de medias, ambos tamaños de muestra deben ser mayores a 30 \\((n_1,n_2&gt;30)\\). Esto debido al teorema del limite central. Para la media \\[IC_{100(1-\\alpha)}(\\mu): \\bar{X}\\pm Z_{\\alpha/2} \\sqrt{\\frac{S^2}{n}}\\] Para la diferencia de medias \\[IC_{100(1-\\alpha)}(\\mu_1-\\mu_2): \\bar{X}_1-\\bar{X}_2 \\pm Z_{\\alpha/2} \\sqrt{\\frac{S_1^2}{n_1}+\\frac{S_2^2}{n_2}}\\] Ejercicio: Se cuenta con muestras de tamaño 35 de dos paralelos en una determinada materia, se esta estudiando las notas obtenidas en el primer parcial que era sobre 30 puntos, la información disponibles es la siguiente: \\[\\bar{x}_1=18 \\quad \\sum_{m2} x_i=490 \\quad S_1=7 \\quad S_2=8.9\\] Calcular los siguientes intervalos de confianza al 90%: + Para cada media de las poblaciones \\(\\mu_1, \\mu_2\\) + Para la diferencia de medias \\(\\mu_1-\\mu_2\\) + Para la diferencia de medias \\(\\mu_2-\\mu_1\\) Solución: \\[IC_{90\\%}(\\mu_1): 18\\pm 1.64 \\sqrt{\\frac{7^2}{35}}=[16.06 \\quad 19.94]\\] \\[IC_{90\\%}(\\mu_1): 14\\pm 1.64 \\sqrt{\\frac{8.9^2}{35}}=[11.53 \\quad 16.47]\\] \\[IC_{90\\%}(\\mu_1-\\mu_2): [0.86 \\quad 7.13]\\] \\[IC_{90\\%}(\\mu_2-\\mu_1): [-7.13 \\quad -0.86]\\] Ejemplo Se tiene una muestra de 32 estudiantes respecto la nota del primer parcial de la materia de estadística 2 de la carrera de informática, calcular un intervalo de confianza para la media al 95 y 97% de confiabilidad. Los datos son: bd&lt;-readxl::read_excel(&quot;C:\\\\Users\\\\Alvaro\\\\Box\\\\2022\\\\umsa\\\\EST-145\\\\r-exams\\\\parcial1_notas_i2022.xlsx&quot;) x&lt;-bd$nota[!is.na(bd$nota)] set.seed(346) s&lt;-sample(x,32) s ## [1] 18 18 12 3 18 9 3 15 18 18 12 6 21 6 15 ## [16] 15 9 6 18 24 15 12 9 18 18 9 9 9 9 18 ## [31] 18 18 Solución #95% mean(s)+c(-1,1)*1.96*sqrt(var(s)/32) ## [1] 11.40977 15.21523 #97 qnorm(0.03/2,lower.tail = F) ## [1] 2.17009 mean(s)+c(-1,1)*qnorm(0.03/2,lower.tail = F)*sqrt(var(s)/32) ## [1] 11.20582 15.41918 Ejemplo 2 se tiene el dato de una muestra aleatoria de 35 estudiantes sobre la variable horas/día en la computadora, se pide calcular los intervalos de confianza al 90, 95 y 99% de confiabilidad, los datos son: n&lt;-35 set.seed(1517) x&lt;-round(rnorm(n,7.6,2.72),0) mean(x)+c(-1,1)*1.64*sqrt(var(x)/n)#90% ## [1] 6.277474 7.608240 mean(x)+c(-1,1)*1.96*sqrt(var(x)/n)#95% ## [1] 6.147643 7.738071 mean(x)+c(-1,1)*2.58*sqrt(var(x)/n)#99% ## [1] 5.896096 7.989618 Ejercicio, se tiene el dato de una muestra aleatoria de 40 personas, respecto su edad. Construya el intervalo de confianza al 95% de confiabilidad. Los datos son: set.seed(1534) x&lt;-round(runif(40,19,29)) x ## [1] 20 26 25 24 22 27 21 28 23 26 23 24 27 22 20 ## [16] 27 23 26 22 23 23 28 21 25 20 19 24 22 27 22 ## [31] 21 28 28 24 23 24 21 28 27 24 mean(x)+c(-1,1)*1.96*sqrt(var(x)/40) ## [1] 23.12872 24.77128 3.3.5 Intervalo de confianza para la media y la diferencia de medias con varianza desconocida pero muestra menor a 30. Para producir un intervalo de confianza valido en estos casos, se debe realizar supuestos fuertes respecto la población de la cual proviene la información. La suposición mas usual es que la población de base viene de una normal, lo que nos lleva en distribuciones muestrales a trabajar con un distribución \\(t-student\\). Para la media, sea: \\[t=\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n}}}\\] Ahora \\(t\\sim t-student(n-1)\\), al igual que para la normal el objetivo es encontrar: \\[P(L\\leq t \\leq U)=1-\\alpha\\] \\[P(-t_{\\alpha/2,n-1} \\leq t \\leq t_{\\alpha/2,n-1})=1-\\alpha\\] Lo que nos a: \\[IC_{100*(1-\\alpha)}(\\mu): \\bar{X} \\pm t_{\\alpha/2,n-1} \\sqrt{\\frac{\\hat{S}^2}{n}}\\] Usando la tabla del libro guía, buscar el valor: \\[t_{0.05,19}=1.729\\] Nota: El valor de \\(t_{\\alpha/2,n-1}\\) en R. qnorm(0.05/2,lower.tail = F)#PARA LA NORMAL ## [1] 1.959964 #para la t-student alpha&lt;-c(0.1,0.05,0.01) n&lt;-20 qt(alpha/2,n-1,lower.tail = F) ## [1] 1.729133 2.093024 2.860935 Ejercicio 1 Se tienen las siguientes edades de una muestra de 10 personas de un grupo de universitarios egresados de la carrera de informática, se pide construir intervalos de confianza para la media de edad al 95% y 98% de confiabilidad. Los datos son: n&lt;-10;set.seed(237) x&lt;-round(runif(n,22,30),0) #x&lt;-c(27, 26, 26, 25 ,26, 29, 26, 22, 28, 26) x ## [1] 27 26 26 25 26 29 26 22 28 26 #solución, asumiendo que los datos provienen de una distribución normal mean(x) ## [1] 26.1 t95&lt;-qt(0.05/2,n-1,lower.tail = F) t98&lt;-qt(0.02/2,n-1,lower.tail = F) #95% mean(x)+c(-1,1)*t95*sqrt(var(x)/n) ## [1] 24.7745 27.4255 #98% mean(x)+c(-1,1)*t98*sqrt(var(x)/n) ## [1] 24.44679 27.75321 Ejercicio 2 Suponga que se tiene el dato de 15 personas sobre sus ingresos mensuales, estos datos proviene de una muestra aleatoria. Calcular el estimador de la media e intervalos de confianza al 95% y 98% de confiabilidad. n&lt;-15 set.seed(1423) x&lt;-round(runif(n,500,10000),0) x ## [1] 3415 1271 7587 5620 1277 2620 8805 1811 5152 ## [10] 4956 7823 9186 6785 3465 9672 Solución, bajo el supuesto que los ingresos tienen un comportamiento normal, se utilizara el intervalo de confianza con la t-student. mean(x)#estimador puntual ## [1] 5296.333 #intervalos de confianza mean(x)+c(-1,1)*qt(0.05/2,n-1,lower.tail = F)*sqrt(var(x)/n)#95% ## [1] 3680.188 6912.479 mean(x)+c(-1,1)*qt(0.02/2,n-1,lower.tail = F)*sqrt(var(x)/n)#98% ## [1] 3318.717 7273.950 Ejemplo, suponga que tenemos las mediciones de la estatura en cm de un grupo de 20 personas que fue seleccionado de forma aleatoria de una determinada población, los datos son: set.seed(1421) x&lt;-round(runif(20,140,180)) x ## [1] 171 173 180 152 148 166 177 179 173 167 175 ## [12] 145 152 142 143 155 179 176 173 147 Encontrar un intervalo de confianza al 95% de confiabilidad para la media poblacional. Solución, mx&lt;-mean(x) ta&lt;-qt(0.05/2,19,lower.tail = F) s2&lt;-var(x) n&lt;-20 mx+c(-1,1)*ta*sqrt(s2/n) ## [1] 157.1698 170.1302 Para la diferencia de medias, pero con varianzas iguales \\(\\sigma_1=\\sigma_2\\) Cuando \\(n_1\\) o \\(n_2\\) no superen ambas a 30, la mejor alternativa es usar el intervalo de confianza en base a la distribución \\(t\\), para la diferencia de medias el IC es: \\[IC_{100*(1-\\alpha)}(\\mu_1-\\mu_2)=\\bar{X}_1-\\bar{X}_2+t_{\\alpha/2,n_1+n_2-2} S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\] Con \\(S^2_p\\) (Varianza ponderada) \\[S^2_p=\\frac{(n_1-1)S^2_1+(n_2-1)S^2_2}{n_1+n_2-2}\\] Para la diferencia de medias, pero con varianzas no iguales \\[IC_{100*(1-\\alpha)}(\\mu_1-\\mu_2)=\\bar{X}_1-\\bar{X}_2+t_{\\alpha/2,v} \\sqrt{\\frac{S_1^2}{n_1}+\\frac{S^2_2}{n_2}}\\] \\[v=\\frac{\\left(\\frac{S_1^2}{n_1}+\\frac{S^2_2}{n_2} \\right)^2}{\\frac{(S_1^2/n_1)^2}{n_1+1}+\\frac{(S^2_2/n_2)^2}{n_2+1}}-2\\] Ejemplo, se tienen dos grupos donde se tomo de forma aleatoria a una muestra de ambos grupos, con el fin de estudiar la diferencia de medias para las notas de un examen, los resultados para ambos grupos fueron: set.seed(1421) x1&lt;-round(runif(10,1,100)) set.seed(1457) x2&lt;-round(runif(13,1,100)) x1 ## [1] 79 82 99 30 20 64 94 97 84 67 x2 ## [1] 73 62 57 23 59 16 65 39 81 23 74 22 20 Armar un intervalo de confianza al 95% de confiabilidad suponiendo que las varianzas son iguales y otro para varianzas distintas. mx1&lt;-mean(x1) mx2&lt;-mean(x2) s21&lt;-var(x1) s22&lt;-var(x2) n1&lt;-10 n2&lt;-13 # Estimador puntual de diferencia de medias mx1-mx2 ## [1] 24.36923 # Intervalo con varianzas iguales ta1&lt;-qt(0.05/2,n1+n2-2,lower.tail = F) s2p&lt;-((n1-1)*s21+(n2-1)*s22)/(n1+n2-2) mx1-mx2+c(-1,1)*ta1*sqrt(s2p)*sqrt(1/n1+1/n2) ## [1] 2.132725 46.605736 # Intervalo con varianzas desiguales v&lt;-((s21/n1+s22/n2)^2)/(((s21/n1)^2)/(n1+1)+((s22/n2)^2)/(n2+1))-2 ta2&lt;-qt(0.05/2,v,lower.tail = F) mx1-mx2+c(-1,1)*ta2*sqrt(s21/n1+s22/n2) ## [1] 1.660652 47.077810 3.3.6 Intervalos de confianza para la diferencia de medias de datos pareados En general, supongamos que los datos constan de \\(n\\) pares \\((X_{11}, X_{21}), (X_{12}, X_{22})...(X_{1n}, X_{2n})\\), usualmente esto sucede cuando se trabaja con observaciones en 2 puntos distintos de tiempo. Definamos las diferencias como \\(D_1=X_{11}-X_{21},D_2=X_{12}-X_{22},\\ldots,D_n=X_{1n}-X_{2n}\\), estableciendo a \\(D\\) como la va, su intervalo es: \\[IC_{100*(1-\\alpha)}(\\mu_D=\\mu_1-\\mu_2): \\bar{D} \\pm t_{\\alpha/2,n-1}*\\sqrt{\\frac{S^2_D}{n}} \\] Si n es grande, entonces el intervalo es dado por: \\[IC_{100*(1-\\alpha)}(\\mu_D=\\mu_1-\\mu_2): \\bar{D} \\pm Z_{\\alpha/2}*\\sqrt{\\frac{S^2_D}{n}} \\] Ejemplo, se tiene las notas de los parciales de una muestra de 12 estudiantes, será que los estudiantes mejoraron su rendimiento del parcial 1 al parcial 2. Construya un intervalo de confianza al 90%. set.seed(240) bd&lt;-data.frame(id=1:12,p1=round(runif(12,40,80)),p2=round(runif(12,30,90))) #estimador puntual mean(bd$p1) ## [1] 56.5 mean(bd$p2) ## [1] 74.91667 mean(bd$p2)-mean(bd$p1) ## [1] 18.41667 #creando la variable D=p2-p1 bd$D&lt;-bd$p2-bd$p1 mean(bd$D)+c(-1,1)*qt(0.1/2,11,lower.tail = F)*sqrt(var(bd$D)/12) ## [1] 9.980298 26.853035 qt(0.1/2,11,lower.tail = F)#ver página 779 del libro guía ## [1] 1.795885 ############################# #suponer una muestra de 100 estudiantes n&lt;-100 set.seed(1536) bd2&lt;-data.frame(id=1:n,p1=round(runif(n,40,80)),p2=round(runif(n,30,90))) #estimador puntual mean(bd2$p1) ## [1] 60.27 mean(bd2$p2) ## [1] 60.08 mean(bd2$p2)-mean(bd2$p1) ## [1] -0.19 #creando la variable D=p2-p1 bd2$D&lt;-bd2$p2-bd2$p1 mean(bd2$D)+c(-1,1)*qnorm(0.1/2,lower.tail=F)*sqrt(var(bd2$D)/n) ## [1] -3.466029 3.086029 3.3.7 Intervalo de confianza para proporciones Si asumimos que el estimador del parámetro \\(P\\) (proporción), se distribuye normal, es decir, \\(\\hat{P}\\sim Normal\\), bajo ciertas condiciones como que el tamaño de muestra es grande, se puede plantear al intervalo de confianza como: \\[IC_{100*(1-\\alpha)}(P)=\\hat{P} \\pm Z_{\\alpha/2} * \\sqrt{V(\\hat{P})}\\] Donde para encontrar la \\(V(\\hat{P})\\) basta con resolver la varianza del estimador de la media para valores binarios. \\[V(\\bar{x})=\\frac{\\sigma^2}{n} =V(\\hat{P})=\\frac{\\sigma^2_p}{n}=\\frac{1}{n}\\left(\\frac{\\sum_{i=1}^N x_i^2 }{N}-\\mu^2 \\right)=\\frac{1}{n}\\left(\\frac{\\sum_{i=1}^N x_i }{N}-P^2 \\right)=\\] \\[=\\frac{1}{n}(P-P^2)=\\frac{P(1-P)}{n}\\] Notar que esta varianza esta en términos del parámetro \\(P\\), por lo que recurrimos en el caso muestral a reemplazarlo por su estimador \\(\\hat{P}\\). Así, el intervalo de confianza queda como: \\[IC_{100*(1-\\alpha)}(P)=\\hat{P} \\pm Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P}(1-\\hat{P})}{n}}\\] &gt; Ejercicio 1 En un municipio se van a realizar elecciones para la alcaldía, se tiene información a partir de una encuesta aleatoria a 60 personas que el partido X tiene un apoyo de 33 personas, calcular el intervalo de confianza al 95% para el porcentaje de apoyo al partido X. ¿Se puede afirmar con base en la encuesta que el partido X ganará? Solución, Como información \\(n=60\\). \\(\\hat{P}=33/60=0.55\\) \\[IC_{95\\%}(P_x)=0.55 \\pm 1.96 * \\sqrt{\\frac{0.55*0.45}{60}}=\\] \\[[0.42 \\quad 0.68]\\rightarrow [42.4 \\quad 67.6]\\%\\] Ejercicio 2 Se toma una muestra de 40 estudiantes de la materia de estadística I con base al listado de inscritos, se identificó en la muestra lo siguiente: 10 estudiantes abandonaron la materia 18 estudiantes tienen notas de aprobación Se pide calcular los intervalos de confianza al 95% de confiabilidad para la proporción de abandono en la materia y la proporción de reprobados. Si el curso tiene un total de 180 inscritos, ¿Cuál sera el intervalo de confianza (95%) de la cantidad de estudiantes que aprobaron? Solución n&lt;-40 ab&lt;-10 ap&lt;-18 re&lt;-12 # estimador puntal p_ab&lt;-10/40 p_ap&lt;-18/40 p_re&lt;-12/40 p_ab;p_ap; p_re ## [1] 0.25 ## [1] 0.45 ## [1] 0.3 p_ab*100;p_ap*100; p_re*100 ## [1] 25 ## [1] 45 ## [1] 30 #intervalo de confianza p_ab+c(-1,1)*1.96*sqrt(p_ab*(1-p_ab)/n) #abandonos ## [1] 0.115808 0.384192 p_ap+c(-1,1)*1.96*sqrt(p_ap*(1-p_ap)/n) #aprobados ## [1] 0.2958251 0.6041749 p_re+c(-1,1)*1.96*sqrt(p_re*(1-p_re)/n) #reprobados ## [1] 0.1579845 0.4420155 #% 100*(p_ab+c(-1,1)*1.96*sqrt(p_ab*(1-p_ab)/n)) #abandonos ## [1] 11.5808 38.4192 100*(p_ap+c(-1,1)*1.96*sqrt(p_ap*(1-p_ap)/n)) #aprobados ## [1] 29.58251 60.41749 100*(p_re+c(-1,1)*1.96*sqrt(p_re*(1-p_re)/n))#reprobados ## [1] 15.79845 44.20155 #personas 180*(p_ap+c(-1,1)*1.96*sqrt(p_ap*(1-p_ap)/n)) #aprobados ## [1] 53.24852 108.75148 Ejemplo, en un curso se tomó una muestra aleatoria de 15 personas, respecto su estatura en centímetros, las mediciones son: set.seed(1431) x&lt;-round(rnorm(15,165,10),0) x ## [1] 163 167 156 156 178 172 152 160 140 160 168 ## [12] 175 170 160 164 Se pide encontrar al estimador de la proporción y su intervalo de confianza al 95% de confiabilidad para la proporción de estudiantes con estatura de 170 o más. Solución, n&lt;-15 x&lt;-(x&gt;=170)*1 x ## [1] 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 #el estimador de la proporción p&lt;-mean(x) p ## [1] 0.2666667 #en porcentaje p*100 ## [1] 26.66667 #intervalo p+c(-1,1)*1.96*sqrt(p*(1-p)/n) ## [1] 0.04287417 0.49045916 #intervalo en % (p+c(-1,1)*1.96*sqrt(p*(1-p)/n))*100 ## [1] 4.287417 49.045916 Ejemplo, La varianza del estimador de la proporción tiene la forma: \\[V(\\hat{P})=\\frac{P(1-P)}{n}\\] Si suponemos un tamaño de muestra \\(n\\) conocido, en que valor de \\(P\\) alcanza la varianza su máximo. Solución \\[\\frac{d V(\\hat{P})}{dP}=\\frac{d}{dP}\\left(\\frac{P}{n}-\\frac{P^2}{n}\\right)=\\frac{1}{n}-\\frac{2P}{n}=0 \\rightarrow P=0.5\\] También, \\[\\frac{d^2 V(\\hat{P})}{d^2P}=-\\frac{2}{n}&lt;0 \\quad (máximo)\\] vp&lt;-function(x){x*(1-x)} curve(vp,xlab=&quot;p&quot;,ylab=&quot;v(p)&quot;,main=&quot;n=1&quot;) 3.3.7.1 Tamaño de muestra para estimar proporciones Para la proporción definimos el margen de error (\\(\\epsilon\\)) como: \\[\\epsilon= Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P}(1-\\hat{P})}{n}}\\] Ahora, podemos usar esta definición como una salida para el calculo del tamaño de muestra necesario para cometer un determinado margen de error (\\(epsilon\\)), sujeto a un nivel de confiabilidad de \\(Z_{\\alpha/2}\\), basta con despejar \\(n\\) de la anterior formula. \\[n=\\left(\\frac{Z_{\\alpha/2}}{\\epsilon}\\right)^2 \\hat{P}(1-\\hat{P})\\] Dado que definir el tamaño de muestra es un paso previo a la recolección de información, notar que se tiene total control sobre el margen de error (\\(\\epsilon\\)) y el nivel de confiabilidad (\\(Z_{\\alpha/2}\\)), sin embargo, en la formula aparece el estimador \\(\\hat{P}\\) que es el de interés, la solución para saltarnos este dilema, es elegir un \\(\\hat{P}\\) basado en un estudio similar o una prueba piloto, la otra alternativa extrema es suponer un \\(\\hat{P}\\) que haga máximo a \\(n\\) con lo demás fijo. tm_p&lt;-function(z,e,p){ n&lt;-((z/e)^2)*(p*(1-p)) return(n) } tm_p(1.96,0.05,0.5) ## [1] 384.16 p&lt;-seq(0,1,0.01) plot(p,tm_p(1.96,0.05,p)) Ejercicio, Una carrera en la universidad esta a punto de elegir a sus autoridades, se busca hacer una encuesta de intención de votos en los estudiantes para el candidato \\(Z\\), se quiere un nivel de confianza del 95%, y no errar en +- 5%. Calcular el tamaño de muestra, (1) suponiendo \\(n\\) máxima y (2) mediante un sondeo se verifico que el candidato \\(Z\\) tiene un 70% de apoyo. Solución, ceiling(tm_p(1.96,0.05,0.5))#1 ## [1] 385 ceiling(tm_p(1.96,0.05,0.7))#2 ## [1] 323 ceiling(tm_p(1.96,0.1,0.5))#epsilon 10% ## [1] 97 ceiling(tm_p(1.96,0.01,0.5))#epsilon 1% ## [1] 9604 Nota: La muestra de 385 es valida para cualquier área, siempre y cuando la selección de la muestra se realiza de forma completamente aleatoria (azar) y provenir de una listado de la población objetivo. 3.3.8 Intervalo de confianza para diferencia de proporciones Esta diferencia de proporciones son ampliamente usadas cuando se comparan dos poblaciones, respecto una característica de interés sobre dos poblaciones independientes. Así el intervalo de la diferencia de proporciones esta dado por: \\[IC_{100*(1-\\alpha)}(P_1-P_2)=(\\hat{P}_1-\\hat{P}_2) \\pm Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P_1}(1-\\hat{P}_1)}{n_1}+\\frac{\\hat{P}_2(1-\\hat{P}_2)}{n_2}}\\] Donde \\(\\hat{P}_1\\) y \\(\\hat{P}_2\\) son estimaciones de proporción para la población 1 y 2, respecto una misma característica, \\(n_1\\) y \\(n_2\\) son los tamaños de muestra es estas poblaciones. Ejercicio: Se tienen muestras aleatorias de tamaño 45 de personas que se postularon a una carrera durante los semestres I-2022 y II-2022, se sabe que en el primer semestre de la muestra aprobaron 22 postulantes, mientras que en el segundo semestre fueron 27, obtener el intervalo de confianza al 95% para la diferencia de proporciones del semestre II respecto el I. (5 minutos) Solución: \\[IC_{95\\%}(P_{II}-P_{I})=\\frac{27}{45}-\\frac{22}{45} \\pm 1.96 * \\sqrt{\\frac{27/45(1-27/45)}{45}+\\frac{22/45(1-22/45)}{45}}\\] \\[=\\frac{5}{45}\\pm 0.2045\\rightarrow [-0.093 \\quad 0.315] \\] Ejemplo, En una muestra aleatoria de 20 estudiantes se midió la estatura del grupo, se tiene conocimiento del sexo de los estudiantes y se busca estimar la diferencia de proporciones por hombre y mujer de la proporción de estudiantes que superan los 170 cm de estatura. Los datos son: n&lt;-20 set.seed(1501) estatura&lt;-round(rnorm(n,165,10),0) set.seed(1501) mujer&lt;-rbinom(n,1,0.4) bd&lt;-data.frame(estatura,mujer) bd ## estatura mujer ## 1 161 0 ## 2 179 0 ## 3 171 1 ## 4 152 0 ## 5 141 1 ## 6 168 0 ## 7 168 0 ## 8 173 1 ## 9 155 0 ## 10 169 0 ## 11 160 1 ## 12 177 1 ## 13 164 1 ## 14 165 1 ## 15 159 1 ## 16 172 1 ## 17 158 0 ## 18 169 0 ## 19 172 1 ## 20 166 0 Se pide calcular el estimador puntual de la diferencia de proporciones y el intervalo de confianza al 90% de confiabilidad. Solución, algunos parámetros del ejercicio \\(n_1=10\\) , \\(n_2=10\\), las proporciones: \\[\\hat{P}_{1,h}=\\frac{\\#a}{n_1}=\\frac{1}{10}=0.1\\] \\[\\hat{P}_{2,m}=\\frac{\\#a}{n_2}=\\frac{5}{10}=0.5\\] El estimador puntual es dado por: \\[\\hat{P}_1-\\hat{P}_2=0.1-0.5=-0.4\\] Ahora, el intervalo de confianza \\[IC_{100*(1-\\alpha)}(P_1-P_2)=-0.4 \\pm 1.64 * \\sqrt{\\frac{0.1*0.9}{10}+\\frac{0.5*0.5}{10}}=-0.4 \\pm 0.3024\\] \\[IC_{100*(1-\\alpha)}(P_1-P_2)=[-0.702 \\quad -0.098]\\] 3.3.9 Intervalo de confianza para la varianza Suponga que \\(X\\sim Normal(\\mu,\\sigma)\\) ambos parámetros desconocidos. Sea \\(X_1,X_2,\\ldots,X_n\\) una muestra aleatoria de tamaño \\(n\\) de \\(X\\). Recodar que la varianza muestra sigue una distribución de muestreo tipo \\(\\chi^2\\). \\[\\chi^2=\\frac{(n-1)S^2}{\\sigma^2}\\] Para desarrollar el intervalo usamos esta distribución: \\[P(\\chi^2_{1-\\alpha/2,n-1} \\leq \\chi^2 \\leq \\chi^2_{\\alpha/2,n-1})=1-\\alpha\\] \\[IC_{100*(1-\\alpha)}(\\sigma^2): \\left[\\frac{(n-1){S}^2}{\\chi^2_{\\alpha/2,n-1}} \\quad \\frac{(n-1){S}^2}{\\chi^2_{1-\\alpha/2,n-1}}\\right]\\] Ejercicio, se tiene una muestra aleatoria de 10 personas respecto la cantidad de horas de sueño del día anterior. Los resultados son: 11 8 4 7 11 9 5 10 9 11 Calcular el intervalo de confianza al 95% para la varianza de la variable horas de sueño. Solución, como información, \\(n=10\\). \\(S^2=6.28\\) \\[IC_{95\\%}(\\sigma^2): \\left[\\frac{9*6.28}{\\chi^2_{0.05/2,9}} \\quad \\frac{9*6.28}{\\chi^2_{1-0.05/2,9}}\\right]=\\left[\\frac{56.52}{19.02} \\quad \\frac{56.52}{2.7}\\right]\\] \\[=[2.97 \\quad 20.93]\\] 3.3.10 Intervalo de confianza para el cociente de varianzas El objetivo de esta medida es tener un intervalo para el cociente de las varianzas de dos poblaciones, esto puede servir para identificar que población (con que variable) tiene mayor variabilidad. El parámetro es: \\[\\theta=\\frac{\\sigma^2_1}{\\sigma^2_2}\\] Supongamos que \\(X_1\\) y \\(X_2\\) son va normales con media \\(\\mu_1\\) y \\(\\mu_2\\) desconocidas y varianzas \\(\\sigma^2_1\\), \\(\\sigma^2_2\\) también desconocidas. Sean dos muestras aleatorias de \\(X_1\\) y \\(X_2\\) de tamaño \\(n_1\\) y \\(n_2\\) y sean \\(S^2_1\\) y \\(S^2_2\\) las varianzas de las muestras. Para armar el intervalo de confianza recurrimos a la distribución \\(F\\). \\[F=\\frac{\\frac{S^2_2}{\\sigma^2_2}}{\\frac{S^2_1}{\\sigma^2_1}}\\] Nota: Recordar que la distribución \\(F\\) es el cociente de chi-cuadrados sobre sus grados de libertad. Se busca: \\[P(F_{1-\\alpha/2,n_2-1,n_1-1} \\leq F \\leq F_{\\alpha/2,n_2-1,n_1-1})=1-\\alpha\\] \\[IC_{100*(1-\\alpha)}\\left(\\frac{\\sigma^2_1}{\\sigma^2_2}\\right): \\left[\\frac{{S}^2_1}{{S}^2_2} F_{1-\\alpha/2,n_2-1,n_1-1} \\quad \\frac{{S}^2_1}{{S}^2_2} F_{\\alpha/2,n_2-1,n_1-1}\\right]\\] Para encontrar el valor de F en R. Suponer una muestra para ambas poblaciones de 13 y un nivel de confiabilidad del 95% qf(1-0.05/2,12,12,lower.tail = F) ## [1] 0.3051314 qf(0.05/2,12,12,lower.tail = F) ## [1] 3.277277 3.4 Ejercicios Un ingeniero de control de calidad midió el espesor de la pared de 25 botellas de vidrio de dos litros. La media de la muestra fue 4.05 mm y la desviación estándar de la muestra \\(\\hat{S}=.08\\) mm. Determine un intervalo de confianza: al 90 % de confiabilidad (dos lados) inferior del 90 por ciento respecto al espesor de pared medio. Solución, \\[IC(\\bar{x})_{90\\%}=\\bar{x} \\pm t_{\\alpha/2,n-1} \\sqrt{\\frac{\\hat{S}^2}{n}}=4.05\\pm 1.71 \\sqrt{\\frac{0.08^2}{25}}\\] ta&lt;-qt(0.1/2,24,lower.tail = F) 4.05+c(-1,1)*ta*sqrt(0.08^2/25) ## [1] 4.022626 4.077374 \\[IC(\\bar{x})_{90\\%}=[4.02 \\quad 4.08]\\] Para el intervalo de confianza inferior. Recordar que para el intervalo de confianza de 2 lados, la formula es: \\[P(L&lt;\\theta&lt;U)=1-\\alpha\\] Mientras para un intervalo inferior, la formula es: \\[P(L&lt;\\theta)=1-\\alpha\\] \\[IC(\\bar{x})_{90\\%, inferior}=\\bar{x} - t_{\\alpha,n-1} \\sqrt{\\frac{\\hat{S}^2}{n}}=4.05- 1.32 \\sqrt{\\frac{0.08^2}{25}}\\] qt(0.1,24,lower.tail = F) ## [1] 1.317836 \\[L: 4.03mm\\] Un ingeniero industrial está interesado en estimar el tiempo medio requerido para ensamblar una tarjeta de circuito impreso. ¿Qué tan grande debe ser la muestra si el ingeniero desea tener una confianza del 95 por ciento de que el error en la estimación de la media es menor que .25 minutos? La desviación estándar del tiempo de ensamble es .45 minutos. Solución, sean los datos \\(Z_{\\alpha/2}=1.96\\), \\(\\sigma=0.45\\), \\(\\epsilon=0.25\\) \\[n=\\left(\\frac{Z_{\\alpha/2}*\\sigma}{\\epsilon} \\right)^2=\\left(\\frac{1.96*0.45}{0.25}\\right)^2=12.45\\approx13\\] 3. Dada la siguiente muestra aleatoria por hombre y mujer, respecto sus ingresos: Hombre: 2078, 3565, 3767, 3600, 2907, 1299, 3483, 8475, 2425, 3500, 3118, 3377, 3897, 21883, 9717, 2078, 8167 Mujeres: 1905, 9500, 996, 3250, 3335, 433, 2917, 616, 1625, 1104, 2050, 3000, 1625 se pide calcular intervalos de confianza para la varianza para ambas poblaciones, y para el cociente de varianzas entre \\(Hombre/Mujer\\) y \\(Mujer/Hombre\\). Estos intervalos al 95 % de confiabilidad. (Asumir normalidad de los datos de origen). Solución: hh&lt;-c(2078, 3565, 3767, 3600, 2907, 1299, 3483, 8475, 2425, 3500, 3118, 3377, 3897, 21883, 9717, 2078, 8167) mm&lt;-c(1905, 9500, 996, 3250, 3335, 433, 2917, 616, 1625, 1104, 2050, 3000, 1625) nh&lt;-17;nm&lt;-13 #estimador de varianzas var(hh)#puntual hombres S^2 ## [1] 24271163 var(mm)#puntual mujeres S^2 ## [1] 5400214 #intervalo hombres lih&lt;-(var(hh)*(nh-1))/qchisq(0.05/2,nh-1,lower.tail = F) lsh&lt;-(var(hh)*(nh-1))/qchisq(1-0.05/2,nh-1,lower.tail = F) lih;var(hh);lsh ## [1] 13462780 ## [1] 24271163 ## [1] 56218511 #intervalo mujeres limj&lt;-(var(mm)*(nm-1))/qchisq(0.05/2,nm-1,lower.tail = F) lsmj&lt;-(var(mm)*(nm-1))/qchisq(1-0.05/2,nm-1,lower.tail = F) limj;var(mm);lsmj ## [1] 2776857 ## [1] 5400214 ## [1] 14715187 ########################### #cociente de varianza H/M var(hh)/var(mm)#puntual ## [1] 4.494481 #intervalos lihm&lt;-(var(hh)/var(mm))*qf(1-0.05/2,nm-1,nh-1,lower.tail = F) lshm&lt;-(var(hh)/var(mm))*qf(0.05/2,nm-1,nh-1,lower.tail = F) lihm; var(hh)/var(mm); lshm ## [1] 1.426128 ## [1] 4.494481 ## [1] 12.98477 #99 confiabilidad lihm&lt;-(var(hh)/var(mm))*qf(1-0.01/2,nm-1,nh-1,lower.tail = F) lshm&lt;-(var(hh)/var(mm))*qf(0.01/2,nm-1,nh-1,lower.tail = F) lihm; var(hh)/var(mm); lshm ## [1] 0.9615647 ## [1] 4.494481 ## [1] 18.42446 ########################### #cociente de varianza M/H var(mm)/var(hh)#puntual ## [1] 0.2224951 #intervalos limh&lt;-(var(mm)/var(hh))*qf(1-0.05/2,nh-1,nm-1,lower.tail = F) lsmh&lt;-(var(mm)/var(hh))*qf(0.05/2,nh-1,nm-1,lower.tail = F) limh; var(mm)/var(hh); lsmh ## [1] 0.0770133 ## [1] 0.2224951 ## [1] 0.7011992 "],["prueba-de-hipótesis.html", "4 Prueba de Hipótesis 4.1 Introducción 4.2 Prueba de hipótesis sobre la media 4.3 Prueba de hipótesis sobre la diferencia de medias 4.4 Prueba de hipótesis sobre la proporción 4.5 Prueba de hipótesis sobre la diferencia de Proporciones 4.6 Prueba de hipótesis sobre la varianza 4.7 Prueba de hipótesis sobre la igualdad de varianzas 4.8 Test de normalidad aplicados en R (bondad de ajuste) 4.9 Tamaño de muestra y su relación con el error de tipo I y tipo II", " 4 Prueba de Hipótesis Dado que el principal objetivo de la inferencia estadística es aproximarse al valor de los parámetros (\\(\\theta\\)) mediante un estimador muestral \\(\\hat{\\theta}\\) que viene de una distribución muestral y por lo tanto es una variable aleatoria también. Otra estrategia a parte de la estimación (puntual o por intervalos) del parámetro \\(\\theta\\) es la de plantear hipótesis al rededor de los valores del parámetro. A esto le vamos a denominar una prueba de hipótesis, donde solamente existen dos posibles resultados; Rechazamos la hipótesis o no la rechazamos. En este tema se explora las pruebas de hipótesis estadísticas sobre parámetros comunes, vistos anteriormente: Media Diferencia de medias Proporción Diferencia de proporciones Datos pareados (colección de información de la misma unidad, en diversos momentos del tiempo) Varianza Igualdad de varianza Adicionalmente, al final del tema se vera la pruebas de bondad de ajuste, que se sirven para plantear cuando una serie de datos tiene una determinada distribución Todas las pruebas que se verán están enmarcadas en lo paramétrico, es decir, se realizarán supuestos respecto la distribución de la información y se trabaja con distribuciones conocidas como; la normal, t-student, \\(\\chi^2\\), F. Mencionar que existen las pruebas de hipótesis no paramétricas, estas funcionan sobre las distribuciones libres. 4.1 Introducción 4.1.1 Hipótesis estadística Tener presente que la hipótesis que se defina debe estar siempre en términos del parámetro, no de la muestra. Cuando elaboramos una hipótesis esta tiene dos elementos; la hipótesis que se plantea (hipótesis nula) y el complemento de esta hipótesis (hipótesis alternativa), esta última puede ser de un lado o de dos lados. \\[H_0: \\theta = k\\] \\[H_1: \\theta \\neq k \\quad(\\text{2 lados})\\] \\[H_1: \\theta &lt; k \\quad(\\text{1 lado})\\] \\[H_1: \\theta &gt; k \\quad(\\text{1 lado})\\] 4.1.2 Región de aceptación y región de rechazo Al plantear una hipótesis estadística sobre un parámetro de alguna población, la manera de verificar dicha hipótesis pasara por estudiar una muestra aleatoria sobre la cual se establecerá una regla que nos permita decir si la hipótesis es correcta o no. Tradicionalmente se definirá un estadístico de prueba que nos permitirá decidir. Por ejemplo, estamos interesados en conocer el promedio de ingresos laborales mensuales de las personas que viven en el municipio de La Paz. Planteamos la hipótesis nula: \\[H_0: \\mu_{ingreso}=3000.Bs\\] La hipótesis alternativa \\[H_1: \\mu_{ingreso} \\neq 3000.Bs\\] Imaginemos que tomamos una muestra aleatoria de personas del municipio de La Paz y les consultamos acerca de su ingreso laboral mensual, el resultado del promedio muestral es de \\[\\bar{x}=?\\] Una regla para contrastar una hipótesis es definir la región de aceptación y la región de rechazo, en este ejemplo podemos momentaneamente de forma arbitraria podemos decir que “aceptamos” la \\(H_0\\) si \\[2700\\leq\\bar{x}\\leq 3100\\] En otro caso, se rechaza la hipótesis y la región que esta fuera de la región de aceptación se conoce como región de rechazo (\\(\\bar{x}&lt;2700\\) ó \\(\\bar{x}&gt;3100\\)). Nota: En la práctica, normalmente la región de aceptación para pruebas de 2 lados coincide con el intervalo de confianza. Tener presente los conceptos de: Hipótesis nula Hipótesis alternativa (1 lado, 2 lados) Estadístico de prueba Región de aceptación Región de rechazo Error de tipo I Error de tipo II 4.1.3 Errores de tipo I y errores de tipo II (investigar de que se trata y buscar ejemplos) (Falso positivo) Error de tipo I (\\(\\alpha\\)) (Falso negativo) Error de tipo II (\\(\\beta\\)) 4.1.4 Pruebas bilaterales \\[H_0: \\theta=k\\] \\[H_1: \\theta\\neq k\\] 4.1.5 Pruebas unilaterales Existen de 2 tipos, 4.1.6 Pasos para la prueba hipótesis estadística Plantear la prueba de hipótesis; definir \\(H_0\\) y \\(H_1\\) Establecer el nivel de significancia (\\(\\alpha\\)) de la prueba; el error de tipo I a tolerar, normalmente los valores más usuales son \\(\\alpha=\\{0.01,0.05,0.1\\}\\) Seleccionar y construir el estadístico de prueba (\\(Z\\)) adecuado; se obtiene usando la información de una muestra aleatoria Determinar las regiones de aceptación y rechazo; estas regiones se construyen usando como insumo el paso 2 y 3 Tomar una decisión en base al estadístico de prueba y las regiones de aceptación y rechazo identificadas. * \\(Z \\in RA \\rightarrow \\sim RH_0(AH_0)\\) * \\(Z \\notin RA \\rightarrow RH_0\\) Alternativamente es recomendable calcular el p-valor 4.2 Prueba de hipótesis sobre la media Para las pruebas al rededor de la media, vamos a suponer que los datos de interés se distribuyen como una normal o al menos que la muestra aleatoria para el estadístico de prueba es “grande” y por lo tanto podemos usar el teorema del limite central. Existen dos variaciones para esta prueba; cuando se conoce la varianza y cuando no se conoce 4.2.1 Con varianza conocida Hipótesis \\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\] Nivel de significancia; es \\(\\alpha\\), dado que es una prueba bilateral existen dos regiones de rechazo, cada una de estas 2 regiones con probabilidad \\(\\alpha/2\\) Estadístico de prueba; Se cuenta con una muestra aleatoria (\\(X_1,X_2,\\ldots,X_n\\)) iid de tamaño \\(n\\), de tal forma que cada una de estas variables \\(X_i\\sim .(E(X_i)=\\mu,\\sigma(conocido))\\). La estadística de prueba es: \\[Z_0=\\frac{\\bar{x}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] Este estadístico supone que \\(Z_0\\sim N(0,1)\\), en el supuesto que \\(E[\\bar{x}]=\\mu=\\mu_0\\), este supuesto se cumple siempre y cuando \\(H_0\\) sea cierta Región de aceptación y rechazo curve(dnorm(x),xlim=c(-4,4),main=&quot;significancia del 10%&quot;) # abline(v=c(-1.64,1.64),col=&quot;red&quot;) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) # al 5 % curve(dnorm(x),xlim=c(-4,4),main=&quot;significancia del 5%&quot;) abline(v=c(-1.96,1.96),col=&quot;red&quot;) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) # al 1 % curve(dnorm(x),xlim=c(-4,4),main=&quot;significancia del 1%&quot;) abline(v=c(-2.58,2.58),col=&quot;red&quot;) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) #qnorm(0.01/2) La decisión; Se rechaza la \\(H_0\\) cuando: \\[Z_0&gt;Z_{\\alpha/2} \\quad ó \\quad Z_0&lt;-Z_{\\alpha/2}\\] \\[|Z_0|&gt;Z_{\\alpha/2}\\] Los valores usuales para los \\(Z_{\\alpha/2}\\) son: al 10 de significancia: \\(Z_{\\alpha/2}=1.64\\) al 5 de significancia: \\(Z_{\\alpha/2}=1.96\\) al 1 de significancia: \\(Z_{\\alpha/2}=2.58\\) Ejemplo Se esta estudiando el rendimiento de un proceso químico. De la experiencia previa se sabe que la varianza del rendimiento con este proceso es de 5. Los últimos cinco días de operación de la planta han dado como resultado los siguientes rendimientos: 91.6, 88.75, 90.8, 89.95 y 91.3. Hay razón para creer que el rendimiento promedio es de 90. Tomar un nivel de significancia del 5%. Solución, \\[H_0: \\mu=90 \\quad ; \\quad H_1: \\mu\\neq 90\\] \\[Z_0=\\frac{\\bar{x}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}=\\frac{90.48-90}{\\frac{\\sqrt{5}}{\\sqrt{5}}}=0.48\\] # al 5 % curve(dnorm(x),xlim=c(-4,4),main=&quot;significancia del 5%&quot;) abline(v=c(-1.96,1.96),col=&quot;red&quot;) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) abline(v=0.48,col=&quot;blue&quot;,lty=2) Para la decisión. Recordar que se rechaza \\(H_0\\) si: \\[0.48&gt;1.96 \\quad ó \\quad 0.48&lt;-1.96\\] Por lo tanto no hay evidencia estadística suficiente para rechazar la hipótesis nula. Ejercicio: En una muestra de 35 recien nacidos en un hospital durante el último mes se logró obtener el peso promedio (al nacer) y este fue de 2.9 kgr. Se sabe por registros historicos del hospital que la desviación estándar es de 0.4 Kgr. Se podría decir que el peso promedio de los recien nacidos es de 3.2 kgr. Considerar un nivel de significancia del 5% y del 1%. (10 minutos) \\[H_0: \\mu=3.2 \\quad H_1: \\mu \\neq 3.2\\] \\[Z_0=\\frac{\\bar{x}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}=\\frac{2.9-3.2}{\\frac{0.4}{\\sqrt{35}}}=-4.43\\] Al 5% de significancia: \\[-4.43&gt;1.96 \\quad ó \\quad -4.43&lt;-1.96\\] Al 1% de significancia: \\[-4.43&gt;2.58 \\quad ó \\quad -4.43&lt;-2.58\\] curve(dnorm(x),xlim=c(-5,5),main=&quot;Ejercicio&quot;) abline(v=c(-1.96,1.96),col=&quot;red&quot;) abline(v=c(-2.58,2.58),col=&quot;darkgreen&quot;) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) abline(v=-4.43,col=&quot;blue&quot;,lty=2) Por lo tanto se rechaza la H0 ya sea al 1% o al 5% de significancia. pnorm(-4.43)# pvalor ## [1] 4.711654e-06 Ejemplo, Se tiene un curso de estadística con su evaluación final sobre 100 puntos, se sabe por información pasada que la desviación estándar es alrededor de 10 pts. Se desea probar la hipótesis que el grupo tuvo un promedio de notas de 65 pts. Para realizar la prueba se obtuvo una muestra de 30 estudiantes, con el siguiente resultado: set.seed(1447) x&lt;-round(runif(30,40,80),0) x ## [1] 57 60 59 44 79 59 63 58 68 62 49 77 79 51 78 ## [16] 44 53 58 50 56 64 77 60 49 46 66 45 78 66 60 \\[H0: \\mu=65\\] \\[H1: \\mu \\neq 65\\] El estadístico de prueba es: \\[Z_0=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}=\\frac{60.5-65}{\\frac{10}{\\sqrt{30}}}=-2.465\\] curve(dnorm(x),xlim=c(-4,4),main=&quot;&quot;) abline(v=(60.5-65)/(10/sqrt(30)),col=&quot;blue&quot;,lty=2) abline(v=c(-1.96,1.96),col=&quot;red&quot;) abline(v=c(-1.64,1.64),col=&quot;red&quot;) abline(v=c(-2.58,2.58),col=&quot;red&quot;,lty=2) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) Se rechaza H0 si: \\[-2.465&gt;1.64 (F) \\quad ó \\quad -2.465&lt;-1.64 (V) \\quad 10\\%\\] \\[-2.465&gt;1.96 (F) \\quad ó \\quad -2.465&lt;-1.96(V) \\quad 5\\%\\] \\[-2.465&gt;2.58 (F) \\quad ó \\quad -2.465&lt;-2.58(F) \\quad 1\\%\\] Al 10 y 5 % de significancia se rechaza la hipótesis nula, sin embargo, al 1% no se rechaza. Ejemplo. Se sabe que los diámetros de tornillos tienen una desviación estándar de 0.0001 plg. Una muestra aleatoria de 35 tornillos produce un diámetro promedio de 0.2546 plg. Prueba la hipótesis de que el diámetro medio real es de 0.255 plg. empleando un \\(\\alpha=0.05\\). \\[H_0: \\mu=0.255\\] \\[H_1: \\mu\\neq0.255\\] \\[Z_0=\\frac{0.2546-0.255}{\\frac{0.0001}{\\sqrt{35}}}=-23.66\\] Se rechaza \\(H_0\\) si: \\[-23.66&gt;1.96 \\quad ó \\quad -23.66&lt;-1.96\\] Por lo tanto rechazamos la hipótesis nula de que la media de diámetro de los tornillos sea de 0.255 plg. 4.2.1.1 Para pruebas unilaterales Para las pruebas unilaterales se utiliza el mismo estadístico de prueba \\(Z_0\\), con las siguientes hipótesis y región de rechazo. \\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu&gt;\\mu_0\\] Se rechaza \\(H_0\\) si: \\[Z_0&gt;Z{\\alpha}\\] \\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu &lt; \\mu_0\\] Se rechaza \\(H_0\\) si: \\[Z_0&lt;-Z{\\alpha}\\] Los valores usuales para los \\(Z_{\\alpha}\\) de pruebas unilaterales. \\(Z_{\\alpha=0.1}=1.28\\) \\(Z_{\\alpha=0.05}=1.64\\) \\(Z_{\\alpha=0.01}=2.33\\) par(mfrow=c(1,2)) curve(dnorm(x),xlim=c(-4,4),main=&quot;HA: u&gt;u0&quot;) abline(v=c(1.28,1.64,2.33),col=&quot;red&quot;) text(c(-2,3),c(0.3,0.3),c(&quot;Aceptación&quot;,&quot;Rechazo&quot;)) text(c(1.28,1.64,2.33),rep(0.35,3),paste0(c(10,5,1),&quot;%&quot;),cex=0.7,col=&quot;blue&quot;) curve(dnorm(x),xlim=c(-4,4),main=&quot;HA: u&lt;u0&quot;) abline(v=-c(1.28,1.64,2.33),col=&quot;red&quot;) text(c(2,-3),c(0.3,0.3),c(&quot;Aceptación&quot;,&quot;Rechazo&quot;)) text(-c(2.33,1.64,1.28),rep(0.35,3),paste0(c(1,5,10),&quot;%&quot;),cex=0.7,col=&quot;blue&quot;) dev.off() ## null device ## 1 Ejercicio: Para el caso de los recien nacidos constrastar ambas pruebas unilaterales con un nivel de significancia del 1%. ¿Qué se puede concluir?. \\[H_0: \\mu=3.2 \\quad H_1: \\mu&gt;3.2\\] Recordar que se rechaza H0 si: \\[Z_0&gt;Z_{\\alpha} \\rightarrow -4.44&gt;2.33\\] Por lo tanto, no se rechaza H0 \\[H_0: \\mu=3.2 \\quad H_1: \\mu&lt;3.2\\] Recordar que se rechaza H0 si se cumple: \\[Z_0&lt;-Z_{\\alpha} \\rightarrow -4.44&lt; -2.33\\] Por lo tanto, se rechaza H0. 4.2.2 Con varianza desconocida Si el tamaño de muestra fuera mayor 30, todo lo visto anteriormente se mantiene, el único cambio se da al momento de calcular el estadístico de prueba donde en lugar de \\(\\sigma\\) tomamos la varianza muestral \\(S^2\\) \\[Z_0=\\frac{\\bar{x}-\\mu_0}{\\frac{S}{\\sqrt{n}}}\\] Si la muestra no es mayor 30, podemos suponer que los datos son normales y la aproximación para la región de aceptación y rechazo vienen de una t-student. Estadístico de prueba \\[t_0=\\frac{\\bar{X}-\\mu_0}{\\frac{S}{\\sqrt{n}}}\\] Región de aceptación y rechazo; si \\(H_0\\) es cierta, entonces: \\[t_0\\sim t(n-1)\\] La decisión; Se rechaza la \\(H_0\\) cuando: \\[t_0&gt;t_{\\alpha/2,n-1} \\quad ó \\quad t_0&lt;-t_{\\alpha/2,n-1}\\] Hasta ahora se tienen 3 casos: Varianza conocida: Se usa el \\(Z_0\\) empleando \\(\\sigma\\), las regiones críticas se arman con base a la distribución normal (Se asume normalidad en los datos) Varianza desconocida, con \\(n&gt;30\\): Se usa el \\(Z_0\\) empleando \\(S\\), las regiones críticas se arman con base a la distribución normal Varianza desconocida, con \\(n\\leq30\\): Se usa el \\(t_0\\) empleando \\(S\\), las regiones críticas se arman con base a la distribución t-student. (Se asume normalidad en los datos) Ejercicio: Se tiene una muestra aleatoria de 7 plantas del mismo tipo, y se mide el largo de su tallo. Los resultados en cm. fueron: 5, 7, 7, 5, 6, 9 y 4. El largo del tallo es una medida que permite evaluar cuando cambiar de ambientes las plantas, esto debe suceder si el tallo supera los 7 cm. Realizar una prueba de hipótesis estadística. Solución: \\[H_0: \\mu = 7 \\quad H_1: \\mu&gt;7\\] \\[t_0=\\frac{\\bar{x}-\\mu_0}{\\frac{S}{\\sqrt{n}}}=\\frac{6.14-7}{0.63}=-1.35\\] Se rechaza la hipótesis nula si el estadístico de prueba cumple con (vamos a usar un \\(\\alpha=0.05\\)): \\[t_0&gt;t_{\\alpha,n-1} \\rightarrow \\quad -1.35&gt;t_{0.05,6}=1.943\\] Por lo tanto no se rechaza la H0. Ejemplo, Para el caso de las notas finales, probar la hipótesis suponiendo varianza desconocida y usar \\(z_0\\) y en otro caso usando la \\(t_0\\). Al 5% evaluar si se rechaza H0 sd(x);length(x);mean(x) ## [1] 11.05706 ## [1] 30 ## [1] 60.5 \\[z_0=\\frac{\\bar{X}-\\mu_0}{\\frac{\\hat{S}}{\\sqrt{n}}}=\\frac{60.5-65}{\\frac{11.057}{\\sqrt{30}}}=-2.23\\] Así como \\(z_0=-2.23&lt;-1.96\\) se rechaza H0. \\[t_0=\\frac{\\bar{X}-\\mu_0}{\\frac{\\hat{S}}{\\sqrt{n}}}=-2.23\\] Para rechazar H0, \\[-2.23&gt;2.045=t_{\\alpha/2,n-1} \\quad ó \\quad -2.23&lt;-2.045=-t_{\\alpha/2,n-1}\\] curve(dt(x,29),xlim = c(-4,4),main=&quot;t(n-1=29)&quot;) ta&lt;-qt(0.05/2,30-1,lower.tail = F) abline(v=c(-ta,ta),col=&quot;red&quot;) Por lo tanto se rechaza H0 4.3 Prueba de hipótesis sobre la diferencia de medias Esta prueba se utiliza principalmente cuando se contrasta valores de 2 poblaciones independientes. Las hipótesis son: \\[H_0: \\mu_1=\\mu_2\\] \\[H_1: \\mu_1\\neq\\mu_2\\] Suponemos que se extrae muestras aleatorias de cada una de las poblaciones de tamaño \\(n_1\\) y \\(n_2\\) respectivamente. Se supone que cada una de las muestras las poblaciones se distribuyen iid con media desconocida y varianza conocida. Si suponemos que la muestra en ambas poblaciones son grandes \\(n_1,n_2&gt;30\\) usando el teorema central del limite podemos afirmar en base a los capítulos anteriores \\[\\bar{x}_1-\\bar{x}_2\\sim N \\left(\\mu_1-\\mu_2,\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}\\right)\\] Por lo tanto se puede plantear como estadístico de prueba si \\(H_0\\) es cierta: \\[Z_0=\\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}}\\sim N(0,1)\\] Finalmente, en base a la distribución de \\(Z_0\\), se rechaza la igualdad de medias cuando: \\[Z_0&gt;Z_{\\alpha/2} \\quad ó \\quad Z_0&lt; -Z_{\\alpha/2} \\] Cuando la varianza no es conocida pero el tamaño de muestra es mayor a 30 para ambas poblaciones, el estadístico de prueba es el único que cambia \\[Z_0=\\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{\\frac{S_1^2}{n_1}+\\frac{S_2^2}{n_2}}}\\sim N(0,1)\\] Cuando \\(H_1: \\mu_1&gt;\\mu2\\) se rechaza la \\(H_0\\) cuando: \\[Z_0&gt;Z_{\\alpha}\\] Cuando \\(H_1: \\mu_1&lt;\\mu2\\) se rechaza la \\(H_0\\) cuando: \\[Z_0&lt;-Z_{\\alpha}\\] Cuando la muestra es menor o igual 30 en una o ambas poblaciones, se debe hacer el supuesto que los datos son normales y aproximar la distribución a una \\(t-student\\). En este caso el estadístico de prueba es: \\[t_0=\\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{\\frac{S_1^2}{n_1}+\\frac{S_2^2}{n_2}}}\\] Lo anterior para el estadístico \\(t_0\\) es valido bajo el supuesto de normalidad de los datos y las varianzas de las poblaciones no son iguales \\(\\sigma_1 \\neq \\sigma_2\\). En el caso donde suponemos igualdad de varianzas \\(\\sigma_1 = \\sigma_2\\). El estadístico de prueba es: \\[t_0=\\frac{\\bar{x}_1-\\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\] Donde: \\[S^2_p=\\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}\\] Nota: Esta \\(S^2_p\\) es una varianza ponderada que emplea información de las varianzas muestrales, en el supuesto que \\(\\sigma_1=\\sigma_2=\\sigma\\) Entonces, para el caso de igualdad de varianza se rechaza la \\(H_0\\) cuando: \\[t_0&gt;t_{\\alpha/2,n_1+n_2-2} \\quad ó \\quad t_0&lt;-t_{\\alpha/2,n_1+n_2-2}\\] Cuando las varianzas no son iguales, se rechaza la \\(H_0\\) cuando: \\[t_0&gt;t_{\\alpha/2,v} \\quad ó \\quad t_0&lt;-t_{\\alpha/2,v}\\] Donde: \\[v=\\frac{\\frac{S_1^2}{n_1}+\\frac{S_2^2}{n_2}}{\\frac{(S^2_1/n_1)^2}{n_1+1}+\\frac{(S^2_2/n_2)^2}{n_2+1}}-2\\] Ejemplo. Dos tipos de plásticos son apropiados para que los utilice un fabricante de componentes electrónicos. La resistencia al rompimiento de estos plásticos es importante. Se sabe que \\(\\sigma_1=\\sigma_2=1.0\\) psi. De una muestra aleatoria de tamaño \\(n_1 = 10\\) y \\(n_2 = 12\\) obtenemos \\(\\bar{X}_1 = 162.5\\) y \\(\\bar{X}_2 = 155.0\\). La compañía no adoptará el plástico 1 a menos que su resistencia al rompimiento exceda la del plástico 2 al menos por 10 psi. De acuerdo con la información de la muestra, ¿debe utilizarse el plástico I? Solución, vamos a suponer normalidad en los datos y tomar un \\(\\alpha=0.05\\). Como el objetivo es verificar si la resistencia del plástico 1 supera en más de 10 psi al plástico 2, se plantea el siguiente cambio: Sea \\(X_2\\) la va de la resistencia del plástico 2: \\[Y=X_2+10\\] La estimación de la media de \\(y\\) es: \\[\\bar{Y}=\\bar{X}_2+10=155.0+10=165\\] \\[H_0: \\mu_1=\\mu_y \\quad ; \\quad H_1: \\mu_1&gt;\\mu_y\\] \\[Z_0=t_0=\\frac{\\bar{X_1}-\\bar{Y}}{\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_y^2}{n_2}}}=\\frac{162.5-165}{\\sqrt{\\frac{1}{10}+\\frac{1}{12}}}=-5.84\\] Suponiendo que \\(Z_0\\sim N(0,1)\\): Se rechaza \\(H_0\\) si se cumple: \\[Z_0&gt; Z_{\\alpha=0.05}\\quad ; \\quad -5.84&gt;1.64(F)\\] Suponiendo que \\(t_0\\sim t(n_1+n_2-2)\\): Se rechaza \\(H_0\\) si se cumple: \\[t_0&gt; t_{\\alpha=0.05,20}\\quad ; \\quad -5.84&gt;1.72(F)\\] Por lo tanto, no existe evidencia estadística suficiente para rechazar la \\(H_0\\), entonces, no se debe tomar el plástico 1. #z0 normal curve(dnorm(x),xlim=c(-6,6),xlab=&quot;&quot;,lwd=2) abline(v=1.64,col=&quot;red&quot;,lwd=2) text(c(-1,2.5),c(0.3,0.3),c(&quot;RA&quot;,&quot;RR&quot;),col=&quot;blue&quot;) #t0 t-student curve(dt(x,20),add=T,col=&quot;blue&quot;,lty=2) abline(v=1.72,col=&quot;blue&quot;) abline(v=-5.84,col=&quot;darkgreen&quot;,lty=2) text(-5.84,0.3,&quot;Z0,t0&quot;) Ejercicio: Se tiene una muestra de 80 postulantes que dieron un examen de ingreso a una universidad, este grupo fue separado en dos aulas cada una de 40 postulantes, los resultados de la prueba fueron: \\[\\bar{x}_a=69 \\quad \\bar{x}_b=73 \\quad S_a=7 \\quad S_b=6.8 \\] ¿Se puede concluir que el promedio de notas en ambos grupos son iguales?. Solución: \\[H_0: \\mu_a=\\mu_b \\quad H_1: \\mu_a \\neq \\mu_b\\] \\[Z_0=\\frac{\\bar{x}_a-\\bar{x}_b}{\\sqrt{\\frac{S_a^2}{n_a}+\\frac{S_b^2}{n_b}}}=\\frac{69-73}{\\sqrt{\\frac{7^2}{40}+\\frac{6.8^2}{40}}}=-2.59\\] Vamos a suponer un \\(\\alpha=0.05\\). Se rechaza la H0 si se cumple: \\[-2.59&gt;1.96 \\quad ó \\quad -2.59&lt;-1.96\\] Por lo tanto, se rechaza H0. 4.4 Prueba de hipótesis sobre la proporción Partimos de las siguientes hipótesis: \\[H_0: P=P_0\\] \\[H_1: P \\neq P_0\\] \\[H_1: P &lt; P_0\\] \\[H_1: P &gt; P_0\\] Como se vio en el capítulo anterior podemos suponer que tenemos una muestra aleatoria de tamaño \\(n\\) donde las variables de esta muestra \\(X\\sim N(nP,n*P*(1-P))\\), entonces si la hipótesis nula es cierta entonces \\(X\\sim N(nP_0,n*P_0*(1-P_0))\\). Así el estadístico de prueba es la estandarización de \\(X\\). \\[Z_0=\\frac{\\hat{P}-P_0}{\\sqrt{\\frac{P_0*(1-P_0)}{n}}}\\sim N(0,1)\\] De esta forma en una prueba bilateral se rechaza la \\(H_0\\) si \\[Z_0&lt; - Z_{\\alpha/2} \\quad ó \\quad Z_0&gt; Z_{\\alpha/2}\\] Para las pruebas unilaterales, con \\(H_1: P&gt; P_0\\), se rechaza \\(H_0\\) cuando: \\[Z_0&gt;Z_\\alpha\\] Con \\(H_1: P&lt; P_0\\), se rechaza \\(H_0\\) cuando: \\[Z_0&lt;-Z_\\alpha\\] Ejemplo: Se realizó una encuesta aleatoria a 120 personas para determinar el apoyo a un candidato en las proximas elecciones municipales, en esta muestra 59 personas confirman su apoyo al candidato. Es posible decir si el candidato superara el apoyo del 50%. \\[H_0: P=0.50 \\quad H_1: P&gt;0.50\\] Con la muestra de 120 personas se puede estimar la proporción de apoyo: \\[\\hat{P}=\\frac{59}{120}=0.4917\\] \\[Z_0=\\frac{\\hat{P}-P_0}{\\sqrt{\\frac{P_0*(1-P_0)}{n}}}=\\frac{0.4917-0.5}{\\sqrt{\\frac{0.5*0.5}{120}}}=-0.1818\\] Vamos a suponer un \\(\\alpha=0.05\\). Se rechaza H0 si se cumple: \\[Z_0&gt;Z_\\alpha \\quad -0.1818&gt;1.64\\] Por lo tanto no se rechaza la H0. Esto significa que es posible que el candidato no supere el 50% de apoyo. Ejemplo De 1000 casos seleccionados de cáncer en el pulmón, 823 terminaron en muerte. ¿Se podría decir que el porcentaje de muertes es del 90%, o mayor a este valor?. Solución, el parámetro de interés es la proporción de muertes por el cáncer de pulmón (\\(P\\)) \\[H_0: P=0.9\\] \\[H_1: P\\neq0.9\\] \\[H_2: P &gt; 0.9\\] \\[H_3: P &lt; 0.9\\] El estadístico de prueba: \\[Z_0=\\frac{X-nP_0}{\\sqrt{n*P_0*(1-P_0)}}=\\frac{823-1000*0.9}{\\sqrt{1000*0.9*0.1}}=-8.12\\] Una alternativa para el calculo a este estadístico de prueba es: \\[Z_0=\\frac{\\hat{P}-P_0}{\\sqrt{\\frac{P_0*(1-P_0)}{n}}}=\\frac{0.823-0.90}{\\sqrt{\\frac{0.9*0.1}{1000}}}=-8.12\\] Para la prueba bilateral a un \\(\\alpha=0.05\\): \\[-8.11&lt; - 1.96 (V) \\quad ó \\quad -8.11&gt; 1.96 (F)\\] Por lo tanto se rechaza la \\(H_0\\). Para \\(H_2: P&gt;0.9\\), \\[-8.11&gt;1.64 (F)\\] No hay evidencia estadística para rechazar \\(H_0\\) y aceptar \\(H_2\\). Para \\(H_3: P&lt;0.9\\), \\[-8.11&lt; - 1.64 (V)\\] Rechazar \\(H_0\\) y “aceptar” \\(H_3\\). Ejercicio Se tomo una muestra de 40 personas al azar en un supermercado y se encontró que 25% de ellas tenían COVID-19, ¿es posible afirmar que más del 30% de las personas tienen COVID-19 en el supermercado? (tomar un 5% de significancia). Solución, \\[H_0: P = 0.30 \\quad ; H_1: P&gt;0.30\\] \\[Z_0=\\frac{\\hat{P}-P_0}{\\sqrt{\\frac{P_0*(1-P_0)}{n}}}=\\frac{0.25-0.3}{\\sqrt{\\frac{0.3*0.7}{40}}}=-0.69\\] Se rechaza la \\(H_0\\) si se cumple: \\[Z_0&gt; Z_{0.05} \\quad -0.69&gt;1.64 (F)\\] No existe evidencia estadística suficiente para rechazar la hipótesis nula. Solución alternativa, \\[H_0: P=0.3\\] \\[H_1: P&gt;0.3\\] \\[Z_0=\\frac{X-nP_0}{\\sqrt{n*P_0*(1-P_0)}}=\\frac{\\hat{P}-P_0}{\\sqrt{\\frac{P_0*(1-P_0)}{n}}}\\] \\[Z_0=\\frac{10-40*0.3}{\\sqrt{40*0.3*0.7}}=\\frac{0.25-0.3}{\\sqrt{\\frac{0.3*0.7}{40}}}=-0.69\\] Se rechaza \\(H_0\\) si se cumple: \\[-0.69&gt; 1.64\\] Por lo que no se rechaza \\(H_0\\), lo que implica que el porcentaje de personas con COVID-19 en el supermercado no es superior a 30% 4.5 Prueba de hipótesis sobre la diferencia de Proporciones En este caso el supuesto principal es que se trabajan con 2 poblaciones independientes y sobre ellas se busca contrastar el valor de sus proporciones de alguna clase en particular. \\[H_0: P_1=P_2\\] \\[H_1: P_1 \\neq P_2\\] \\[H_1: P_1 &gt; P_2\\] \\[H_1: P_1 &lt; P_2\\] Se parte con dos muestras aleatorias de tamaños \\(n_1\\) y \\(n_2\\) de ambas poblaciones, y sea \\(X_1\\), \\(X_2\\) el número de observaciones que pertenecen a la clase de interés (total clase) en las muestras. Si aproximamos a \\(X_1\\) y \\(X_2\\) como normales al ser estas binomiales, si se trabaja a nivel de los estimadores, sean \\(\\hat{P}_1\\) y \\(\\hat{P}_2\\) se sabe que: \\[Z_0=\\frac{\\hat{P_1}-\\hat{P_2}}{\\sqrt{\\hat{P}(1-\\hat{P})*\\left[\\frac{1}{n_1}+\\frac{1}{n_2} \\right]}}\\sim N(0,1)\\] Recordando que si \\(n&gt;30\\) \\(\\hat{P_1}-\\hat{P_2}\\sim N(P_1-P_2,\\sigma_{\\hat{P_1}-\\hat{P_2}})\\) \\[\\hat{P}=\\frac{X_1+X_2}{n_1+n_2}=\\frac{n_1\\hat{P}_1+n_2*\\hat{P}_2}{n_1+n_2}\\] De esta forma en una prueba bilateral se rechaza la \\(H_0\\) si \\[Z_0&lt; - Z_{\\alpha/2} \\quad ó \\quad Z_0&gt; Z_{\\alpha/2}\\] Para las pruebas unilaterales, con \\(H_1: P_1&gt; P_2\\), se rechaza \\(H_0\\) cuando: \\[Z_0&gt;Z_\\alpha\\] Con \\(H_1: P_1&lt; P_2\\), se rechaza \\(H_0\\) cuando: \\[Z_0&lt;-Z_\\alpha\\] Nota: El supuesto principal para las pruebas de hipótesis de la proporción y la diferencia de proporciones es que el total clase (\\(X\\)) se distribuye como un normal o en su defecto los tamaños de muestra superan los 30, ya que esto permite utilizar el teorema del limite central. Existen casos muy particulares cuando se trabajan con muestras pequeñas \\(n&lt;20\\) es una alternativa aproximar las regiones críticas mediante una distribución \\(t-student\\). Ejercicio: Se tiene el dato de una muestra de 60 nacimientos por hospital en 2 hospitales, donde se cuenta con el peso en kg de estos nacimientos, si el peso es inferior a 2 kg representa una alerta, en el primer hospital se registraron 5 nacimientos inferiores a 2 kg y en el segundo se registraron 7 nacimientos. Se puede decir que el porcetaje de nacimientos inferiores a 2 kg es el mismo en los 2 hospitales. Solución: \\[H_0: P_1=P_2 \\quad H_1: P_1\\neq P_2\\] \\[\\hat{P}_1=\\frac{5}{60}=0.0833 \\quad \\hat{P}_2=\\frac{7}{60}=0.117 \\quad \\hat{P}=\\frac{12}{120}=0.1\\] \\[Z_0=\\frac{\\hat{P_1}-\\hat{P_2}}{\\sqrt{\\hat{P}(1-\\hat{P})*\\left[\\frac{1}{n_1}+\\frac{1}{n_2} \\right]}}=\\frac{-\\frac{2}{60}}{\\sqrt{0.1*0.9*\\frac{1}{30}}}=-0.6086\\] Como no se cumple: \\[Z_0&gt;1.96 \\quad Z_0&lt;-1.96\\] No hay evidencia estadístca suficientes para rechazar la H0 Ejercicio. Dos tipos diferentes de maquinas de moldeo de inyección se utilizan para formar piezas plásticas. Una parte se considera defectuosa si tiene un encogimiento excesivo o si se decolora. Se selecciona 2 muestras aleatorias, cada una de tamaño 500, y se encuentran 21 piezas defectuosas en la muestra de la máquina 2. ¿Es razonable concluir que ambas máquinas producen la misma fracción de piezas defectuosas? Solución, \\[Z_0=\\frac{\\hat{P_1}-\\hat{P_2}}{\\sqrt{\\hat{P}(1-\\hat{P})*\\left[\\frac{1}{n_1}+\\frac{1}{n_2} \\right]}}\\sim N(0,1)\\] \\[\\hat{P}=\\frac{X_1+X_2}{n_1+n_2}=\\frac{n_1\\hat{P}_1+n_2*\\hat{P}_2}{n_1+n_2}\\] Solución, \\[H_0: P_1=P_2\\] \\[H_1: P_1 \\neq P_2\\] \\[Z_0=\\frac{\\hat{P_1}-\\hat{P_2}}{\\sqrt{\\hat{P}(1-\\hat{P})*\\left[\\frac{1}{n_1}+\\frac{1}{n_2} \\right]}}=\\frac{\\frac{X_1}{500}-\\frac{27}{500}}{\\sqrt{\\frac{X_1+27}{1000}(1-\\frac{X_1+27}{1000})*\\left[\\frac{1}{500}+\\frac{1}{500} \\right]}}\\] La pregunta es cuando se rechaza \\(H_0\\), esto sucede cuando: \\[Z_0&lt; - 1.96 \\quad ó \\quad Z_0&gt; 1.96\\] Despejar \\(X_1\\) y encontrar el rango de esta para mantener \\(H_0\\). ee&lt;-function(x){ nn&lt;-(1/500)*(x-21) dd&lt;-sqrt(((x+21)/1000)*(1-((x+21)/1000))*(1/250)) z0&lt;-nn/dd return(z0) } plot(0:500,ee(0:500)) abline(h=c(1.96,-1.96),col=&quot;red&quot;) 4.6 Prueba de hipótesis sobre la varianza Para esta prueba estamos interesados en plantear hipótesis sobre el valor de la varianza poblacional, muchas veces esta necesidad esta vinculada a conocer la variabilidad de la información. Recordar que la varianza poblacional se define como: \\[\\sigma^2=V(x)=\\frac{\\sum_U (x_i-\\mu_x)^2}{N}\\] Sea: \\[H_0: \\sigma^2=\\sigma^2_0\\] \\[H_1: \\sigma^2 \\neq \\sigma^2_0\\] Vamos a suponer que la variable \\(X\\sim N(\\mu,\\sigma^2)\\), tanto \\(\\mu\\) y \\(\\sigma^2\\) no se conocen, Suponer que se toma una muestra aleatoria de tamaño \\(n\\), \\(X_1, X_2, \\ldots,X_n\\). La estadística de prueba es: \\[\\chi^2_0=\\frac{(n-1)S^2}{\\sigma^2_0}=\\sum_s\\left(\\frac{x_i-\\bar{x}}{\\sigma_0}\\right)^2\\] Si \\(H_0\\) es cierta, entonces: \\[\\chi^2_0\\sim \\chi^2(n-1)\\] Finalmente, se rechaza \\(H_0\\) si: \\[\\chi^2_0&gt;\\chi^2_{\\alpha/2,n-1}\\quad ó \\quad \\chi^2_0&lt;\\chi^2_{1-\\alpha/2,n-1}\\] Como ejemplo, la zona de aceptación y rechazo para una muestra de \\(n=20\\) y \\(\\alpha=0.05\\) es: curve(dchisq(x,19),xlim=c(0,80),main=expression(paste(chi^{2},&quot;(n-1=19)&quot;))) x1&lt;-qchisq(0.05/2,19,lower.tail = F) x2&lt;-qchisq(1-0.05/2,19,lower.tail = F) abline(v=c(x1,x2),col=&quot;red&quot;,lty=2) text(c(x1,x2),c(0.02,0.02),round(c(x1,x2),2),col=&quot;blue&quot;) pchisq(x1,19)-pchisq(x2,19) ## [1] 0.95 Para las pruebas unilaterales, \\[H_0: \\sigma^2=\\sigma^2_0\\] \\[H_1: \\sigma^2&gt;\\sigma^2_0\\] Se rechaza \\(H_0\\) si: \\[\\chi^2_0&gt; \\chi^2_{\\alpha,n-1}\\] \\[H_0: \\sigma^2=\\sigma^2_0\\] \\[H_1: \\sigma^2&lt;\\sigma^2_0\\] Se rechaza \\(H_0\\) si: \\[\\chi^2_0 &lt; \\chi^2_{1-\\alpha,n-1}\\] En el R para el ejemplo anterior. #&gt; qchisq(0.05,19,lower.tail = F) ## [1] 30.14353 #&lt; qchisq(1-0.05,19,lower.tail = F) ## [1] 10.11701 par(mfrow=c(1,2)) curve(dchisq(x,19),xlim=c(0,80),main=&quot;H_1: sigma&gt;sigma_0&quot;) abline(v=qchisq(0.05,19,lower.tail = F),col=&quot;blue&quot;) curve(dchisq(x,19),xlim=c(0,80),main=&quot;H_1: sigma&lt;sigma_0&quot;) abline(v=qchisq(1-0.05,19,lower.tail = F),col=&quot;blue&quot;) Ejemplo El fabricante de una fuente de poder esta interesado en la variabilidad del voltaje de salida. Ha probado 12 unidades, elegidas al azar, con los siguientes resultados: 5.34, 5.00, 5.07, 5.25, 5.65, 5.55, 5.35, 5.35, 4.76, 5.54, 5.44, 4.61 Probar que la variabilidad es de 0.5. Tomar una significancia del 5% Solución. \\[H_0: \\sigma^2=0.5 \\quad ; \\quad H_1: \\sigma^2 \\neq 0.5\\] \\[\\chi^2_0=\\frac{(n-1)S^2}{\\sigma^2_0}=\\frac{11*0.1038}{0.5}=2.28\\] dev.off() ## null device ## 1 curve(dchisq(x,11),xlim=c(0,80),main=expression(paste(chi^{2},&quot;(n-1=11)&quot;))) x1&lt;-qchisq(0.05/2,11,lower.tail = F) x2&lt;-qchisq(1-0.05/2,11,lower.tail = F) abline(v=c(x1,x2),col=&quot;red&quot;,lty=2) text(c(x1,x2),c(0.02,0.02),round(c(x1,x2),2),col=&quot;blue&quot;) abline(v=2.28,lty=2,col=&quot;darkgreen&quot;) se rechaza \\(H_0\\) si: \\[2.28&gt;21.92 (F)\\quad ó \\quad 2.28&lt;3.82 (V)\\] Por lo tanto se rechaza la H0. 4.7 Prueba de hipótesis sobre la igualdad de varianzas La idea inicial es contar con dos poblaciones, sobre la cuales se observa una variable \\(X_1\\) para la población uno y \\(X_2\\) para la población dos. Suponer que \\(X_1\\sim N(\\mu_1,\\sigma^2_1)\\), \\(X_2\\sim N(\\mu_2,\\sigma^2_2)\\), para las dos poblaciones no se conocen sus parámetros. Si tomamos muestras aleatorias de ambas poblaciones, con \\(n_1\\) y \\(n_2\\) los tamaños de muestra respectivos, la prueba de igualdad de varianzas es: \\[H_0: \\sigma^2_1=\\sigma^2_2\\] \\[H_1: \\sigma^2_1\\neq\\sigma^2_2\\] El estadístico de prueba es: \\[F_0=\\frac{{S}^2_1}{{S}^2_2}\\] Si \\(H_0\\) es cierta, entonces: \\[F_0\\sim F(n_1-1,n_2-1)\\] Finalmente, se rechaza \\(H_0\\), cuando: \\[F_0&gt;F_{\\alpha/2,n_1-1,n_2-1} \\quad ó\\quad F_0&lt;F_{1-\\alpha/2,n_1-1,n_2-1}\\] En el R, para dos muestras de tamaño 20 con una significancia del 5%. curve(df(x,19,19),xlim=c(0,4),main=&quot;F(19,19)&quot;) x1&lt;-qf(0.05/2,19,19,lower.tail = F) x2&lt;-qf(1-0.05/2,19,19,lower.tail = F) abline(v=c(x1,x2),col=&quot;red&quot;,lty=2) text(c(x1,x2),c(0.4,0.4),round(c(x1,x2),2),col=&quot;blue&quot;) pf(x1,19,19)-pf(x2,19,19) ## [1] 0.95 qf(0.05/2,10,10,lower.tail = F) ## [1] 3.716792 qf(1-0.05/2,10,10,lower.tail = F) ## [1] 0.2690492 j&lt;-1 curve(df(x,19,19),xlim=c(0,4),ylim=c(0,4)) for(i in seq(20,1000,10)){ curve(df(x,i,i),col=j,add = T) j&lt;-j+1 } Para las pruebas unilaterales, \\[H_0: \\sigma^2_1=\\sigma^2_2\\] \\[H_1: \\sigma^2_1&gt;\\sigma^2_2\\] Se rechaza \\(H_0\\) si: \\[F_0&gt;F_{\\alpha,n_1-1,n_2-1}\\] Para las pruebas unilaterales, \\[H_0: \\sigma^2_1=\\sigma^2_2\\] \\[H_1: \\sigma^2_1&lt;\\sigma^2_2\\] Se rechaza \\(H_0\\) si: \\[F_0&lt;F_{1-\\alpha,n_1-1,n_2-1}\\] En el R, para el ejemplo anterior. qf(0.05,19,19,lower.tail = F)#sigma1&gt;sigma2 ## [1] 2.168252 qf(1-0.05,19,19,lower.tail = F)#sigma1&lt;sigma2 ## [1] 0.4612011 par(mfrow=c(1,2)) curve(df(x,19,19),xlim=c(0,4),main=&quot;H_1: sigma1&gt;sigma2&quot;) abline(v=qf(0.05,19,19,lower.tail = F),col=&quot;blue&quot;) curve(df(x,19,19),xlim=c(0,4),main=&quot;H_1: sigma1&lt;sigma2&quot;) abline(v=qf(1-0.05,19,19,lower.tail = F),col=&quot;blue&quot;) dev.off() ## null device ## 1 Nota: Tanto para la prueba de varianza e igualdad de varianzas el supuesto principal es que los datos originales se distribuyen como una normal. 4.8 Test de normalidad aplicados en R (bondad de ajuste) Una manera rápida y más visual es simulando una muestra muy grande de los datos originales (re muestreo) xr&lt;-sample(x,1000,replace = T) hist(xr) plot(density(xr)) Una alternativa más formal es usar un test estadístico de normalidad. En R se puede usar el test de Shapiro o Kolmogorov Smirnov. \\[H_0: x\\sim N(.)\\] x ## [1] 57 60 59 44 79 59 63 58 68 62 49 77 79 51 78 ## [16] 44 53 58 50 56 64 77 60 49 46 66 45 78 66 60 shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.92896, p-value = 0.04609 y&lt;-runif(1000) shapiro.test(y) ## ## Shapiro-Wilk normality test ## ## data: y ## W = 0.95518, p-value &lt; 2.2e-16 El p-valor se puede entender como la probabilidad que la hipótesis nula sea cierta. Con el p-valor no se rechaza la hipótesis nula si este no supera la significancia. Es decir, se rechaza la \\(H_0\\) si: \\[P-valor&lt;\\alpha\\] 4.9 Tamaño de muestra y su relación con el error de tipo I y tipo II Recordar que el error de tipo 1 denotado por \\(\\alpha\\) es el falso positivo o también establece el riesgo de rechazar algo que es verdadero. Mientras que el error de tipo 2, denotado por \\(\\beta\\) es el falso negativo, que establece el riesgo de aceptar algo que es falso. Para la prueba de hipótesis para la media, la formula para el tamaño de muestra es: Bilateral, \\[n=\\frac{(Z_{\\alpha/2}+Z_\\beta)^2 \\sigma^2}{\\delta^2 }\\] Unilaterales \\[n=\\frac{(Z_{\\alpha}+Z_\\beta)^2 \\sigma^2}{\\delta^2 }\\] Los valores más comunes para el error de tipo 2 \\(\\beta\\) son: (20%) \\(Z_{\\beta=20}=0.84\\) (10%) \\(Z_{\\beta=10}=1.28\\) (5%) \\(Z_{\\beta=5}=1.64\\) Ejemplo: Se está estudiando el rendimiento de un proceso químico. De la experiencia previa se sabe que la varianza del rendimiento con este proceso es 5. Los últimos cinco días de operación de la planta han dado como resultado los siguientes rendimientos (en porcentajes): \\[91.6, 88.75, 90.8, 89.95, 91.3\\] ¿Hay razón para creer que el rendimiento es menor al 90%? ¿Que tamaño de muestra se requeriría para detectar un rendimiento medio verdadero de 85% con probabilidad de 0.95? Solución, \\[H_0: \\mu=90\\] \\[H_1: \\mu&lt;90\\] \\[Z_0=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}=\\frac{90.48-90}{\\frac{2.24}{\\sqrt{5}}}=0.48\\] Se rechaza \\(H_0\\) si (a un \\(\\alpha=0.05\\)): \\[0.48&lt;-Z_{\\alpha}=-1.64 (F)\\] Por lo tanto no hay evidencia estadística para rechazar \\(H_0\\). \\[n=\\frac{(Z_{\\alpha}+Z_\\beta)^2 \\sigma^2}{\\delta^2 }=\\frac{(1.64+1.64)^2 5}{(-5)^2 }=2.15 \\approx 3\\] De forma análoga en las otras pruebas estadísticas se tienen los tamaños de muestra: Para la diferencia de medias Bilateral \\[n=\\frac{(Z_{\\alpha/2}+Z_\\beta)^2 (\\sigma_1^2+\\sigma_2^2)}{\\delta^2 }\\] \\[n=\\frac{(Z_{\\alpha}+Z_\\beta)^2 (\\sigma_1^2+\\sigma_2^2)}{\\delta^2 }\\] Para la proporción, Bilateral \\[n=\\left(\\frac{Z_{\\alpha/2} \\sqrt{P_0(1-P_0)}+Z_{\\beta} \\sqrt{P(1-P)}}{P-P_0} \\right)^2\\] Donde \\(P\\) es el valor propuesto para \\(H_1\\). Unilateral \\[n=\\left(\\frac{Z_{\\alpha} \\sqrt{P_0(1-P_0)}+Z_{\\beta} \\sqrt{P(1-P)}}{P-P_0} \\right)^2\\] Ejercicio: Se busca realizar una encuesta para estimar la proporción de apoyo a un candidato para la elección en un determinado municipio. Se pide calcular el tamaño de muestra tomando en cuanta tener un nivel de significancia del 5%, un poder del 80% y que las estimaciones sean validas hasta una diferencia del 9%. Suponer que \\(\\sqrt{P_0*(1-P_0)}=\\sqrt{P*(1-P)}=0.5\\). Solución. \\(P-P_0=0.09\\) za2&lt;-qnorm(0.05/2,lower.tail = F) zb&lt;-qnorm(0.2,lower.tail = F) n&lt;-((za2*0.5+zb*0.5)/0.09)^2 n ## [1] 242.2494 Sugerencia: Repasar los ejercicios de los libros de referencia. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
