[["index.html", "Estadística II EST-145 Prefacio Audiencia Estructura del libro Software y acuerdos Bases de datos Agradecimiento", " Estadística II EST-145 Alvaro Chirino Gutierrez 2022-03-28 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Estadística II, una materia del pregrado de la carrera de Informática de la Universidad Mayor de San Ándres. Estructura del libro El libro incluye 6 capítulos, estos son: Distribuciones de probabilidad bivariada Distribuciones muestrales Distribuciones Chi cuadrado, t-student y Fisher Estimación puntual de parámetros Estimación de parámetros por intervalos de confianza Pruebas de hipótesis Software y acuerdos sessionInfo() ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19044) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] stats graphics grDevices ## [4] utils datasets methods ## [7] base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.25 digest_0.6.29 ## [3] R6_2.5.1 jsonlite_1.8.0 ## [5] magrittr_2.0.2 evaluate_0.15 ## [7] highr_0.9 stringi_1.7.6 ## [9] rlang_1.0.2 cli_3.2.0 ## [11] rstudioapi_0.13 jquerylib_0.1.4 ## [13] bslib_0.3.1 rmarkdown_2.13 ## [15] tools_4.1.3 stringr_1.4.0 ## [17] xfun_0.30 yaml_2.3.5 ## [19] fastmap_1.1.0 compiler_4.1.3 ## [21] htmltools_0.5.2 knitr_1.38 ## [23] sass_0.4.1 Bases de datos En este documento se emplearan 2 bases de datos del contexto Boliviano: Encuesta a Hogares 2019 y 2019. Vivienda y Personas Computo oficial de las elecciones del 18 de Octubre de 2020 Estas bases de datos se encuentran disponibles en formato \\(.RData\\) en el repositorio de Github del texto. Agradecimiento "],["distribuciones-conjuntas-de-probabilidad.html", "1 Distribuciones conjuntas de probabilidad 1.1 Variables aleatorias bivariantes 1.2 Función de distribución bivariada 1.3 Función masa de probabilidad (función de densidad) 1.4 Distribución marginal 1.5 Independencia 1.6 Valores esperados 1.7 Distribuciones condicionales 1.8 Esperanza condicional 1.9 Medidas de relación entre dos variables 1.10 Transformaciones 1.11 Distrubuciones conjuntas de más de 2 variables aleatorias", " 1 Distribuciones conjuntas de probabilidad En el caso univariado se tenia a una va \\(X\\) definida en los reales \\(IR\\), a esta va se le asignaba una función de distribución \\(F(x)\\) y una función de densidad \\(f(x)\\). Ambas distribuciones tienen su correspondencia en lo discreto y lo continuo: Caso discreto: \\[\\sum_{Rx} P(X=x)=1\\] \\[F(t)=P(X\\leq t)=\\sum_{x\\leq t} P(X=x)\\] Caso continuo: \\[\\int_{Rx}f(x)dx=1\\] \\[F(t)=P(X\\leq t)=\\int_{-\\infty}^{t}f(x)dx\\] La idea de este capitulo es ver las propiedades en el caso bivariante y generalizar para el caso multivariante. 1.1 Variables aleatorias bivariantes Son un par de variables aleatorias con una distribución conjunta, son típicamente representadas con mayúscula \\((X, Y)\\) o \\((X_1,X_2)\\), las realizaciones de estas variables aleatorias se representan como \\((x,y)\\) o \\((x_1,x_2)\\). Definición 1. Un par de variables aleatorias bivariadas es un par numérico de resultados; una función definida en \\(IR^2\\) Ejemplos: Considerar el par (edad, estatura): \\((23,170)\\) \\((20,172)\\) \\((20,154)\\) \\((26,159)\\) \\((19,175)\\) Considerar el par: (ingreso, años experiencia) Imaginar lanzar 2 monedas simultáneamente, \\(\\Omega=\\{CC,CS,SC,SS\\}\\), si definimos a \\(Cara=1\\) y \\(Sello=0\\), \\(R_{(X,Y)}=\\{(1,1),(1,0),(0,1),(0,0)\\}\\). 1.2 Función de distribución bivariada Definición 2. La función de distribución conjunta de \\((X,Y)\\) es \\[F(x,y)=P(X\\leq x,Y\\leq y)=P\\left[\\{X\\leq x\\} \\cap \\{Y\\leq y\\} \\right]\\] Las propiedades de \\(F\\) son similares al caso univariante, \\(0\\leq F(x,y)\\leq 1\\). Ejemplo: \\[F(x,y)=(1-e^{-x})(1-e^{-y});\\hspace{2cm} x,y\\geq0 \\] * \\(F(0,0)=0\\) * \\(F(\\infty,\\infty)=1\\) Calcular: \\[P(X&lt;2,Y&lt;70)=F(2,70)=0.8647\\] \\[P(X\\leq 10,Y\\leq 100)=F(10,100)=(1-e^{-10})(1-e^{-100})=0.9999546\\] \\[P(X\\leq 5,Y\\leq 20)=F(5,20)=0.99326\\] La distribución conjunta satisface: \\[P(a&lt;X\\leq b,c&lt; Y\\leq d)=F(b,d)-F(b,c)-F(a,d)+F(a,c)\\] Para \\(a&lt;b\\) y \\(c&lt;d\\) 1.3 Función masa de probabilidad (función de densidad) Para el par \\((X,Y)\\) con una función de distribución conjunta \\(F(x,y)\\). Para el caso continuo, Definición 3. \\[f(x,y)=\\frac{\\partial^2}{\\partial x \\partial y}F(x,y)\\] Por un tema de notación, a veces escribiremos \\(f_{X,Y}(x,y)\\) Ejercicio: encontrar \\(f(x,y)\\) para la \\(F\\) dada en el ejemplo anterior \\[f(x,y)=\\frac{\\partial^2}{\\partial x \\partial y}\\left[(1-e^{-x})(1-e^{-y})\\right]=e^{-x}e^{-y}\\] \\(f\\) satisface de forma similar las propiedades vistas en el caso univariante. \\[\\int_{Rx}\\int_{Ry}f(x,y)dydx=1\\] &gt; Ejercicio Probar que la siguiente función es una función de densidad \\[f(x,y)=x+y \\quad ;0 \\leq x \\leq 1 \\quad 0\\leq y \\leq 1\\] \\[\\int_0^1 \\int_0^1 (x+y) dydx=\\int_0^1 \\left(x*y/_0^1+\\frac{y^2}{2}/_0^1 \\right)dx =\\int_0^1 x+\\frac{1}{2} dx\\] \\[=\\frac{x^2}{2}/_0^1+\\frac{1}{2}=\\frac{1}{2}+\\frac{1}{2}=1\\] Para el ejercicio: \\[\\int_0^{\\infty} \\int_0^{\\infty}e^{-x}e^{-y} dx dy=\\int_0^{\\infty} e^{-y} \\left[-e^{-x}/_0^{\\infty} \\right] dy =1\\] \\[P(a&lt;X\\leq b,c&lt; Y\\leq d)=\\int_a^b \\int_c^d f(x,y)dxdy\\] Para el caso discreto podemos definirlo de la siguiente forma: \\[f(x,y)=\\pi(x,y)=P(X=x,Y=y)\\] \\[\\sum_{Rx}\\sum_{Ry}\\pi(x,y)=1\\] Ejemplo, en un restaurante de pizza se vende porciones de pizza y gaseosa, el dueño del local realizó un monitoreo del patrón de como los clientes ordenan sus pedidos, respecto la cantidad de porciones de pizza con la cantidad de gaseosas, encontrando el siguiente resultado: Cuál sera la probabilidad de: \\(P(X=3,Y=2)=0.13\\) \\(P(X\\geq 2, Y=1)=P(X=2, Y=1)+P(X=3,Y=1)=\\pi_{21}+\\pi_{31}=0.17+0.05=0.22\\) Ejercicios. Sea la función de probabilidad discreta: \\[\\pi(x,y)=\\frac{x+y}{30}, \\quad x=0,1,2,3; y=0,1,2\\] Verificar que se trata de una función de probabilidad y luego encontrar: \\[P(X\\leq 2, Y=1)=\\pi_{0,1}+\\pi_{1,1}+\\pi_{2,1}=0.2\\] \\[P(X&gt; 2, Y\\leq 1)=\\pi_{3,0}+\\pi_{3,1}=0.233\\] Solución en R. #crear la función fxy&lt;-function(x,y){ z&lt;-(x+y)/30 return(z) } aux&lt;-0 for(x in 0:3){ for(y in 0:2){ aux&lt;-aux+fxy(x,y) } } aux ## [1] 1 sum(fxy(0:2,1)) ## [1] 0.2 sum(fxy(3,0:1)) ## [1] 0.2333333 Ejercicio Sea la función de densidad: \\[f(x,y)=\\frac{x}{8}; \\quad 0\\leq x\\leq 2;0\\leq y\\leq 4 \\] Verificar si es una función de densidad y calcular: \\[P(X&gt;1,Y&gt;2)\\] Obtener la función de distribución (\\(F\\)) Solución \\[\\int_0^2 \\int_0^4 \\frac{x}{8}dy dx=\\int_0^2 \\frac{x}{8} y/_0^4=\\int_0^2\\frac{x}{2}dx=\\frac{x^2}{4}/_0^2=1\\] \\[P(X\\geq1,Y\\geq2)=P(X&gt;1,Y&gt;2)=\\int_1^2 \\int_2^4 \\frac{x}{8}dy dx=\\] \\[=\\int_1^2 \\frac{x}{8} y/_2^4 dx=\\int_1^2 \\frac{x}{4} dx= \\frac{x^2}{8}/_1^2=\\frac{4-1}{8}=\\frac{3}{8}\\] Tarea: \\[F(x,y)=F(t,r)=\\int_0^t \\int_0^r \\frac{x}{8} dy dx\\] 1.4 Distribución marginal La distribución conjunta del vector aleatorio \\((X,Y)\\) describe la distribución del vector aleatorio, sin embargo, es posible a partir de la distribución conjunta, generar las distribuciones para cada componente del vector aleatorio. Definición 4, la distribución marginal de X es: \\[F_X(x)=P(X\\leq x)=P(X\\leq x, Y\\leq \\infty)=lim_{y\\rightarrow \\infty} F(x,y)\\] \\[F_Y(y)=P(Y\\leq y)=P(X\\leq \\infty, Y\\leq y)=lim_{x\\rightarrow \\infty} F(x,y)\\] De manera más usual se tiene: Para el caso discreto: \\[P(X=x)=\\pi(x)=\\sum_{Ry} \\pi(x,y)\\] \\[P(Y=y)=\\pi(y)=\\sum_{Rx} \\pi(x,y)\\] Para el caso continuo: \\[f(x)=\\int_{Ry} f(x,y)dy\\] \\[f(y)=\\int_{Rx} f(x,y)dx\\] Ejercicio Encontrar las distribuciones marginales de la función \\[f(x,y)=\\frac{x}{8}; \\quad 0\\leq x\\leq 2;0\\leq y\\leq 4\\] Solución: \\[f(x)=\\int_0^4 \\frac{x}{8} dy= \\frac{x}{2}; \\quad 0\\leq x \\leq2\\] \\[f(y)=\\int_0^2 \\frac{x}{8} dx=\\frac{1}{4}; \\quad 0\\leq y \\leq 4\\] Notar que: \\[f(x)*f(y)=\\frac{x}{2}*\\frac{1}{4}=\\frac{x}{8}=f(x,y)\\] Nota: Si el producto de las marginales da la densidad conjunta, las variables son independientes. Ejercicio, para la función: \\[f(x,y)=e^{-x}e^{-y}\\] Con \\(x,y\\geq 0\\), encontrar las marginales de \\(f(x)\\) y \\(f(y)\\). Solución: \\[f(x)=\\int_0^{\\infty}e^{-x}e^{-y} dy=e^{-x}\\] \\[f(y)=\\int_0^{\\infty}e^{-x}e^{-y} dx=e^{-y}\\] Nota: las marginales deben estar en función de su propia variable aleatoria y no contener otras variables, dado que son marginales. Notar que en este ejercicio: \\[f(x,y)=e^{-x}e^{-y}=f(x)*f(y)\\] Esto no siempre sucede, este caso se da cuando las variables son independientes. Ejercicio, Sea la función conjunta \\[f(x,y)=\\frac{1}{500}\\] Para \\(0&lt;X&lt;0.25\\) y \\(0&lt;y&lt;2000\\). Verificar que es una función de probabilidad Encontrar la marginal de \\(x\\) Encontrar la marginal de \\(y\\) \\[\\int_0^{0.25}\\int_0^{2000}\\frac{1}{500}dydx=1\\] \\[f(x)=\\int_{Ry} f(x,y) dy=\\int_0^{2000}\\frac{1}{500}dy=4\\] \\[f(y)=\\int_{Rx} f(x,y) dx=\\int_0^{0.25}\\frac{1}{500}dx=\\frac{1}{2000}\\] 1.5 Independencia Definición 6, dos variables aleatorias son independientes si: \\[f(x,y)=f(x)*f(y)\\] \\[\\pi(x,y)=\\pi(x)*\\pi(y)\\] &gt; Ejercicio Dada la siguiente función de densidad conjunta: \\[f(x,y)=ke^{-2x-3y}; \\quad x,y\\geq 0\\] Encontrar el valor de \\(k\\) para que la función se una función de densidad Encontrar las marginales Verificar si \\(X\\), \\(Y\\) son independientes Solución \\[1=\\int_0^\\infty \\int_0^\\infty ke^{-2x-3y}dx dy=k\\int_0^\\infty e^{-2x} dx \\int_0^\\infty e^{-3y} dy=k*\\frac{1}{2}*\\frac{1}{3}\\] Siendo \\(k=6\\), de esta forma la función queda como: \\[f(x,y)=6e^{-2x-3y}; \\quad x,y\\geq 0\\] Para las marginales: \\[f(x)=\\int_0^\\infty 6e^{-2x-3y}dy=6 e^{-2x} \\left(-\\frac{e^{-3y}}{3} \\right)/_0^\\infty=2e^{-2x}\\] \\[f(y)=\\int_0^\\infty 6e^{-2x-3y}dx=3 e^{-3y}\\] Evidentemente las variables son independientes, ya que: \\[f(x)f(y)=2e^{-2x}3e^{-3y}=6e^{-2x-3y}=f(x,y)\\] Ejemplo, para la distribución conjunta: \\[f(x,y)=\\frac{1}{500}\\] Si \\(X\\) e \\(Y\\) son independientes, se debe cumplir: \\[f(x,y)=f(x)*f(y)=4*\\frac{1}{2000}=\\frac{1}{500}\\] Ejercicio, verificar si la siguiente función esta bien definida y si las variables son independientes. \\[f(x,y)=\\frac{1}{4}(x+y)*xy*e^{-x-y}; \\hspace{2cm} x,y&gt;0\\] Solución, Si esta bien definida, esto significa: \\[\\int_0^{\\infty}\\int_0^{\\infty}\\frac{1}{4}(x+y)*xy*e^{-x-y}=1\\] \\[f(x)=\\int_{Ry}f(x,y)dy=\\frac{x^2+2x}{4} ( e^{-x})\\] Ejercicio, Sea la función conjunta: \\[f(x,y)=k (6-x-y) \\quad 0&lt;x&lt;2, 2&lt;y&lt;4\\] Encontrar el valor de \\(k\\) para que \\(f\\) sea una función de densidad Encontrar las marginales de \\(x\\) e \\(y\\) ¿Son independientes? \\[\\int_{0}^2 \\int_2^4 k (6-x-y) dydx=1\\] Así \\(k=\\frac{1}{8}\\), de esta forma: \\[f(x,y)=\\frac{6-x-y}{8} \\quad 0&lt;x&lt;2, 2&lt;y&lt;4\\] \\[f(x)=\\int_2^4 \\frac{6-x-y}{8} dy=\\frac{3-x}{4}\\] \\[f(y)=\\int_0^2 \\frac{6-x-y}{8} dx=\\frac{5-y}{4}\\] \\[f(x)*f(y)=\\frac{3-x}{4}*\\frac{5-y}{4}\\neq \\frac{6-x-y}{8}\\] Por lo tanto \\(x\\) e \\(y\\) no son independientes. 1.6 Valores esperados En el caso univariado, sea \\(X\\) una variable aleatoria con función de probabilidad \\(\\pi(x)\\) para el caso discreto o \\(f(x)\\) para el caso continuo, el operador matemático esperanza se define como: Para el caso discreto, \\[E[g(X)]=\\sum_{Rx}g(x)P(X=x)\\] Para el caso continuo, \\[E[g(X)]=\\int_{Rx}g(x)f(x)dx\\] Definición 6, el valor esperado para la función \\(g(X,Y)\\), se define como: Para el caso discreto: \\[E[g(X,Y)]=\\sum_{Rx}\\sum_{Ry}g(x,y)\\pi(x,y)\\] Para el caso continuo: \\[E[g(X,Y)]=\\int_{Rx}\\int_{Ry}g(x,y)f(x.y)dydx\\] Nota, hay valores esperados más usuales que otros, Por ejemplo, las varianzas para cada variable \\[E[(X-E[X])^2]=V(X)\\] \\[E[(Y-E[Y])^2]=V(Y)\\] Otras medidas son \\(E[X]\\), \\(E[Y]\\) que son referencias muy similares a un promedio aritmético. Otra valor esperado bastante usado en los casos bivariados es: \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx\\] Encontrar la forma de \\(E[X]\\) usando la definición anterior. \\[E[X]=\\int_{Rx}\\int_{Ry} x f(x,y) dy dx=\\int_{Rx}xf(x) dx\\] \\[E[X]=\\sum_{Rx}\\sum_{Ry} x \\pi(x,y) =\\sum_{Rx}x \\pi(x) dx\\] Ejercicio Sea la función de densidad: \\[f(x,y)=6 e^{-2x-3y}; \\quad x,y \\geq 0\\] Encontrar: \\[E[XY]=\\int_0^\\infty \\int_0^\\infty xy 6 e^{-2x-3y}dxdy=6 \\int_0^\\infty x e^{-2x}dx\\int_0^\\infty y e^{-3y}dy= \\alpha \\] Usando la función gamma. \\[t=2x\\quad x=\\frac{t}{2} \\quad \\frac{dx}{dt}=\\frac{1}{2}\\quad \\] \\[\\int_0^\\infty x e^{-2x}dx=\\int_0^\\infty\\frac{t}{2}e^{-t}\\frac{dt}{2}=\\frac{1}{4}\\int_0^\\infty t^{2-1}e^{-t}dt=\\frac{\\Gamma(2)}{4}=\\frac{1!}{4}=\\frac{1}{4}\\] \\[t=3y\\quad y=\\frac{t}{3} \\quad \\frac{dy}{dt}=\\frac{1}{3}\\quad \\] \\[\\int_0^\\infty y e^{-3x}dx=\\int_0^\\infty\\frac{t}{3}e^{-t}\\frac{dt}{3}=\\frac{1}{9}\\int_0^\\infty t^{2-1}e^{-t}dt=\\frac{\\Gamma(2)}{9}=\\frac{1!}{9}=\\frac{1}{9}\\] Retomando \\[\\alpha=6\\frac{1}{4}\\frac{1}{9}=\\frac{1}{6}=E[XY]\\] \\[E[X]=\\int_0^\\infty x 2e^{-2x}dx=2\\int_0^\\infty x e^{-2x}dx=\\frac{2}{4}=\\frac{1}{2}\\] \\[E[X^2]=\\int_0^\\infty x^2 2e^{-2x}dx=2\\int_0^\\infty x^2 e^{-2x}dx=Tarea\\] \\[E[Y]=3*\\frac{1}{9}=\\frac{1}{3}\\] \\[E[Y^2]=tarea\\] Notar que \\[E[X]E[Y]=\\frac{1}{2}\\frac{1}{3}=\\frac{1}{6}=E[XY]\\] Esto sucede únicamente cuando las variables son independientes Ejercicio, Sea la función conjunta \\[f(x,y)=\\frac{1}{500}, \\quad 0&lt;X&lt;0.25 \\quad 0&lt;y&lt;2000\\] Encontrar: \\(E[X]=1/8\\), \\(V[X]=\\frac{0.25^2}{12}=\\frac{1}{192}\\), \\(E[Y]=1000\\), \\(V[Y]=\\frac{2000^2}{12}=\\frac{1000000}{3}\\) \\(E[XY]\\) \\[E[X]=\\int_0^{0.25}\\int_0^{2000} x \\frac{1}{500}dy dx=\\int_0^{0.25}x\\int_0^{2000} \\frac{1}{500}dy dx=4\\int_0^{0.25}x dx=4*\\frac{(x^2)_0^{0.25}}{2}=\\frac{1}{8}\\] \\[E[XY]=125\\] Nota: Si dos variables aleatorias son independientes: \\[E[XY]=E[X]E[Y]\\] Demostración, \\[E[XY]=\\int_{Rx}\\int_{Ry} xyf(x,y)dydx=\\int_{Rx}\\int_{Ry} xyf(x)f(y)dydx=\\int_{Rx}x f(x)\\left(\\int_{Ry} yf(y)dy\\right)dx=E[X]*E[Y]\\] Ejemplo para el caso discreto, de las pizza y las gaseosas. \\[E[X]=1*0.5+2*0.31+3*0.19=1.69\\] \\[E[Y]=0*0.12+1*0.63+2*0.25=1.13\\] \\[E[X^2]=\\sum_{x=1}^{x=3}x^2 \\pi(x)=1^2*0.5+2^2*0.31+3^2*0.19=3.45\\] \\[E[Y^2]=0^2*0.12+1^2*0.63+2^2*0.25=1.63\\] \\[E[XY]=\\sum_{Rx}\\sum_{Ry}xy \\pi(x,y)=1*0*0.04+1*1*0.42+\\ldots+3*2*0.13=2.067\\] 1.7 Distribuciones condicionales Recordar de estadística I: \\[P(B/A)=\\frac{P(A\\cap B)}{P(A)}\\] Estas distribuciones nos ayudan a entender el comportamiento de una variable, cuando fijamos a otra. Definición, una distribución condicional se define como: Caso discreto, \\[\\pi_{x/y}(X/Y=y)=\\frac{\\pi(x,y)}{\\pi(y)}\\] \\[\\pi_{y/x}(Y/X=x)=\\frac{\\pi(x,y)}{\\pi(x)}\\] Caso continuo, \\[f_{X/Y}(x/y)=\\frac{f(x,y)}{f(y)}\\] \\[f_{Y/X}(y/x)=\\frac{f(x,y)}{f(x)}\\] Estas funciones condicionales cumplen todas las propiedades de una función de probabilidad. Demostrar que: \\[\\int_{Rx} f_{X/Y}(x/y) dx=1\\] \\[\\int_{Rx} f_{X/Y}(x/y) dx=\\int_{Rx} \\frac{f(x,y)}{f(y)} dx=\\frac{1}{f(y)}\\int_{Rx} f(x,y)dx=\\frac{f(y)}{f(y)}=1\\] Que sucede si \\(X\\) e \\(Y\\) son variables independientes: \\[f_{X/Y}(x/y)=\\frac{f(x,y)}{f(y)}=\\frac{f(x)f(y)}{f(y)}=f(x)\\] \\[f_{Y/X}(y/x)=\\frac{f(x,y)}{f(x)}=\\frac{f(x)f(y)}{f(x)}=f(y)\\] Ejercicio, Sea la densidad conjunta de \\(X,Y\\): \\[f(x,y)=x^2+\\frac{xy}{3}, \\quad 0&lt;x\\leq 1, \\quad 0\\leq y \\leq2\\] Se pide: Verificar que es una función de probabilidad Encontrar las marginales Encontrar las condicionales (\\(X/Y\\), \\(Y/X\\)) Encontrar \\(E[X]\\), \\(E[Y]\\), \\(E[XY]\\), \\(E[X/Y]\\), \\(E[Y/X]\\) ¿Son independientes? \\[\\int_{Rx} \\int_{Ry}f(x,y)dydx=\\int_0^1 \\int_0^2 x^2+\\frac{xy}{3} dy dx=1\\] \\[f(x)=\\int_{Ry}f(x,y)dy=2x^2 +\\frac{2x}{3}\\] \\[f(y)=\\int_{Rx}f(x,y)dx=\\frac{1}{3}+\\frac{y}{6}\\] \\[f_{X/Y}(x/y)=\\frac{x^2+\\frac{xy}{3}}{\\frac{1}{3}+\\frac{y}{6}}=\\frac{x(3x+y)}{1+\\frac{y}{2}}\\] \\[f_{Y/X}(y/x)=\\frac{x^2+\\frac{xy}{3}}{2x^2 +\\frac{2x}{3}}=\\frac{3x^2+xy}{6x^2+2x}\\] \\[E[X]=\\int_0^1 x\\left( 2x^2 +\\frac{2x}{3} \\right)dx=\\frac{13}{18}\\] \\[E[Y]=\\int_0^2 y\\left( \\frac{1}{3}+\\frac{y}{6} \\right)dy=\\frac{10}{9}\\] \\[E[XY]=\\int_{Rx} \\int_{Ry} xy f(x,y)dydx=\\int_0^1 \\int_0^2 xy \\left(x^2+\\frac{xy}{3}\\right) dy dx=\\frac{43}{54}\\] 1.8 Esperanza condicional Se refiere a calcular el valor esperado sobre la función condicional. \\[E[Y/X]=\\int_{Ry} y* f_{Y/X}(y/x)dy\\] \\[E[X/Y]=\\int_{Rx} x* f_{X/Y}(x/y)dx\\] Para el ejercicio anterior, \\[E[Y/X]=\\int_0^2 y* \\left(\\frac{3x^2+xy}{6x^2+2x}\\right) dy=\\frac{9x+4}{9x+3}\\] Calcular: \\[E[Y/X=0.5]=\\frac{9*0.5+4}{9*0.5+3}=\\frac{8.5}{7.5}=1.13\\] \\[E[Y/X=1]=\\frac{9*1+4}{9*1+3}=\\frac{13}{12}=1.083\\] \\[E[Y/X=0.5]=\\frac{17}{15}\\] \\[E[X/Y]=\\frac{9+4y}{12+6y}\\] Nota, si \\(X\\) e \\(Y\\) son independientes, \\[E[X/Y=y]=E[X] \\quad;\\quad E[Y/X=x]=E[Y] \\quad ; \\quad E[XY]=E[X]E[Y]\\] Tarea, demostrar lo anterior. 1.9 Medidas de relación entre dos variables Estas medidas nos ayudan a conocer si dos variables están relacionadas y nos permite saber el tipo de relación (directa, inversa) y también podemos saber la intensidad de la relación. Las medidas son: La Covarianza \\(cov(X,Y)\\) es una medida absoluta de relación: \\[cov(X,Y)=E\\left\\{(X-E[X])(Y-E[Y]) \\right\\}\\] Una alternativa a esta formula (versión corta). \\[cov(X,Y)=E[XY]-E[X]*E[Y]\\] Tarea, demostrar lo anterior. Nota: La covarianza es una medida absoluta, nos ayuda a conocer el tipo de relación entre las variables, pero no es útil para conocer la intensidad de la relación Otra medida importantes es la correlación ($corr(X,Y)=_{xy} $) entre \\(X\\) e \\(Y\\), esta es una medida relativa, que cumple la propiedad: \\(-1 \\leq corr(X,Y) \\leq 1\\), esta se define como: \\[\\rho_{xy}=corr(X,Y)=\\frac{cov(X,Y)}{\\sqrt{V(X)V(Y)}}=\\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}\\] Si \\(cov_{xy}\\) o \\(corr_{xy}\\) son distintas de 0, podemos afirmar que existe relación Si \\(cov_{xy}&gt;0\\) o \\(corr_{xy}&gt;0\\) la relación entre \\(X\\) e \\(Y\\) es directa Si \\(cov_{xy}&lt;0\\) o \\(corr_{xy}&lt;0\\) la relación entre \\(X\\) e \\(Y\\) es inversa La intensidad de la dirección de la relación nos la da \\(corr_{xy}\\), mientras más cercana a \\(corr_{xy}\\rightarrow 1\\) la relación directa es más fuerte, \\(corr_{xy}\\rightarrow -1\\) la relación inversa es más fuerte Si \\(corr_{xy}\\rightarrow 0\\) podemos decir que las variables no están relacionadas (cuasi-independencia) la correlación mide principalmente relaciones lineales. Tarea, Calcular la Covarianza y la correlación de las variables aleatorias \\(X,Y\\). \\[cov(X,Y)=\\frac{43}{54}-\\frac{13}{18}*\\frac{10}{9}=-\\frac{1}{162}\\] &gt; Nota Demostrar que si \\(X\\) y \\(Y\\) son independientes entonces: \\[E[XY]=E[X]*E[Y]\\] Demostración, \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx=\\int_{Rx}\\int_{Ry} xy f(x)f(y) dy dx\\] \\[=\\int_{Rx} x f(x)\\left( \\int_{Ry} y f(y) dy \\right) dx=E[Y] \\int_{Rx} xf(x) dx=E[X]E[Y] \\] Como resultado de lo anterior, si \\(X\\) e \\(Y\\) son independientes: \\[cov(X,Y)=E[XY]-E[X]*E[Y]=E[X]E[Y]-E[X]E[Y]=0\\] Si dos variables son independientes la covarianza y la correlación son iguales a cero, el inverso de esta afirmación no necesariamente es cierta. 1.9.1 Ejercicio por 1 punto. Sea la función de densidad: \\[f(x,y)=\\frac{3}{2}(x^2+y^2); \\quad 0&lt;x&lt;1;\\quad 0&lt;y&lt;1\\] Se pide: Verificar que es una función de densidad Encontrar las marginales Encontrar las distribuciones condicionales Calcular la covarianza y correlación Verificar si son independientes Ejercicio Para \\[f(x,y)=\\frac{x(1+3y^2)}{4} \\quad 0&lt;x&lt;2, \\quad 0&lt;y&lt;1\\] Calcular \\[P(1/4&lt;X&lt;1/2 | Y=1/3)\\] Solución Se debe encontrar \\(f_{X|Y}(X|Y)=\\frac{f(x,y)}{f(y)}\\) \\[f(y)=\\int_0^2 \\frac{x(1+3y^2)}{4} dx=\\frac{(1+3y^2)}{4} \\int_0^2 x dx=\\frac{(1+3y^2)}{4} \\frac{x^2}{2}/_0^2=\\] \\[=\\frac{(1+3y^2)}{4}(2)=\\frac{(1+3y^2)}{2}\\] \\[f_{X|Y}(X|Y)=\\frac{\\frac{x(1+3y^2)}{4}}{\\frac{(1+3y^2)}{2}}=\\frac{x}{2}\\] Verificando que sea correcta \\[\\int_0^2 \\frac{x}{2}dx=\\frac{x^2}{4}/_0^2=1\\] \\[P(1/4&lt;X&lt;1/2 | Y=1/3)=\\int_{1/4}^{1/2} f_{X|Y}(X|Y) dx=\\int_{1/4}^{1/2} \\frac{x}{2} dx=\\frac{x^2}{4}/_{0.25}^{0.5}=\\] \\[=\\frac{0.5^2}{4}-\\frac{0.25^2}{4}=\\frac{3}{64}\\] Ejercicio de práctica, Sea la función de densidad: \\[f(x,y)=24xy, \\quad x&gt;0 \\quad y&gt;0 \\quad x+y&lt;1\\] Verificar si es una función de probabilidad Encontrar la función de distribución \\(F\\) Calcular la probabilidad que \\[P(X&gt;0.3 , Y&lt;0.5 )\\] Recordar; \\[F(x,y)=P(X&lt;x,Y&lt;y)\\] Hay que tomar al menos 2 casos: \\(0&lt;x&lt;1\\), \\(x+y&lt;1\\) \\(0&lt;x&lt;1\\), \\(1-x\\leq y\\leq1\\) Para el primer caso: \\[\\int_0^y \\int_0^x 24xy dxdy=6x^2 y^2\\] Para el segundo caso: \\[\\int_0^y \\int_0^{1-y} 24xy dxdy=\\] Ejercicio de práctica, Sea la función de densidad \\[f(x,y)=4 e^{-2(x+y)} \\quad x&gt;0 \\quad y&gt;0\\] * Verificar si es una función de probabilidad * Suponer que estamos interesados en el comportamiento de la variable: \\[Z=\\frac{X}{Y}\\] encontrar la distribución de \\(Z\\). Definimos a \\(z=\\frac{x}{y}\\) y \\(w=x+y\\). Ahora encontramos las funciones \\(G\\) \\[x=\\frac{zw}{1+z}; \\quad y=\\frac{w}{1+z}\\] \\[l(w,z)=f(X=G_1(Z,W),Y=G_2(Z,W))*|J(Z,W)|=4 e^{-2(\\frac{zw}{1+z}+\\frac{w}{1+z})}*\\frac{w}{(1+z)^2}\\] \\[l(z,w)=\\frac{4w e^{-2w}}{(1+z)^2}, \\quad w&gt;0, \\quad z&gt;0\\] finalmente: \\[f(z)=\\int_0^{\\infty} \\frac{4w e^{-2w}}{(1+z)^2} dw=\\frac{1}{(1+z)^2}\\] 1.10 Transformaciones Supongamos que se busca encontrar a partir de la función de densidad \\(f(X,Y)\\) de dos variables X, Y una tercera variable \\(Z=H_1(X,Y)\\). La estrategia para resolver este tipo de problemas es: Definir una cuarta variable \\(W=H_2(X,Y)\\), es elegida por conveniencia Se encuentran \\(X=G_1(Z,W)\\), \\(Y=G_2(Z,W)\\) Una vez con estos valores la función conjunta de \\(Z,W\\) es: \\[l(Z,W)=f(X=G_1(Z,W),Y=G_2(Z,W))*|J(Z,W)|\\] Donde \\(J(Z,W)\\) se llama jacobiano de la transformación y es dado por: \\[J(Z,W)=\\left| \\begin{array} &amp; \\frac{\\partial X }{\\partial Z} &amp; \\frac{\\partial X }{\\partial W} \\\\ \\frac{\\partial Y }{\\partial Z} &amp; \\frac{\\partial Y }{\\partial W} \\end{array} \\right| \\] Ejemplo, Suponer que tenemos una función de densidad conjunto \\(f(x,y)\\), ahora estamos interesados en encontrar la función de densidad de la variable z, definida como: \\[Z=X+Y=H_1(X,Y)\\] \\[Z=XY=H_1(X,Y)\\] Ejemplo Sea la función de densidad conjunta definida como: \\[f(x,y)=4 e^{-2(x+y)} \\quad x&gt;0 \\quad y&gt;0\\] Encontrar la función de densidad de la variable \\(Z=X+Y\\) \\[Z=X+Y=H_1(X,Y); \\quad W=Y=H_2(X,Y)\\] \\[X=Z-W=G_1(Z,W) \\quad Y=W=G_2(Z,W)\\] \\[J(Z,W)=\\left| \\begin{array} &amp; \\frac{\\partial X }{\\partial Z} &amp; \\frac{\\partial X }{\\partial W} \\\\ \\frac{\\partial Y }{\\partial Z} &amp; \\frac{\\partial Y }{\\partial W} \\end{array} \\right|=\\left| \\begin{array} &amp; 1 &amp; -1 \\\\ 0 &amp; 1 \\end{array} \\right|=1 \\] \\[l(z,w)=4 e^{-2(z-w+w)}|1|=4e^{-2z}; z&gt;0,w&gt;0\\] \\[l(z)=\\int_0^\\infty 4e^{-2z}dw=4e^{-2z} w/_0^\\infty=\\] Se debe volver a plantear una nueva forma de \\(W\\) Tarea Encontrar una W para el ejercicio anterior, que permita encontrar la función de densidad de \\(Z\\) (Recomendación \\(W=X/Y\\)) Sea \\(X\\) y \\(Y\\) variables aleatorias independientes, cada una con la misma distribución exponencial. Encuentre la función de densidad de \\(Z=X/Y\\), Recomendación \\(W=X+Y\\) 1.11 Distrubuciones conjuntas de más de 2 variables aleatorias Estamos introduciendo ahora \\(n\\) variables aleatorias, que se puede denotar por vector \\[\\left[X_1,X_2,\\ldots,X_n \\right]\\] Se puede definir su función de densidad conjunta: \\[f(X_1,X_2,\\ldots,X_n)\\] Evidentemente esta función cumple con: \\[\\int_{Rx_1}\\int_{Rx_2}\\ldots \\int_{Rx_n}f(X_1,X_2,\\ldots,X_n)dx_n\\ldots dx_2dx_1=1\\] \\[f(X_1,X_2,\\ldots,X_n)\\geq 0\\] Las marginales de cada variables se obtienen integrando la función de densidad sobre todas las variables aleatorias excepto la variable de interés. \\[f(x_i)=\\int \\ldots\\int_{Rx_{j\\neq i}} f(X_1,X_2,\\ldots,X_n)dx\\ldots dx_{j \\neq i}\\] Notar también que si las \\(n\\) variables aleatorias son independientes entre ellas: \\[f(X_1,X_2,\\ldots,X_n)=f(X_1)*f(X_2)*\\ldots f(X_n)\\] "],["tema-2-distribuciones-muestrales.html", "2 Tema 2: Distribuciones muestrales 2.1 Muestras y población 2.2 Parámetros, estadísticas y estimadores. 2.3 Distribución muestral 2.4 Propiedades de los estimadores 2.5 Distribución muestral para la media 2.6 Teorema del límite central 2.7 Distribución muestral para la diferencia de medias 2.8 Distribución muestral para la proporción 2.9 Distribución muestral para la varianza 2.10 Distribución \\(\\chi^2\\) 2.11 Distribución t-student 2.12 Distribución Fisher (F) 2.13 Ejercicios", " 2 Tema 2: Distribuciones muestrales A partir de este tema la estadística esta vinculada con la inferencia sobre los parámetros de la información/datos. 2.1 Muestras y población Definición: Una población es una colección completa de objetos, estos objetos tienen variables. Sea nuestra población \\(U\\), esta población puede ser finita o infinita \\[U=\\{u_1, u_2, \\ldots , u_i,...,u_N \\}\\] \\[U=\\{u_1, u_2, \\ldots , u_i,... \\}\\] Cada elemento de \\(U\\) tiene variables o características asociadas: \\[u_i=\\{X_{i1}, X_{i2}, \\ldots, X_{iP} \\}\\] \\[u_j=\\{X_{j1}, X_{j2}, \\ldots, X_{jP} \\}\\] Definición, Muestra: Una muestra es un subconjunto de U. \\[s \\subset U ,\\quad s \\in U \\quad \\] Donde \\(s\\) representa al conjunto denominado muestra Normalmente una muestra tiene un tamaño \\(n\\) (puede ser fijo y aleatorio), el mecanismo para obtener la muestra de \\(U\\) puede ser con reposición o sin reposición, en cualquier caso podemos anotar esto de la siguiente forma, sea \\(s\\) una muestra: \\[s=\\{u_{1}^*,u_2^*, \\ldots, u_n^*\\}\\] En un caso extremo para el muestreo con reposición \\[u_i^*=u_j^* \\quad i,j=1, \\ldots ,n\\] Note que los elementos \\(u_1\\) y \\(u_1^*\\) no necesariamente son los mismos. Una característica ideal al momento de obtener la muestra, es que todos los elementos de la población tengan alguna probabilidad de ser parte de la muestra. (Esto depende del esquema de selección) El subconjunto \\(s\\) no es único y en realidad existen muchas muestras posibles, según el contexto, esto depende: Del tamaño de \\(N\\), \\(n\\) Del mecanismo s/rep, c/rep. Ejemplo, Sea la población \\(U=\\{a,b,c,d,e,f\\}\\), se define una muestra de \\(n=3\\), escriba todas las muestras posibles según ambos mecanismos de reposición. Solución, (s/rep), 20: \\(s_1=\\{a,b,c\\}\\), \\(s_2=\\{a,b,d \\}\\), \\(\\ldots\\) ,\\(s_{20}=\\{d,e,f\\}\\) (c/rep), 216: \\(s_1=\\{a,a,a\\}\\), \\(s_2=\\{a,a,b\\}\\), \\(\\ldots\\), \\(s_{216}=\\{f,f,f\\}\\) N&lt;-6;n&lt;-3 choose(N,n) ## [1] 20 U&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;) combn(U,n) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; ## [2,] &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;c&quot; ## [3,] &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;d&quot; &quot;e&quot; ## [,7] [,8] [,9] [,10] [,11] ## [1,] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; ## [2,] &quot;c&quot; &quot;d&quot; &quot;d&quot; &quot;e&quot; &quot;c&quot; ## [3,] &quot;f&quot; &quot;e&quot; &quot;f&quot; &quot;f&quot; &quot;d&quot; ## [,12] [,13] [,14] [,15] [,16] ## [1,] &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; ## [2,] &quot;c&quot; &quot;c&quot; &quot;d&quot; &quot;d&quot; &quot;e&quot; ## [3,] &quot;e&quot; &quot;f&quot; &quot;e&quot; &quot;f&quot; &quot;f&quot; ## [,17] [,18] [,19] [,20] ## [1,] &quot;c&quot; &quot;c&quot; &quot;c&quot; &quot;d&quot; ## [2,] &quot;d&quot; &quot;d&quot; &quot;e&quot; &quot;e&quot; ## [3,] &quot;e&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; N^n ## [1] 216 En una población de 58 estudiantes, si se define una muestra de 15 estudiantes, según ambos mecanismos de selección ¿Cuántas muestras se pueden armar? (s/r) 2.9752626^{13} (c/r) 2.8276126^{26} Doscientos ochenta y dos cuatrillones setecientos sesenta y un mil doscientos cincuenta y seis trillones ochocientos ochenta y un mil doscientos noventa y siete billones trescientos tres mil setecientos seis millones seiscientos sesenta y seis mil cuatrocientos cuatro En la carrera de informática de la UMSA se planea realizar una encuesta de opinión con una muestra de 500 estudiantes, según ambos mecanismos de selección ¿Cuántas muestras se pueden armar? (Suponer que \\(N=2500\\)) (s/r, c/r) Es un número muy grande. Para s/r son 542 dígitos Sin reposición: \\[Muestras_{Posibles}=\\binom{N}{n}\\] Con reposición: \\[Muestras_{Posibles}=N^n\\] Imaginemos a la primera variable de interés \\(X_1\\), para el universo esta variable tiene los elementos: \\[X_1=\\{X_{11}, X_{21}, X_{31}, \\ldots, X_{N1} \\}\\] Imaginemos que observamos a \\(X_1\\), para la muestra. \\[X_1^*=\\{X_{11}^*, X_{21}^*, X_{31}^*, \\ldots, X_{n1}^* \\}\\] Estos \\(X_{i1}^*\\) para los \\(i=1,\\ldots,n\\) son variables aleatorias. Por lo tanto \\(X_1^*\\) es un vector aleatorio de tamaño \\(n\\). De ahora en adelante vamos a trabajar con un solo vector aleatorio denominado \\(X\\), de tal forma que este sea la colección de \\(n\\) variables aleatorias. \\[X=\\{X_1,X_2,\\ldots,X_n \\}\\] Definición. La colección del vector aleatorio \\(X=\\{X_1,X_2,\\ldots,X_n \\}\\), son independientes e idénticamente distribuidas (iid) si la distribución conjunta de las \\(n\\) variables puede ser escrita como: \\[f(x_1,x_2,\\ldots,x_n)=f(x_1)*f(x_2)*\\ldots*f(x_n)\\] y además todas las \\(x_i\\) tienen la misma función de distribución \\(F(x)\\). Definición Sea \\(N\\) el tamaño de la población y \\(n\\) el tamaño de la muestra, ambos valores para fines de este capítulo son constantes. 2.2 Parámetros, estadísticas y estimadores. El objetivo de la estadística es aprender acerca de las características de una población. Estas características las vamos a llamar parámetros. Definición, Un parámetro \\(\\theta\\) es una función sobre la población \\(U\\). \\[\\theta=f(U,X,Y,Z,\\ldots)\\] Nota: Los parámetros de una población son constantes. Los parámetros más usuales son, el total, la media, la proporción, la varianza, las razones. Ejemplo, Sea el universo los 10 primeros números naturales y sus valores. \\(Y=\\{1,2,3,4,5,6,7,8,9,10\\}\\). Sobre estos valores de esta población de \\(N=10\\) se pueden calcular los siguientes parámetros. Total \\[\\theta_1=t_y=\\sum_U y_i=55\\] Media \\[\\theta_2=\\mu_y=\\frac{t_y}{N}=\\frac{55}{10}=5.5\\] Máximo: \\(\\theta_3=max(y)=10\\) Mínimo: \\(\\theta_4=min(y)=1\\) Proporción \\[P_{pares}=\\frac{5}{10}=0.5\\] Es posibles hacer transformaciones sobre \\(Y\\), sea \\(Z\\) una variables binaria que identifique a los números primos de \\(Y\\); \\(1=primo\\), \\(0=\\sim primo\\) \\[Z=\\{1,1,1,0,1,0,1,0,0,0 \\}\\] Calcular el promedio de \\(Z\\) \\[\\theta_5=\\mu_z=\\frac{5}{10}=0.5\\] Cuando obtenemos la media de un vector binario, obtenemos lo que se denomina un proporción. \\[\\theta_5=P_a=\\frac{\\#A}{N}\\] \\[\\theta_5=P_{primos}=\\frac{\\#primos}{N}\\] Diferencias de medias: asume que tenemos a 2 poblaciones de interés, de tamaño \\(N_1\\) y \\(N_2\\) \\[\\theta_6=\\mu_1-\\mu_2\\] Diferencias de proporciones: asume que tenemos a 2 poblaciones de interés, de tamaño \\(N_1\\) y \\(N_2\\) \\[\\theta_7=P_1-P_2\\] Definición, estadística Se denomina estadística a una función sobre la muestra. Definición, estimador \\[Aleatorio\\quad:f(s,X)=\\hat{\\theta} \\rightarrow \\theta=f(U,X): \\quad fijo\\] Un estimador \\(\\hat{\\theta}\\) para el parámetro \\(\\theta\\) es una estadística que busca aproximar/adivinar el valor de \\(\\theta\\) Ejemplo Imaginemos una población de 10 estudiantes sobre la cuales nos interesa conocer el gasto diario en pasajes a la universidad (pasaje ida y vuelta desde su casa al monoblock), para ello se planea tomar una muestra de 3 estudiantes. El objetivo de este estudio es conocer el gasto total de estos 10 estudiantes. Sea \\(y_i\\) la variable gasto es pasajes del estudiante \\(i\\). \\[\\theta=t_y=\\sum_U y_i=\\sum_{i=1}^{10} y_i\\] N&lt;-10 y&lt;-c(5.2,6,8,10,3,4,7,4,4,10.5) ty&lt;-sum(y)#parámetro Ahora vamos a obtener una muestra de tamaño 3 de esta población. (s/rep) n&lt;-3 choose(10,3)#muestra posibles ## [1] 120 set.seed(1439)#semilla ys&lt;-sample(y,n) ys ## [1] 10.5 3.0 5.2 \\[\\hat{\\theta}_1=\\hat{t}_y=\\frac{\\prod_s y_i}{n}=54.6\\] \\[\\hat{\\theta}_2=N*\\frac{\\sum_s y_i}{n}=62.3\\] \\[\\hat{\\theta}_3=y_{min}+\\prod_{s(-y_{min})}y_i=57.6\\] \\[\\hat{\\theta}_4=n\\sum_s y_i=56.1\\] Nota, el estimador hace referencia a la función matemática, mientras que una estimación es una evaluación de esa función matemática usando la muestra seleccionada. s&lt;-combn(y,n)#muestras posibles #theta1 et1&lt;-apply(s,2,prod)/n #theta2 et2&lt;-N*apply(s, 2,mean) #theta4 et4&lt;-n*apply(s, 2,sum) par(mfrow=c(2,2)) plot(density(et1),&quot;theta1&quot;,xlim=c(0,150)) abline(v=ty,col=&quot;red&quot;) plot(density(et2),&quot;theta2&quot;,xlim=c(0,150)) abline(v=ty,col=&quot;red&quot;) plot(density(et4),&quot;theta4&quot;,xlim=c(0,150)) abline(v=ty,col=&quot;red&quot;) dev.off() ## null device ## 1 e1&lt;-sum(et1*(1/120)) e2&lt;-sum(et2*(1/120)) e4&lt;-sum(et4*(1/120)) sum(((et1-e1)^2)*(1/120)) ## [1] 2348.766 sum(((et2-e2)^2)*(1/120)) ## [1] 162.2989 sum(((et4-e4)^2)*(1/120)) ## [1] 131.4621 \\[E[X]=\\sum_{Rx}x*P(X=x)\\] \\[E[\\hat{\\theta}]=\\theta\\] 2.3 Distribución muestral Recordar que una estadística es una función sobre la muestra y sobre los valores que toman las variables aleatorias vinculadas a esta. Como la estadística es una función sobre las muestras aleatorias (muestras posibles) las evaluaciones que se realizan para cada una de las muestras posibles (estimaciones) conforman lo que vamos a denominar una distribución muestral. Por ejemplo si planteamos al estimador del parámetro del total, recordar: \\[\\theta=t_y=\\sum_U y_i\\] Un estimador para este parámetro será: \\[\\hat{\\theta}=\\hat{t}_y=\\frac{N}{n} \\sum_s y_i\\] Este \\(\\hat{\\theta}\\) es una estadística sobre las muestras aleatorias, por lo tanto podemos decir que existe una distribución de probabilidad para este estimador, a esa distribución de probabilidad se conoce como distribución muestral. Ejemplo práctico. Supongamos que de una población de 6 personas tenemos la información de sus ingresos mensuales. \\(Y_{Ingresos}=\\{2000,3000,3500,0,6000,4500\\}\\). \\(N=6\\) Supongamos que seleccionamos una muestra de tamaño \\(n=3\\) de esta población, para ambos mecanismos de selección (s/rep, c/rep), se pide para ambos mecanismos: Conocer la cantidad de muestras posibles y mostrar estas. Para el estimador \\[\\hat{\\bar{Y}}=\\frac{1}{n}\\sum_s y_i\\] construir su distribución muestral y calcular su esperanza y su varianza Para el estimador; \\[\\hat{t}_y=\\frac{N}{n}\\sum_s y_i\\] construir su distribución muestral y calcular su esperanza y su varianza Respuesta, (S/rep) Las muestras posibles son 20, estas muestras posibles son: Y&lt;-c(2000,3000,3500,0,6000,4500) s&lt;-combn(Y,3) s ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 2000 2000 2000 2000 2000 2000 ## [2,] 3000 3000 3000 3000 3500 3500 ## [3,] 3500 0 6000 4500 0 6000 ## [,7] [,8] [,9] [,10] [,11] ## [1,] 2000 2000 2000 2000 3000 ## [2,] 3500 0 0 6000 3500 ## [3,] 4500 6000 4500 4500 0 ## [,12] [,13] [,14] [,15] [,16] ## [1,] 3000 3000 3000 3000 3000 ## [2,] 3500 3500 0 0 6000 ## [3,] 6000 4500 6000 4500 4500 ## [,17] [,18] [,19] [,20] ## [1,] 3500 3500 3500 0 ## [2,] 0 0 6000 6000 ## [3,] 6000 4500 4500 4500 Para el estimador de la media; Tomar en cuenta que el valor del parámetro de la media poblacional es: \\(\\mu_y=\\sum_U y_i /N=3166.667\\) y&lt;-apply(s,2,sum)/3 #Distribución muestral para el estimador de la media hist(y) abline(v=mean(Y),col=&quot;red&quot;,lwd=3) # calcular la esperanza y la varianza uy&lt;-sum(y*(1/20)) # esperanza del estimador de la media sum((y-uy)^2*(1/20)) # varianza de la media muestral ## [1] 711111.1 \\[E[\\hat{\\theta}]=\\sum_{Rs} \\hat{\\theta_s} P(\\hat{\\theta}=\\hat{\\theta_s})\\] \\[V(\\hat{\\theta})=E[(\\hat{\\theta}-E[\\hat{\\theta}])^2]=\\sum_{s}(\\hat{\\theta_s}-E[\\hat{\\theta}])^2*P(\\hat{\\theta}=\\hat{\\theta})\\] Nota, Si \\(E[\\hat{\\theta}]=\\theta\\) decimos que el estimador \\(\\hat{\\theta}\\) es un estimador insesgado (sin sesgo) El estimador de la media muestral, es un estimador insesgado de la media poblacional. Para el estimador del total; Tomar en cuenta que el valor del parámetro del total poblacional es: \\(t_y=\\sum_U y_i=19000\\) ty&lt;-apply(s,2,sum)*(6/3) #Distribución muestral para el estimador del total hist(ty) abline(v=sum(Y),col=&quot;red&quot;,lwd=3) pty&lt;-sum(ty*(1/20)) # esperanza sum((ty-pty)^2*(1/20)) # varianza de la media muestral ## [1] 25600000 \\[E[\\hat{t}_y]=E[N*\\bar{Y}]=N E[\\bar{Y}]=N*u_y=N*\\frac{\\sum_U y_i}{N}=\\sum_U {y_i}=t_y\\] Repetir los cálculos para un muestreo con reposición. Muestras probables \\(6^3=N^n=216\\). Y&lt;-round(rnorm(25,30,5)) s&lt;-combn(Y,10) y&lt;-apply(s,2,sum)/10 hist(y) abline(v=mean(Y),col=&quot;red&quot;,lwd=2) 2.4 Propiedades de los estimadores Se busca que un estimador cumpla al menos 2 de las siguientes propiedades: 2.4.1 Estimador insesgado \\[E[\\hat{\\theta}]=\\theta\\] 2.4.2 Estimador eficiente \\[V[\\hat{\\theta}_1]&lt;V[\\hat{\\theta}_2]\\] Se dice que \\(\\hat{\\theta}_1\\) es más eficiente que \\(\\hat{\\theta}_2\\) si tiene una menor varianza 2.5 Distribución muestral para la media Recordar que para una población (\\(U\\)) con alguna variable \\(X\\) de tipo cuantitativa se puede obtener el parámetro de la media, definido como: \\[\\mu_x=\\frac{\\sum_U x_i}{N}\\] Esta variable \\(X\\) en la población por lo tanto tiene su media \\(\\mu_x\\) y también tiene su varianza, denotada por: \\[V(X)=\\sigma_x^2=\\frac{\\sum_U (x_i-\\mu_x)^2}{N}\\] Teorema: Sean \\(X_1,X_2,\\ldots,X_n\\) variables aleatorias para una muestra de tamaño \\(n\\) extraida de la población \\(U\\), donde estas \\(X_i\\) independientes e idénticamente distribuidas (iid) como: \\(X_i\\sim .(E[X_i]=\\mu_x,V(X_i)=\\sigma_x^2)\\), entonces, si: \\[\\bar{X}=\\frac{\\sum_s x_i}{n}\\] Tenemos que \\[E[\\bar{X}]=\\mu_x\\] \\[V(\\bar{X})=\\sigma^2_{\\bar{x}}=\\frac{\\sigma^2_x}{n}\\] Demostración, \\[E[\\bar{X}]=E\\left[\\frac{\\sum_s x_i}{n}\\right]=\\frac{1}{n}E[x_1+x_2+\\ldots+x_n]=\\frac{1}{n}\\left(E[x_1]+E[x_2]+\\ldots+E[x_n] \\right)=\\] \\[=\\frac{1}{n}(\\mu_x+\\mu_x+\\ldots+\\mu_x)=\\frac{n \\mu_x}{n}=\\mu_x\\] Si, \\(X\\) e \\(Y\\) son independientes \\(Cov(X,Y)=0\\). \\[V(X+Y)=V(X)+V(Y)\\] \\[V(\\bar{X})=V\\left(\\frac{\\sum_s x_i}{n}\\right)=\\frac{1}{n^2}V(x_1+x_2+\\ldots+x_n)=\\frac{1}{n^2}\\{V(x_1)+\\ldots+V(x_n)\\}=\\] \\[=\\frac{1}{n^2}(\\sigma^2_x+\\sigma^2_x+\\ldots+\\sigma^2_x)=\\frac{n \\sigma_x^2}{n^2}=\\frac{\\sigma^2_x}{n}\\] Nota: Cuando no es posible tener acceso al valor de \\(\\sigma^2_x\\) se puede estimar este parámetro, mediante la muestra usando en su lugar a la varianza muestral: \\[\\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] Ejemplo Sean los siguientes 10 valores de una variable \\(X\\) set.seed(999) x&lt;-round(runif(10,20,50),0) x ## [1] 32 37 23 46 44 24 38 22 32 39 mux&lt;-mean(x) mux#media poblacional ## [1] 33.7 n&lt;-4 #muestras s/rep choose(10,4)#muestras posibles ## [1] 210 s&lt;-combn(x,n) s ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 32 32 32 32 32 32 ## [2,] 37 37 37 37 37 37 ## [3,] 23 23 23 23 23 23 ## [4,] 46 44 24 38 22 32 ## [,7] [,8] [,9] [,10] [,11] ## [1,] 32 32 32 32 32 ## [2,] 37 37 37 37 37 ## [3,] 23 46 46 46 46 ## [4,] 39 44 24 38 22 ## [,12] [,13] [,14] [,15] [,16] ## [1,] 32 32 32 32 32 ## [2,] 37 37 37 37 37 ## [3,] 46 46 44 44 44 ## [4,] 32 39 24 38 22 ## [,17] [,18] [,19] [,20] [,21] ## [1,] 32 32 32 32 32 ## [2,] 37 37 37 37 37 ## [3,] 44 44 24 24 24 ## [4,] 32 39 38 22 32 ## [,22] [,23] [,24] [,25] [,26] ## [1,] 32 32 32 32 32 ## [2,] 37 37 37 37 37 ## [3,] 24 38 38 38 22 ## [4,] 39 22 32 39 32 ## [,27] [,28] [,29] [,30] [,31] ## [1,] 32 32 32 32 32 ## [2,] 37 37 23 23 23 ## [3,] 22 32 46 46 46 ## [4,] 39 39 44 24 38 ## [,32] [,33] [,34] [,35] [,36] ## [1,] 32 32 32 32 32 ## [2,] 23 23 23 23 23 ## [3,] 46 46 46 44 44 ## [4,] 22 32 39 24 38 ## [,37] [,38] [,39] [,40] [,41] ## [1,] 32 32 32 32 32 ## [2,] 23 23 23 23 23 ## [3,] 44 44 44 24 24 ## [4,] 22 32 39 38 22 ## [,42] [,43] [,44] [,45] [,46] ## [1,] 32 32 32 32 32 ## [2,] 23 23 23 23 23 ## [3,] 24 24 38 38 38 ## [4,] 32 39 22 32 39 ## [,47] [,48] [,49] [,50] [,51] ## [1,] 32 32 32 32 32 ## [2,] 23 23 23 46 46 ## [3,] 22 22 32 44 44 ## [4,] 32 39 39 24 38 ## [,52] [,53] [,54] [,55] [,56] ## [1,] 32 32 32 32 32 ## [2,] 46 46 46 46 46 ## [3,] 44 44 44 24 24 ## [4,] 22 32 39 38 22 ## [,57] [,58] [,59] [,60] [,61] ## [1,] 32 32 32 32 32 ## [2,] 46 46 46 46 46 ## [3,] 24 24 38 38 38 ## [4,] 32 39 22 32 39 ## [,62] [,63] [,64] [,65] [,66] ## [1,] 32 32 32 32 32 ## [2,] 46 46 46 44 44 ## [3,] 22 22 32 24 24 ## [4,] 32 39 39 38 22 ## [,67] [,68] [,69] [,70] [,71] ## [1,] 32 32 32 32 32 ## [2,] 44 44 44 44 44 ## [3,] 24 24 38 38 38 ## [4,] 32 39 22 32 39 ## [,72] [,73] [,74] [,75] [,76] ## [1,] 32 32 32 32 32 ## [2,] 44 44 44 24 24 ## [3,] 22 22 32 38 38 ## [4,] 32 39 39 22 32 ## [,77] [,78] [,79] [,80] [,81] ## [1,] 32 32 32 32 32 ## [2,] 24 24 24 24 38 ## [3,] 38 22 22 32 22 ## [4,] 39 32 39 39 32 ## [,82] [,83] [,84] [,85] [,86] ## [1,] 32 32 32 37 37 ## [2,] 38 38 22 23 23 ## [3,] 22 32 32 46 46 ## [4,] 39 39 39 44 24 ## [,87] [,88] [,89] [,90] [,91] ## [1,] 37 37 37 37 37 ## [2,] 23 23 23 23 23 ## [3,] 46 46 46 46 44 ## [4,] 38 22 32 39 24 ## [,92] [,93] [,94] [,95] [,96] ## [1,] 37 37 37 37 37 ## [2,] 23 23 23 23 23 ## [3,] 44 44 44 44 24 ## [4,] 38 22 32 39 38 ## [,97] [,98] [,99] [,100] [,101] ## [1,] 37 37 37 37 37 ## [2,] 23 23 23 23 23 ## [3,] 24 24 24 38 38 ## [4,] 22 32 39 22 32 ## [,102] [,103] [,104] [,105] ## [1,] 37 37 37 37 ## [2,] 23 23 23 23 ## [3,] 38 22 22 32 ## [4,] 39 32 39 39 ## [,106] [,107] [,108] [,109] ## [1,] 37 37 37 37 ## [2,] 46 46 46 46 ## [3,] 44 44 44 44 ## [4,] 24 38 22 32 ## [,110] [,111] [,112] [,113] ## [1,] 37 37 37 37 ## [2,] 46 46 46 46 ## [3,] 44 24 24 24 ## [4,] 39 38 22 32 ## [,114] [,115] [,116] [,117] ## [1,] 37 37 37 37 ## [2,] 46 46 46 46 ## [3,] 24 38 38 38 ## [4,] 39 22 32 39 ## [,118] [,119] [,120] [,121] ## [1,] 37 37 37 37 ## [2,] 46 46 46 44 ## [3,] 22 22 32 24 ## [4,] 32 39 39 38 ## [,122] [,123] [,124] [,125] ## [1,] 37 37 37 37 ## [2,] 44 44 44 44 ## [3,] 24 24 24 38 ## [4,] 22 32 39 22 ## [,126] [,127] [,128] [,129] ## [1,] 37 37 37 37 ## [2,] 44 44 44 44 ## [3,] 38 38 22 22 ## [4,] 32 39 32 39 ## [,130] [,131] [,132] [,133] ## [1,] 37 37 37 37 ## [2,] 44 24 24 24 ## [3,] 32 38 38 38 ## [4,] 39 22 32 39 ## [,134] [,135] [,136] [,137] ## [1,] 37 37 37 37 ## [2,] 24 24 24 38 ## [3,] 22 22 32 22 ## [4,] 32 39 39 32 ## [,138] [,139] [,140] [,141] ## [1,] 37 37 37 23 ## [2,] 38 38 22 46 ## [3,] 22 32 32 44 ## [4,] 39 39 39 24 ## [,142] [,143] [,144] [,145] ## [1,] 23 23 23 23 ## [2,] 46 46 46 46 ## [3,] 44 44 44 44 ## [4,] 38 22 32 39 ## [,146] [,147] [,148] [,149] ## [1,] 23 23 23 23 ## [2,] 46 46 46 46 ## [3,] 24 24 24 24 ## [4,] 38 22 32 39 ## [,150] [,151] [,152] [,153] ## [1,] 23 23 23 23 ## [2,] 46 46 46 46 ## [3,] 38 38 38 22 ## [4,] 22 32 39 32 ## [,154] [,155] [,156] [,157] ## [1,] 23 23 23 23 ## [2,] 46 46 44 44 ## [3,] 22 32 24 24 ## [4,] 39 39 38 22 ## [,158] [,159] [,160] [,161] ## [1,] 23 23 23 23 ## [2,] 44 44 44 44 ## [3,] 24 24 38 38 ## [4,] 32 39 22 32 ## [,162] [,163] [,164] [,165] ## [1,] 23 23 23 23 ## [2,] 44 44 44 44 ## [3,] 38 22 22 32 ## [4,] 39 32 39 39 ## [,166] [,167] [,168] [,169] ## [1,] 23 23 23 23 ## [2,] 24 24 24 24 ## [3,] 38 38 38 22 ## [4,] 22 32 39 32 ## [,170] [,171] [,172] [,173] ## [1,] 23 23 23 23 ## [2,] 24 24 38 38 ## [3,] 22 32 22 22 ## [4,] 39 39 32 39 ## [,174] [,175] [,176] [,177] ## [1,] 23 23 46 46 ## [2,] 38 22 44 44 ## [3,] 32 32 24 24 ## [4,] 39 39 38 22 ## [,178] [,179] [,180] [,181] ## [1,] 46 46 46 46 ## [2,] 44 44 44 44 ## [3,] 24 24 38 38 ## [4,] 32 39 22 32 ## [,182] [,183] [,184] [,185] ## [1,] 46 46 46 46 ## [2,] 44 44 44 44 ## [3,] 38 22 22 32 ## [4,] 39 32 39 39 ## [,186] [,187] [,188] [,189] ## [1,] 46 46 46 46 ## [2,] 24 24 24 24 ## [3,] 38 38 38 22 ## [4,] 22 32 39 32 ## [,190] [,191] [,192] [,193] ## [1,] 46 46 46 46 ## [2,] 24 24 38 38 ## [3,] 22 32 22 22 ## [4,] 39 39 32 39 ## [,194] [,195] [,196] [,197] ## [1,] 46 46 44 44 ## [2,] 38 22 24 24 ## [3,] 32 32 38 38 ## [4,] 39 39 22 32 ## [,198] [,199] [,200] [,201] ## [1,] 44 44 44 44 ## [2,] 24 24 24 24 ## [3,] 38 22 22 32 ## [4,] 39 32 39 39 ## [,202] [,203] [,204] [,205] ## [1,] 44 44 44 44 ## [2,] 38 38 38 22 ## [3,] 22 22 32 32 ## [4,] 32 39 39 39 ## [,206] [,207] [,208] [,209] ## [1,] 24 24 24 24 ## [2,] 38 38 38 22 ## [3,] 22 22 32 32 ## [4,] 32 39 39 39 ## [,210] ## [1,] 38 ## [2,] 22 ## [3,] 32 ## [4,] 39 # aplicando el estimador de la media muestral xbar&lt;-apply(s,2,mean)#distribución muestral hist(xbar) # según el teorema, E[xbar]=mux sum(xbar*(1/210)) ## [1] 33.7 mean(xbar) ## [1] 33.7 mean(xbar)==mux ## [1] TRUE Para el caso de la varianza, cuando la población es finita (\\(N\\) conocida, pequeña). La varianza del estimador tiene una forma distinta. (Tarea). \\[V(\\bar{X})=\\left(1-\\frac{n}{N}\\right)*\\frac{S^2}{n}\\] 2.6 Teorema del límite central Teorema: Si \\(\\bar{X}\\) es la media de una muestra aleatoria de tamaño \\(n\\). Tomada de una población \\(U\\) con media \\(\\mu_x\\) y varianza finita \\(\\sigma^2_x\\). Entonces la forma límite de la distribución de: \\[Z=\\frac{\\bar{X}-E[\\bar{X}]}{\\sqrt{V(\\bar{X})}}=\\frac{\\bar{X}-\\mu_x}{\\frac{\\sigma_x}{\\sqrt{n}}}\\] a medida que \\(n \\rightarrow \\infty\\), podemos asegurar que \\(Z\\sim N(0,1)\\), en este marco se puede decir a medida que \\(n\\) es más grande: \\[\\bar{X}\\sim N\\left(\\mu_x,\\frac{\\sigma^2_x}{n}\\right)\\] Nota: esta idea de \\(n\\) grande se usa tradicionalmente el valor de \\(n&gt;30\\), hay textos que plantean \\(n=20\\). 2.6.1 Simulación del teorema del límite central Vamos a suponer los siguiente: \\(N=1000000\\) Vamos a simular a una variable de edad \\(X:Edad\\) \\(Rx \\in [0,100]\\) Vamos tener dos comportamientos de \\(X\\), 1) Volátil (uniforme) 2) Concentrada (Normal) N&lt;-1000000 set.seed(999) x1&lt;-round(runif(N,0,100),0) set.seed(999) x2&lt;-round(abs(rnorm(N,50,15)),0) par(mfrow=c(1,2)) hist(x1,xlim=c(0,100),main=&quot;Uniforme&quot;) hist(x2,xlim=c(0,100),main=&quot;Normal&quot;) dev.off() ## null device ## 1 #### Teorema del limite central #n=30 ,100, 500 n&lt;-30 format(choose(N,n),scientific = F) ## [1] &quot;3768348024714503837228662244820288262846862426804084842200866866628242624608408262040682806208086200444462662662060844642062864868204860466460404466&quot; k&lt;-10000 (k/choose(N,n))*100 ## [1] 2.653683e-142 #simular 10000 muestras distintas de tamaño n y calcular su media. xbar1&lt;-NULL xbar2&lt;-NULL for(i in 1:k){ s1&lt;-sample(x1,n) s2&lt;-sample(x2,n) xbar1[i]&lt;-mean(s1) xbar2[i]&lt;-mean(s2) } #teorema 1 abs(mean(x1)-mean(xbar1)) ## [1] 0.036567 abs(mean(x2)-mean(xbar2)) ## [1] 0.008464333 abs(var(xbar1)-var(x1)/n) ## [1] 0.2840957 abs(var(xbar2)-var(x2)/n) ## [1] 0.02454654 #teorema del limite central par(mfrow=c(1,2)) plot(density(xbar1)) plot(density(xbar2)) dev.off() ## null device ## 1 plot(density(xbar),col=&quot;blue&quot;,lwd=2) points(density(rnorm(10^6,mean(x),sqrt(var(x)*((n-1)/n))/sqrt(n))),type=&quot;l&quot;,col=&quot;red&quot;,lwd=2) Ejemplo, Se tiene una muestra de 50 personas de una población, sobre la cual se mide su estatura en centímetros, se calculó la media muestral de 167.6 cm. Suponiendo un varianza poblacional de 44. Calcular la probabilidad que el estimador de la media sea mayor a 170 cm. Solución. Se pide: \\[P(\\bar{X}&gt;170)\\] Por el teorema del limite central ya que \\(n=50\\) \\[\\bar{X}\\sim N(\\hat{\\mu}_{\\bar{x}}=167.6,\\sigma_{\\bar{x}}=\\sqrt{\\frac{44}{50}})=N(167.6,0.94)\\] \\[P(\\bar{X}&gt;170)=P(Z&gt;2.55)=1-P(Z\\leq2.55)=1-\\phi(2.55)=\\] \\[=1-0.9946139=0.005486\\] 2.7 Distribución muestral para la diferencia de medias Sean dos poblaciones \\(U_1\\) y \\(U_2\\) independientes con medias y varianzas respectivamente: \\(\\mu_{x_1}\\) y \\(\\mu_{x_2}\\), \\(\\sigma^2_{x_1}\\) y \\(\\sigma^2_{x_2}\\). Teorema: La distribución muestral de las diferencias de media \\(\\bar{X_1}-\\bar{X_2}\\) esta tiene una distribución aproximadamente normal (\\(n\\rightarrow \\infty\\)) con medias y varianzas dadas por: \\[E[\\bar{X_1}-\\bar{X_2}]=\\mu_{x_1}-\\mu_{x_2}\\] \\[V(\\bar{X_1}-\\bar{X_2})=\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\] Demostración: \\[E[\\bar{X_1}-\\bar{X_2}]=E[\\bar{X_1}]-E[\\bar{X_2}]=\\mu_{x_1}-\\mu_{x_2}\\] \\[V(\\bar{X_1}-\\bar{X_2})=V(\\bar{X_1})+V(\\bar{X_2})=\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\] Por el teorema del limite central con \\(n\\geq30\\): \\[\\bar{X_1}-\\bar{X_2}\\sim N\\left(\\mu_{x_1}-\\mu_{x_2},\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\right)\\] Ejercicio de clase. Se tienen 2 paralelos de la materia de estadística 2, se obtuvo una muestra de ambos paralelos tal que \\(n_1=40\\), \\(n_2=25\\), las medias muestrales de las notas sobre un examen similar fueron de 15.9 y 17.1, suponer que la varianza poblacional es igual en ambos paralelos y es de 27. Calcular la probabilidad que el rendimiento de ambos paralelos sea el mismo. Solución, por el teorema del limite central \\[\\bar{X_1}-\\bar{X_2}\\sim N\\left(15.9-17.1,\\frac{27}{40}+\\frac{27}{25}\\right)=N(-1.2,\\sigma=1.32)\\] \\[P(\\bar{X_1}-\\bar{X_2}=0)=0\\] \\[P(|\\bar{X_1}-\\bar{X_2}|&lt; \\epsilon)=P(|\\bar{X_1}-\\bar{X_2}|&lt; 0.5)=P(-0.5&lt;\\bar{X_1}-\\bar{X_2}&lt;0.5)=\\] \\[=P(-1.29&lt;\\bar{X_1}-\\bar{X_2}&lt;-0.53)=\\phi(-0.53)-\\phi(-1.29)=\\] \\[=0.2980-0.0985=0.199\\] 2.8 Distribución muestral para la proporción La proporción no es nada más que un caso especial de la media para \\(X\\) que toma valores binarios según alguna característica de interés. Sea \\(P_A=\\frac{\\#A}{N}=\\frac{\\sum_U x_i}{N}\\), \\(x_i=1\\) si \\(i \\in A\\) \\(x_i=0\\) eoc. la proporción de alguna característica de la población. Así la el estimador de la proporción sera: \\[\\hat{P}_A=\\frac{\\sum_s{x_i}}{n}=\\frac{\\#a}{n}\\] Teorema: Para el estadístico \\(\\hat{P}_A\\) se cumple cuando \\(n\\) tiende a infinito los siguientes resultados: \\(E[\\hat{P}_A]=P_A\\) \\(V(\\hat{P}_A)=\\frac{\\sigma^2_A}{n}\\) \\(\\hat{P}_A\\sim N(P_A,\\frac{\\sigma^2_A}{n})\\), cuando \\(n \\rightarrow \\infty\\) Donde \\(\\sigma^2_A\\), sabiendo que \\(x_i\\) es binaria. \\[\\sigma^2_A=\\frac{\\sum_U(x_i-\\mu_x)^2}{N}= P_A *(1-P_A)\\] Demostración \\[\\mu_x=\\frac{\\sum_U{x_i}}{N}=\\frac{\\#A}{N}=P_A\\] \\[\\sigma^2_A=\\frac{\\sum_U(x_i-P_A)^2}{N}=\\frac{\\sum_U x_i^2-2P_A \\sum_U{x_i}+NP_A^2}{N}=\\] \\[=\\frac{NP_A-2P_A NP_A+NP_A^2}{N}=P_A-P_A^2=P_A(1-P_A)\\] N&lt;-100000 x&lt;-round(runif(N,18,60)) a&lt;-(x&gt;30)*1 mean(a)#P_A ## [1] 0.70149 n&lt;-100 r&lt;-10000 pa&lt;-NULL for(i in 1:r){ s&lt;-sample(a,n) pa[i]&lt;-mean(s) } PA&lt;-mean(a) plot(density(pa),col=&quot;blue&quot;,lwd=1.5) points(density(rnorm(10^6,PA,sqrt(PA*(1-PA)/n))),col=&quot;red&quot;,type=&quot;l&quot;,lwd=2) Ejercicio: De una población de 150 estudiantes de la materia de estadística I se toma una muestra de 40 estudiantes, sobre los cuales se realiza un test sobre 100 puntos. Con los siguientes resultados: 59 54 61 58 66 56 35 42 49 65 61 61 58 57 57 60 55 63 66 48 52 54 49 54 55 32 60 40 53 67 54 60 44 74 55 24 43 56 62 50 Calcule: la probabilidad que el promedio de nota sea menor a 50, la probabilidad que el promedio de nota sea mayor a 60 la probabilidad que el promedio de nota se encuentre entre 50 y 55 Solución, Como información \\(N=150\\), \\(n=40\\), \\[\\bar{X}=54.225\\] \\[\\hat{S}^2_x=99.35833\\] Usando el teorema del limite central, podemos decir (aproximar) que: \\[\\bar{X}\\sim N\\left(\\mu_x\\approx\\bar{x},\\frac{\\sigma^2_x}{n}\\approx \\frac{\\hat{S}^2_x}{n} \\right)=N(54.225,2.484)\\] La probabilidad que el promedio de nota sea menor a 50, \\[P(\\bar{X}&lt;50)=P\\left(\\frac{\\bar{X}-\\mu_x}{\\frac{\\sigma_x}{\\sqrt{n}}} &lt;\\frac{50-54.225}{\\sqrt{2.484}}\\right)=P(Z&lt;-2.68)=\\phi(-2.68)=\\] \\[=0.00368\\] la probabilidad que el promedio de nota sea mayor a 60 \\[P(\\bar{X}&gt;60)=P(Z&gt;3.66)=1-P(Z\\leq 3.66)=1-\\phi(3.66)=0.00013\\] la probabilidad que el promedio de nota se encuentre entre 50 y 55 2.9 Distribución muestral para la varianza Recordar que para una población \\(U\\), si observamos a una variable de interés respecto sus características podemos obtener medidas de centralidad y también medidas de variabilidad, por ejemplo, sea \\(X\\) una variables definida para toda la población, y definamos los siguientes parámetros de \\(X\\). \\[\\mu_x=\\frac{\\sum_U x_i}{N}\\] Esta \\(\\mu_x\\) es una medida de centralidad, normalmente conocida como media, promedio de \\(X\\), la otra medida puede ser: \\[\\sigma^2_x=\\frac{\\sum_U (x_i-\\mu_x)^2}{N}\\] \\(\\sigma^2_x\\) es la varianza poblacional Ejemplo, Sea una población de \\(N=5\\) elementos con la variable \\(X=\\{10,15,20,20,35\\}\\), calcular \\(\\mu_x\\) y \\(\\sigma^2_x\\). \\(\\mu_x=20\\) \\(\\sigma^2_x=70\\) Suponer que se toman muestras aleatorias de esta población de tamaño \\(n=3\\) sin reposición. La cantidad de muestras posibles es 10. x&lt;-c(10,15,20,20,35) s&lt;-combn(x,3) s ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 10 10 10 10 10 10 ## [2,] 15 15 15 20 20 20 ## [3,] 20 20 35 20 35 35 ## [,7] [,8] [,9] [,10] ## [1,] 15 15 15 20 ## [2,] 20 20 20 20 ## [3,] 20 35 35 35 #distribución muestral de la media apply(s, 2, mean) ## [1] 15.00000 15.00000 20.00000 ## [4] 16.66667 21.66667 21.66667 ## [7] 18.33333 23.33333 23.33333 ## [10] 25.00000 mean(apply(s, 2, mean)) ## [1] 20 # varianza poblacional vp&lt;-sum((x-mean(x))^2)/5 vp ## [1] 70 Pensemos para el caso de la varianza en posibles estadísticos (estimadores): \\[\\hat{\\theta}_1=\\hat{\\sigma}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n}\\] \\[\\hat{\\theta}_2=\\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] Una forma de elegir quien de estos dos estimadores es mejor, podría ser viendo cual es insesgado. \\[E[\\hat{\\sigma}_x^2]=\\sigma_x^2\\quad ; \\quad E[\\hat{S}_x^2]=\\sigma_x^2\\] theta1&lt;-apply(s,2,var)*(2/3) theta2&lt;-apply(s,2,var) theta1 ## [1] 16.666667 16.666667 116.666667 ## [4] 22.222222 105.555556 105.555556 ## [7] 5.555556 72.222222 72.222222 ## [10] 50.000000 theta2 ## [1] 25.000000 25.000000 175.000000 ## [4] 33.333333 158.333333 158.333333 ## [7] 8.333333 108.333333 108.333333 ## [10] 75.000000 mean(theta1) ## [1] 58.33333 mean(theta2) ## [1] 87.5 vp ## [1] 70 #cuasi varianza cvp&lt;-sum((x-mean(x))^2)/4 cvp ## [1] 87.5 La cuasi varianza se denota como: \\[S^2_x=\\frac{\\sum_U (x_i-\\mu_x)^2}{N-1}\\] Se puede demostrar que: \\[E[\\hat{S}_x^2]=S^2_x\\] &gt; Teorema Sea \\(X_1,X_2,\\ldots,X_n\\) una muestra aleatoria extraída de una población Normal \\(N(\\mu_x,\\sigma^2_x)\\), definamos al estadístico (varianza muestral): \\[\\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] Entonces, se cumple \\[\\chi^2=\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}\\sim \\chi^2(n-1)\\] 2.10 Distribución \\(\\chi^2\\) Se dice que una variable aleatoria \\(X\\) tiene una distribución Chi-cuadrado \\(\\chi^2\\) con \\(v\\) grados de libertad. Se escribe como: \\(X \\sim \\chi^2(v)\\), donde el \\(Rx=\\{x&gt;0\\}\\), si su función de densidad es: \\[f(x)=\\frac{1}{2^\\frac{v}{2} \\Gamma(\\frac{v}{2})}*x^{\\frac{v}{2}-1}*e^{-\\frac{x}{2}}\\] curve(dchisq(x,1),xlim=c(0,60),ylim=c(0,0.4)) curve(dchisq(x,2),col=2,add=T) for(v in 2:50){ curve(dchisq(x,v),add=T,col=v) } curve(dchisq(x,10),xlim=c(0,60),main=&quot;v=10&quot;) abline(v=10) curve(dchisq(x,100),xlim=c(0,300),main=&quot;v=100&quot;) abline(v=100) Donde, \\[E[X]=v \\quad ; \\quad V(X)=2v\\] Ejercicio 1, Sea X una va, tal que: \\(X\\sim \\chi^2(35)\\). Calcular la probabilidad que: \\[P(30&lt;X&lt;40)=P(X&lt;40)-P(X&lt;30)=0.4503\\] \\[P(X&gt;35)=1-P(X\\leq 35)=0.4682\\] \\[P(X&lt;32)=0.3863\\] En el R curve(dchisq(x,35),xlim=c(0,100),main=&quot;v=35&quot;) abline(v=c(30,40)) pchisq(40,35)-pchisq(30,35) ## [1] 0.4503496 1-pchisq(35,35) ## [1] 0.4682027 pchisq(32,35) ## [1] 0.386295 Ejercicio 2, Para una muestra aleatoria de \\(n=30\\), se busca estimar la varianza poblacional, mediante la varianza muestral, suponiendo que la variable de interés \\[X\\sim N(\\mu_x,\\sigma_x^2=16)\\] Encuentre la probabilidad que la varianza muestral se encuentre entre 12 y 18. \\[P\\left(12&lt;\\hat{S}^2_x&lt;18\\right)=?\\] Solución, \\[P\\left(12&lt;\\hat{S}^2_x&lt;18\\right)=P\\left(12*29/16&lt;(n-1)\\hat{S}^2_x/\\sigma^2_x&lt;18*29/16\\right)=\\] \\[\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=Y\\sim \\chi^2(29)\\] \\[P(21.75&lt;Y&lt;32.625)=0.5373\\] pchisq(32.625,29)-pchisq(21.75,29) ## [1] 0.5372716 Ejercicio 3, Encuentre la probabilidad de que una muestra aleatoria de \\(n=45\\) de un población normal con varianza \\(\\sigma_x^2=14\\), tenga una varianza muestral \\(\\hat{S^2}\\) entre 15 y 22. Solución: \\[P\\left(15&lt;\\hat{S}^2_x&lt;22\\right)=P\\left(15*44/14&lt;(n-1)\\hat{S}^2_x/\\sigma^2_x&lt;22*44/14\\right)=\\] \\[\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=Y\\sim \\chi^2(44)\\] \\[P(47.14&lt;Y&lt;69.14)=0.34\\] pchisq(69.14,44)-pchisq(47.14,44) ## [1] 0.3362603 pchisq(22*44/14,44)-pchisq(15*44/14,44)# más exacto ## [1] 0.3361588 Tomar en cuenta que: \\[\\chi^2=\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}\\sim \\chi^2(n-1)\\] #ejemplo para usar R para calcular probabilidades de la Chi2 pchisq(4,10) # F(t)=P(X&lt;t): F(4) ## [1] 0.05265302 Nota, \\[\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}=\\sum_s\\left(\\frac{x_i-\\bar{x}}{\\sigma_x}\\right)^2\\] En el fondo la distribución \\(\\chi^2\\) es la suma de variables aleatorias Normales estándar al cuadrado. 2.11 Distribución t-student Teorema Sea \\(Z\\) una variable aleatoria normal estándar y \\(V\\) una variable aleatoria chi-cuadrado con \\(v\\) grados de libertad. Si \\(Z\\) y \\(V\\) son independientes, entonces la distribución de la variable aleatoria \\(X\\), donde: \\[X=\\frac{Z}{\\sqrt{V/v}}\\] Se comporta como una distribución \\(t\\) con \\(v\\) grados de libertad. En notación, decimos \\(X\\sim t(v)\\). \\[f(x)=\\frac{\\Gamma(\\frac{v+1}{2})}{\\Gamma{(\\frac{v}{2})}\\sqrt{v\\pi} }\\left(1+\\frac{x^2}{v} \\right)^{-(\\frac{v+1}{2})}, \\quad -\\infty&lt;x&lt;\\infty\\] Al igual que la distribución normal estándar, la \\(t\\) es simétrica al rededor del cero. Y levemente más plana que una normal. Apariencia de la \\(t\\) curve(dt(x,2),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;v=2&quot;) curve(dt(x,2),xlim=c(-5,5),ylim=c(0,0.4),lwd=2) for(v in 3:40){ curve(dt(x,v),xlim=c(-5,5),col=v,add=T) } Nota: Cuando \\(v\\rightarrow \\infty\\) \\[t \\sim N\\left(\\mu=0,\\sigma^2=\\frac{v}{v-2}\\right)\\] v&lt;-5 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;5 grados de libertad&quot;) curve(dt(x,v),xlim=c(-5,5),main=&quot;t-student (v=5)&quot;,col=&quot;red&quot;,add=T) v&lt;-30 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;30 grados de libertad&quot;) curve(dt(x,v),xlim=c(-5,5),main=&quot;t-student (v=30)&quot;,col=&quot;red&quot;,add=T) v&lt;-50 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;50 grados de libertad&quot;) curve(dt(x,v),xlim=c(-5,5),main=&quot;t-student (v=50)&quot;,col=&quot;red&quot;,add=T) v&lt;-100 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4),main=&quot;100 grados de libertad&quot;) curve(dt(x,v),xlim=c(-5,5),main=&quot;t-student (v=100)&quot;,col=&quot;red&quot;,add=T) Corolario Sean \\(X_1, X_2, \\ldots, X_n\\) variables aleatorias e independientes e idénticamente distribuidas (iid) \\(X_i\\sim N(\\mu,\\sigma^2_x), \\quad i=\\{1,\\ldots,n\\}\\). Sean los estimadores: \\[\\bar{X}=\\frac{\\sum_s x_i}{n} \\quad y \\quad \\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{X})^2}{n-1}\\] Entonces, \\[ \\frac{\\bar{X}-\\mu}{\\hat{S}/\\sqrt{n}}\\sim t(v=n-1) \\] En R, para obtener \\(P(X&lt;t)=F(t)\\) con \\(X\\sim t(v)\\). curve(dt(x,df=10),xlim=c(-5,5),main=&quot;v=10&quot;) # P(X&lt;2)=F(2), X ~ t(v=10) pt(2,10) ## [1] 0.963306 pt(0,10) ## [1] 0.5 Ejercicio Calcular, si \\(X\\sim t(v=30)\\) \\(P(X&gt;0.5)=1-P(X\\leq 0.5)= 0.3103615\\) \\(P(-1&lt;X&lt;1.2)=F(1.2)-F(-1)=0.7175805\\) \\(P(X&lt;3)=F(3)=0.997305\\) curve(dt(x,df=30),xlim=c(-5,5),main=&quot;v=30&quot;) abline(v=0.5) 1-pt(0.5,30) ## [1] 0.3103615 pt(1.2,30)-pt(-1,30) ## [1] 0.7175805 pt(3,30) ## [1] 0.997305 Suponer que se puede utilizar la norma para aproximar a la variable anterior, usando esa aproximación calcular las mismas probabilidades. \\[X\\sim N\\left(0,\\sigma^2=\\frac{30}{28}\\right)\\] \\(P(X&gt;0.5)=1-P(X\\le 0.5)=0.3145316\\) \\(P(-1&lt;X&lt;1.2)=F(1.2)-F(-1)=0.709836\\) \\(P(X&lt;3)=F(3)=0.9981239\\) 1-pnorm(0.5,0,sqrt(30/28)) ## [1] 0.3145316 pnorm(1.2,0,sqrt(30/28))-pnorm(-1,0,sqrt(30/28)) ## [1] 0.709836 pnorm(3,0,sqrt(30/28)) ## [1] 0.9981239 2.12 Distribución Fisher (F) Teorema Sean \\(U\\) y \\(V\\) dos variables aleatorias independientes, con \\(U\\sim \\chi^2(v_1)\\) y \\(V \\sim \\chi^2(v_2)\\). Y sea la variable \\(X\\) definida como: \\[X=\\frac{U/v_1}{V/v_2}\\] Así, decimos que \\(X\\) se distribuye como una Fisher, \\(X\\sim F(v_1,v_2)\\), donde estas \\(v_1\\) y \\(v_2\\) son los grados de libertad de la Fisher. La forma de la distribución \\(f(x)\\) es: \\[ f(x)=\\frac{\\left(\\frac{v_1}{v_2} \\right)^{v_1/2} x^{v_1/2-1}\\Gamma{(\\frac{v_1+v_2}{2})} }{\\Gamma{(\\frac{v_1}{2})} \\Gamma{(\\frac{v_2}{2})}\\left(1+\\frac{v_1}{v_2}x \\right)^{(v_1+v_2)/2}}, \\quad x&gt;0 \\] 2.12.1 Para las varianzas muestrales Suponga que las muestras aleatorias de tamaños \\(n_1\\) y \\(n_2\\) se selecciona de 2 poblaciones normales con varianzas \\(\\sigma^2_1\\) y \\(\\sigma^2_2\\) respectivamente. Sabemos: \\[\\chi^2_1=\\frac{(n_1-1)\\hat{S}_1^2}{\\sigma^2_1}\\sim \\chi^2(v_1=n_1-1)\\] \\[\\chi^2_2=\\frac{(n_2-1)\\hat{S}_2^2}{\\sigma^2_2}\\sim \\chi^2(v_2=n_2-1)\\] Teorema: Si \\(\\hat{S_1}\\) y \\(\\hat{S_2}\\) son los estimadores de la varianza de muestras aleatorias independientes entre ellas de tamaño \\(n_1\\) y \\(n_2\\), tomadas de poblaciones normales con varianzas \\(\\sigma^2_1\\) y \\(\\sigma^2_2\\) entonces: \\[\\frac{\\hat{S_1}^2/\\sigma^2_1}{\\hat{S_2}^2/\\sigma^2_2}=\\frac{\\hat{S}_1^2*\\sigma^2_2}{\\hat{S}_2^2*\\sigma^2_1}\\sim F(v_1=n_1-1,v_2=n_2-1)\\] curve(df(x,10,10),xlim=c(0,6),ylim=c(0,2)) curve(df(x,50,50),xlim=c(0,6),add = T,col=&quot;red&quot;) curve(df(x,100,100),xlim=c(0,6),add = T,col=&quot;blue&quot;) abline(v=1,lty=2) curve(df(x,20,100),xlim=c(0,6),ylim=c(0,2)) curve(df(x,100,20),xlim=c(0,6),ylim=c(0,2),col=&quot;red&quot;,add=T) curve(df(x,50,200),xlim=c(0,6),ylim=c(0,2),col=&quot;blue&quot;,add=T) abline(v=1,lty=2) En R \\[P(X&lt;t)=F(t)\\] Donde \\(X \\sim F(v1,v2)\\). Suponer que \\[X\\sim F(15,16)\\] \\[P(X&lt;1.5)= 0.7851078\\] pf(1.5,15,16) ## [1] 0.7851078 #P(0.5&lt;X&lt;1) pf(1,15,16)-pf(0.5,15,16) ## [1] 0.4085825 2.13 Ejercicios Suponga que una variable aleatoria se distribuye como una normal con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Extraiga una muestra aleatoria de cinco observaciones. ¿Cuál es la función de densidad conjunta de la muestra? Solución, \\[X\\sim N(\\mu,\\sigma^2); \\quad X_1,X_2,\\ldots,X_5\\] \\[f(X_1,X_2,\\ldots,X_5)=f(X_1)*f(X_2)*\\ldots*f(X_5)\\] Notar que estas \\(X_i\\) son independientes e idénticamente distribuidas. \\[f(x_1,x_2,x_3,x_4,x_5)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(\\frac{x_1-\\mu}{\\sigma})^2}*\\ldots*\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(\\frac{x_5-\\mu}{\\sigma})^2}\\] Para cierta prueba de aptitud se sabe con base en la experiencia que el número de aciertos es 1000 con una desviación estándar de 125. Si se aplica la prueba a 100 personas seleccionadas al azar, aproximar las siguientes probabilidades que involucran a la media muestral: \\[P(985&lt;\\bar{x}&lt; 1015)\\] \\[P(\\bar{x}&gt; 1020)\\] Solución, \\(X=\\) Nota de la prueba de aptitud. \\[X\\sim .(\\mu=1000,\\sigma=125)\\] Ya que \\(n\\) es grande, podemos usar el teorema del limite central: \\[\\bar{x}\\sim N(1000,125/\\sqrt{100})=N(1000,12.5)\\] \\[P(985&lt;\\bar{x}&lt; 1015)=P(-1.2&lt;z&lt;1.2)=\\phi(1.2)-\\phi(-1.2)=0.7698607\\] \\[P(\\bar{x}&gt; 1020)=1-P(\\bar{x}\\leq 1020)= 0.05479929\\] Si se obtiene una muestra aleatoria de tamaño 16 de una distribución normal con media y varianza desconocida. obtener: \\[P\\left(\\hat{S}^2/\\sigma^2\\leq 2.041\\right)=?\\] &gt; Solución. Se tiene que \\(n=16\\). \\(X\\sim N(\\mu,\\sigma^2)\\). \\[P\\left((n-1)\\hat{S}^2/\\sigma^2\\leq 2.041*15\\right)=P(X\\leq 30.614)= 0.9901098\\] \\[X\\sim \\chi^2(15)\\] La calificación promedio de los estudiantes de primer año en un examen de aptitudes en cierta universidad es 540, con una desviación estándar de 50. Suponga que las medias se miden con cualquier grado de precisión. ¿Cuál es la probabilidad de que dos grupos seleccionados al azar, que constan de 32 y 50 estudiantes, respectivamente, difieran en sus calificaciones promedio por: más de 20 puntos? una cantidad entre 5 y 10 puntos? Solución, como información se tienen 2 muestras, \\(n_1=32\\), \\(n_2=50\\). Sea \\(X\\sim . (\\mu=540,\\sigma=50)\\), nos piden analizar la diferencia de las medias de los dos grupos, \\(\\bar{X_1}\\), \\(\\bar{X}_2\\): más de 20 puntos? \\[P( |\\bar{X_1}-\\bar{X}_2| &gt;20)=1-P( |\\bar{X_1}-\\bar{X}_2| \\leq 20)=1-P( -20\\leq \\bar{X_1}-\\bar{X}_2 \\leq 20)\\] Para la diferencia de medias \\(\\bar{X_1}-\\bar{X}_2\\sim N(\\mu_{\\bar{X_1}-\\bar{X}_2}=\\mu_x-\\mu_x=0,\\sigma^2_{\\bar{X_1}-\\bar{X}_2}=\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}=50^2/32+50^2/50)\\), entonces, \\(Y=\\bar{X_1}-\\bar{X}_2\\sim N(\\mu_y=0,\\sigma^2_y=128.125)\\), bajo el supuesto que las \\(\\bar{X}_1\\) y \\(\\bar{X}_2\\) tienden a ser normales por el teorema del límite central. \\[P( -20\\leq \\bar{X_1}-\\bar{X}_2 \\leq 20)=P(-20/11.31\\leq Z \\leq 20/11.31)\\approx \\phi(1.77)-\\phi(-1.77)=\\] \\[=0.962-0.038=0.9232\\] \\[P( |\\bar{X_1}-\\bar{X}_2| &gt;20)=1-0.9232=0.0768\\] Suponga que las varianzas muestrales son mediciones continuas. Calcule la probabilidad de que una muestra aleatoria de 25 observaciones, de una población normal con varianza \\(\\sigma^2 = 6\\), tenga una varianza muestral \\(\\hat{S}^2\\) mayor que 9.1 entre 3.462 y 10.745. \\[P(\\hat{S}^2&gt;9.1)=P\\left(\\frac{(n-1)\\hat{S^2}}{\\sigma^2_x}&gt; 24*9.1/6 \\right)=P(\\chi^2&gt;36.4)=1-P(\\chi^2\\leq36.4)=\\] \\[=1-0.9498=0.0502\\] Teniendo en cuenta que \\(\\chi^2\\sim \\chi^2(v=24)\\) El viaje en un autobús especial para ir de un campus de una universidad al campus de otra en una ciudad toma, en promedio, 28 minutos, con una desviación estándar de 5 minutos. En cierta semana un autobús hizo el viaje 40 veces. ¿Cuál es la probabilidad de que el tiempo promedio del viaje sea mayor a 30 minutos? Solución, \\(n=40\\), Sea \\(X\\) una va. que explica el tiempo de viaje entre los dos campus. \\(X \\sim .(\\mu_x=28,\\sigma=5)\\), nos pide: \\[P(\\bar{X}&gt;30)\\] Recordar por el teorema del límite central que \\(\\bar{X} \\sim N(\\mu_{\\bar{x}}=28,\\sigma^2_{\\bar{x}}= \\sigma^2_x/n=25/40)\\) cuando \\(n&gt;30\\). \\[P(\\bar{X}&gt;30)=P\\left(\\frac{\\bar{X}-\\mu_x}{\\sigma_x/\\sqrt{n}} &gt;\\frac{30-28}{5/\\sqrt{40}}\\right)=P(Z&gt;2.52)=1-P(Z\\leq 2.52)\\approx 1-\\phi(2.52)=\\] \\[=1-0.9941=0.0059\\] La capacidad máxima de un ascensor es de 500 kilos. Si la distribución \\(X\\) de los pesos de los usuarios es \\[X\\sim N(\\mu=70,\\sigma^2=100)\\] Cuál es la probabilidad de que 8 pasajeros sobrepasen ese límite? Cuál es la probabilidad de que 7 pasajeros sobrepasen ese límite? Cuál es la probabilidad de que 6 pasajeros sobrepasen ese límite? Solución, Sean \\(X_1, X_2, \\ldots,X_p\\) con \\(p\\) la cantidad de pasajeros en el ascensor, suponemos que estas \\(X_i\\) son iid \\(X_i\\sim N(\\mu=70,\\sigma=10)\\). Se pide: \\[P(Y=X_1+X_2+\\ldots+X_p&gt;500)\\] Notar que la suma de variables normales es también normal. \\(Y\\sim N(\\mu_y=p*\\mu_x,\\sigma^2_y=p*\\sigma^2_x )\\) \\[E[Y]=E[X_1+\\ldots+X_p]=E[X_1]+\\ldots+E[X_p]=\\mu_x+\\ldots+\\mu_x=p*\\mu_x\\] \\[V(Y)=V(X_1+\\ldots+X_p)=V(X_1)+\\ldots+V(X_p)=\\sigma^2_x+\\ldots+\\sigma^2_x=p*\\sigma^2_x\\] Cuál es la probabilidad de que 8 pasajeros sobrepasen ese límite; \\(Y\\sim N(\\mu_y=8*70=560,\\sigma^2_y=8*100=800)\\) \\[P(Y&gt;500)=P(Z&gt;\\frac{500-560}{\\sqrt{800}})=P(Z&gt;-2.12)=1-P(Z\\leq -2.12)=1-\\phi(-2.12)=\\] \\[=1-0.017=0.983\\] Cuál es la probabilidad de que 7 pasajeros sobrepasen ese límite, \\(Y\\sim N(\\mu_y=490,\\sigma^2_y=700)\\) \\[P(Y&gt;500)=P(Z&gt;0.3779)=1-\\phi(0.3779)=1-0.647=0.353\\] Cuál es la probabilidad de que 6 pasajeros sobrepasen ese límite. (Ejercicio) "],["estimaciones-de-una-y-dos-muestras.html", "3 Estimaciones de una y dos muestras 3.1 Inferencia estadística 3.2 Estimadores puntuales 3.3 Estimación por intervalos de confianza 3.4 Ejercicios", " 3 Estimaciones de una y dos muestras 3.1 Inferencia estadística El proceso por el cual, mediante una muestra estadísticamente seleccionada se busca describir a la población/universo de la cual esta proviene. Podemos clasificar a la inferencia estadística en: Inferencia descriptiva: Tiene el único objetivo de describir a la población mediante la muestra, tradicionalmente se enfoca en estimaciones comunes como; la media, la varianza, total, un porcentaje, diferencia de medias, diferencia de proporciones. Estimación puntual: \\(\\hat{\\theta}\\) Estimación por intervalos: \\([ \\hat{\\theta}_{LI},\\hat{\\theta}_{LS}]\\) Pruebas de hipótesis: \\(\\theta=k\\), \\(\\theta&gt;k\\), \\(\\theta&lt;k\\) Tamaño de la muestra (\\(n\\)): \\(n=f(U,V(\\hat{\\theta}),\\hat{\\theta},\\ldots)\\) Inferencia predictiva: Tiene una idea de estudiar; por un lado, la evolución de las estimaciones y sus posibles valores futuros (series de tiempo), por otro lado, le interesa conocer las relaciones (no causales) entre las variables. Series de tiempo Modelos lineales Técnicas multivariantes etc. Inferencia causal: Tiene el objetivo de medir la relación causal entre variables. \\(X \\rightarrow Y\\) Diseños experimentales Diseños cuasi-experimentales Modelos estructurales Etc. Tarea: Indagar a que se refiere la inferencia bayesiana. 3.2 Estimadores puntuales Recordemos que tenemos un universo \\(U\\) de tamaño \\(N\\). \\[U=\\{u_1, u_2,\\ldots,u_N\\}\\] Donde cada unidad del universo tiene variables (características) asociadas, pensemos en \\(p\\) características. \\[u_i=\\{X_{i1},X_{i2}, \\ldots , X_{ip}\\}\\] Un parámetro es una función sobre el universo y sus variables, lo denotamos por \\(\\theta\\) \\[\\theta=f(U,X)\\] Un estimador se construye a partir de la definición de una estadística (\\(\\Theta\\)) y tiene el objetivo de aproximar de la mejor forma a un parámetro. \\(\\hat{\\theta}\\rightarrow \\theta\\). El estimador \\(\\hat{\\theta}\\) se construye a partir de una muestra aleatoria (\\(s\\)) de tamaño \\(n\\) obtenida de \\(U\\). Nota: Para un parámetro \\(\\theta\\), pueden existir muchos estimadores candidatos: \\(\\hat{\\theta}_1,\\hat{\\theta}_2, \\hat{\\theta}_3,...\\), la pregunta es ¿Cuál es mejor?. Existen al menos dos criterios: 3.2.1 Estimador insesgado \\[E[\\hat{\\theta}]=\\theta\\] 3.2.2 Estimador eficiente Supongamos que tenemos dos estimadores para \\(\\theta\\), \\(\\hat{\\theta}_1\\), \\(\\hat{\\theta}_2\\), el estimador más eficiente entre los dos será quien tenga el valor más pequeño en su varianza. \\(min(V(\\hat{\\theta_1}),V(\\hat{\\theta_2})) \\rightarrow \\hat{\\theta}\\)$ Ejemplo. Sea el vector \\(X=\\{10,10,20,25,30\\}\\) de una población con \\(N=5\\), se define una muestra de \\(n=3\\) y se busca estimar la media de \\(X\\): \\(\\mu_x\\). A partir de los estimadores de la media y la mediana muestral. Determinar: Son estimadores insesgados Cuál estimador es más eficiente Suponga un muestreo sin reposición. Solución. x&lt;-c(10,10,20,25,30) choose(5,3) ## [1] 10 s&lt;-combn(x,3) #distribución muestral de la media muestral dmedia&lt;-apply(s,2,mean) #distribución muestral de la mediana muestral dmediana&lt;-apply(s,2,median) # Son estimadores insesgados #media sum(dmedia*(1/10)) # E[] ## [1] 19 mean(dmedia) ## [1] 19 #media sum(dmediana*(1/10)) # E[] ## [1] 18.5 mean(dmediana) ## [1] 18.5 #parámetro media poblacional mean(x) ## [1] 19 # Cuál estimador es más eficiente sum((dmedia-mean(dmedia))^2*1/10) # media muestral ## [1] 10.66667 sum((dmediana-mean(dmediana))^2*1/10) # mediana muestral ## [1] 35.25 Para este ejercicio, la media muestral es insesgado y más eficiente que la mediana muestral. Nota Los principales problemas de estimación ocurren con frecuencia para estimar: El promedio o media de una población \\(\\mu\\) \\[\\mu_X=\\frac{\\sum_U X_i}{N}\\] La varianza poblacional \\(\\sigma^2\\) \\[\\sigma^2=\\frac{\\sum_U (X_i-\\mu_X)^2}{N}\\] La proporción de una característica en la población \\(P\\) \\[P_A=\\frac{\\#A}{N}=\\frac{\\sum_{U} X_i}{N}; \\quad \\{X_i=1, \\quad i \\in A, X_i=0,\\quad eoc\\}\\] La diferencia de medias de dos poblaciones \\(\\mu_1-\\mu_2\\) La diferencia de proporciones de dos poblaciones \\(P_1-P_2\\) Estimaciones puntuales razonables de estos parámetros son las siguientes: Para \\(\\mu\\), la estimación es \\(\\hat{\\mu_x}=\\bar{X}\\) la media muestral \\[\\bar{X}=\\frac{\\sum_s X_i}{n}\\] * Para \\(\\sigma^2\\), la estimación es \\(\\hat{\\sigma}^2=\\hat{S^2}\\), la varianza muestral \\[\\hat{S}^2=\\frac{\\sum_s (X_i-\\bar{X})^2}{n-1}\\] * Para \\(P\\), la estimación es \\(\\hat{P}\\), la proporción muestral \\[\\hat{P}_A=\\frac{\\#_sA}{N}=\\frac{\\sum_{s} X_i}{N}; \\quad \\{X_i=1, \\quad i \\in A, X_i=0,\\quad eoc\\}\\] * Para \\(\\mu_1-\\mu_2\\), la estimación es \\(\\hat{\\mu_1}-\\hat{\\mu}_2=\\bar{X}_1-\\bar{X}_2\\), la diferencia entre las medias de las muestras de dos muestras aleatorias independientes. * Para \\(P_1-P_2\\), la estimación es \\(\\hat{P_1}-\\hat{P}_2\\), la diferencia entre las proporciones de las muestras de dos muestras aleatorias independientes. Ejercicio, Suponga que \\(X\\) es una variable aleatoria con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Sea \\(X_1, X_2, \\ldots, X_n\\) una muestra aleatoria de tamaño \\(n\\) de \\(X\\). DEMOSTRAR que la media de muestra \\(\\bar{X}\\) y la varianza muestral \\(\\hat{S}^2\\) son estimadores insesgados de \\(\\mu\\) y \\(\\sigma^2\\), respectivamente. Como información; \\(E[X]=\\mu\\), \\[V(X)=\\sigma^2=E[X^2]-E[X]^2\\]. Solución, \\[E[\\bar{X}]=E\\left[\\frac{\\sum_s X_i}{n} \\right]=\\frac{1}{n}\\left(\\sum_s E[X_i]\\right)=\\frac{1}{n}\\left(\\sum_{i=1}^n \\mu\\right)=\\mu \\] Así, \\[V(\\bar{X})=\\frac{\\sigma^2}{n}=E[\\bar{X}^2]-E[\\bar{X}]^2=E[\\bar{X}^2]-\\mu^2\\] \\[E[\\hat{S^2}]=E\\left[\\frac{\\sum_s (X_i-\\bar{X})^2}{n-1} \\right]=\\frac{1}{n-1} E\\left[\\sum_s (X_i^2-2X_i \\bar{X}+\\bar{X}^2) \\right]=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2\\bar{X} \\sum_sX_i +n\\bar{X}^2 \\right]= \\] \\[=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2\\bar{X}n\\frac{\\sum_s X_i}{n} +n\\bar{X}^2 \\right]=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2n\\bar{X}^2 +n\\bar{X}^2 \\right]=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-n\\bar{X}^2\\right]=\\] \\[=\\frac{1}{n-1}\\left(\\sum_s E[X_i^2]-nE[\\bar{X}^2] \\right)= \\alpha\\] Notar \\(\\sigma^2=E[X^2]-E[X]^2=E[X^2]-\\mu^2\\) para un \\(X_i\\), \\(\\sigma^2=E[X_i^2]-E[X_i]^2=E[X_i^2]-\\mu^2\\), entonces, \\(E[X_i^2]=\\sigma^2+\\mu^2\\). Por otro lado \\(E[\\bar{X}^2]=\\frac{\\sigma^2}{n}+\\mu^2\\), Así: \\[\\alpha=\\frac{1}{n-1}\\left[\\sum_s (\\sigma^2+\\mu) -n \\left(\\frac{\\sigma^2}{n}+\\mu \\right) \\right]=\\frac{1}{n-1}\\left[ n \\sigma^2+n\\mu -\\sigma^2-n\\mu \\right]=\\frac{\\sigma^2(n-1)}{n-1}=\\sigma^2\\] 3.2.3 Error cuadrático medio (ECM) Este se define para un estimador como: \\[ECM(\\hat{\\theta})=E\\left[(\\hat{\\theta}-\\theta)^2\\right]\\] Recordar que \\(V(\\hat{\\theta})=E\\left[(\\hat{\\theta}-E[\\hat{\\theta}])^2\\right]\\). \\[ECM(\\hat{\\theta})=E\\left[\\left[(\\hat{\\theta}-E[\\hat{\\theta}])-(\\theta-E[\\hat\\theta]) \\right]^2\\right]=E\\left[(\\hat{\\theta}-E[\\hat{\\theta}])^2-2(\\hat{\\theta}-E[\\hat{\\theta}])(\\theta-E[\\hat\\theta])+ (\\theta-E[\\hat\\theta])^2 \\right]=\\] \\[=E[(\\hat{\\theta}-E[\\hat{\\theta}])^2]-2(\\theta-E[\\hat\\theta])E\\left[\\hat{\\theta}-E[\\hat{\\theta}]\\right]+E[(\\theta-E[\\hat\\theta])^2]=V(\\hat\\theta)=V(\\hat{\\theta})+E[(\\theta-E[\\hat\\theta])^2]=\\] \\[=V(\\hat{\\theta})+sesgo(\\hat\\theta)^2\\] 3.2.4 Cota de Cramer Rao Es posible obtener una cota inferior de la varianza de todos los estimadores (\\(\\hat{\\theta}_1, \\hat{\\theta}_2,\\ldots\\)) insesgados de \\(\\theta\\). Sea \\(\\hat{\\theta}\\) un estimador insesgado del parámetro \\(\\theta\\), basado en una muestra aleatorio de \\(n\\) observaciones, y considérese que \\(f(x,\\theta)\\) denota la función de distribución de probabilidades de una variable aleatoria \\(X\\). Entonces una cota inferior en la varianza de \\(\\hat{\\theta}\\) es: \\[V(\\hat{\\theta})\\geq\\frac{1}{nE\\left\\{ \\left[\\frac{d}{d\\theta }ln f(X,\\theta) \\right]^2 \\right\\}}\\] Esta desigualdad se denomina cota de Cramer Rao. Si un estimador insesgado \\(\\hat{\\theta}\\) satisface la desigualdad, se tratará del estimador insesgado de varianza mínima de \\(\\theta\\). Ejemplo, Demostrar que la media muestra \\(\\bar{X}\\) es el estimador insesgado de varianza mínima de la media de una distribución normal con varianza conocida. Sea \\(X\\sim N(\\mu,\\sigma^2)\\), sabemos \\(E[\\bar{X}]=\\mu\\) \\[f(X,\\mu)=\\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2}\\] \\[ln f(X,\\mu)=ln\\left( \\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2} \\right)=-ln\\left( \\sqrt{2\\pi} \\sigma \\right) -\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2\\] \\[E\\left\\{\\left[ \\frac{d}{d\\mu} ln f(X,\\mu)\\right]^2 \\right\\}=E\\left\\{ \\left[ \\frac{(X-\\mu)}{\\sigma^2} \\right]^2\\right\\} =E\\left[\\frac{(X-\\mu)^2}{\\sigma^4} \\right]=\\frac{E[(X-\\mu)^2]}{\\sigma^4}=\\] \\[=\\frac{\\sigma^2}{\\sigma^4}=\\frac{1}{\\sigma^2}\\] Finalmente, para la cota de Cramer-Rao \\[V(\\bar{X})\\geq \\frac{1}{\\frac{n}{\\sigma^2}}=\\frac{\\sigma^2}{n}=V(\\bar{X})\\] 3.2.5 Método de Maxima verosimilitud Suponga que \\(X\\) es una va, con distribución \\(f(X,\\theta)\\), donde \\(\\theta\\) es un parámetro desconocido. Sean \\(X_1, X_2,\\ldots, X_n\\) va. iid. como \\(X\\), la muestra de tamaño \\(n\\). La función de probabilidad de las \\(n\\) va. se escribe como: \\[f(X_1,X_2, \\ldots,X_n,\\theta)=f(X_1,\\theta)*f(X_2,\\theta)*\\ldots*f(X_n,\\theta)=L(\\theta)\\] El estimador de máxima verosimilitud de \\(\\theta\\) es el valor que maximiza la función de probabilidad \\(L(\\theta)\\). Pasos para obtener el estimador de máxima verosimilitud para un parámetro \\(\\theta\\) Obtener \\(L(\\theta)\\) Calcular \\(ln [L(\\theta)]\\) Resolver la ecuación: \\[\\frac{d}{d\\theta} ln [L(\\theta)]=0\\] En el caso de que tengamos más de un parámetro, los pasos son: Obtener \\(L(\\theta_1,\\theta_2,\\ldots)=f(X_1,\\theta_1,\\theta_2,\\ldots)*\\ldots*f(X_n,\\theta_1,\\theta_2,\\ldots)\\) Calcular \\(ln [L(\\theta_1,\\theta_2,\\ldots)]\\) Resolver el sistema de ecuaciones: \\[\\frac{\\partial }{\\partial \\theta_1} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] \\[\\frac{\\partial }{\\partial \\theta_2} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] \\[\\frac{\\partial }{\\partial \\theta_p} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] Ejemplos Para el caso discreto, sea \\(x\\) una variable aleatoria con función de probabilidad: \\[\\pi(x)=P(X=x)=\\theta (1-\\theta)^{x-1}; \\quad x=1,2,3,\\ldots; \\quad 0\\leq\\theta\\leq1\\] Encontrar al estimador de máxima verosimilitud de \\(\\theta\\) Solución, Sea \\(X_1, X_2, \\ldots, X_n\\) una muestra aleatoria, donde las \\(X_i\\) son iid. Así: \\[\\pi(x_i)=P(X_i=x_i)=\\theta (1-\\theta)^{x_i-1}; \\quad x_i=1,2,3,\\ldots; \\quad 0\\leq\\theta\\leq1\\] \\[L(\\theta)=\\prod_{i=1}^n \\theta (1-\\theta)^{x_i-1}=\\theta^n (1-\\theta)^{\\sum_s x_i - n}\\] \\[ln (L (\\theta))=ln \\left(\\theta^n (1-\\theta)^{\\sum_s x_i - n} \\right)=n ln (\\theta)+\\left(\\sum_s x_i -n\\right) ln(1-\\theta)\\] \\[\\frac{d}{d \\theta} ln(L(\\theta))=\\frac{n}{\\theta}-\\left(\\sum_s x_i -n\\right)\\frac{1}{1-\\theta}=0\\] \\[\\frac{n}{\\theta}=\\frac{\\left(\\sum_s x_i -n\\right)}{1-\\theta}\\rightarrow n-n\\theta=\\theta \\sum_s x_i-n\\theta\\] \\[n=\\theta \\sum_s x_i\\] \\[\\hat{\\theta}=\\frac{n}{\\sum_s x_i}=\\frac{1}{\\bar{x}}\\] Sea \\(X\\sim Bernoulli(p)\\), la función de probabilidad es: \\[P(X=x)=\\pi(x)=p^x (1-p)^{1-x} \\quad ; x=\\{0,1\\}\\] Si \\(p\\) es el parámetro de interés que se busca estimar, ¿qué forma tendrá el estimador de máxima verosimilitud? Solución, Supongamos que se extrae una muestra de tamaño \\(n\\), así: \\[L(p)=f(X_1,p)*f(X_2,p)*\\ldots*f(X_n,p)=p^{x_1} (1-p)^{1-x_1}*p^{x_2} (1-p)^{1-x_2}*\\ldots*p^{x_n} (1-p)^{1-x_n}\\] \\[L(p)=p^{\\sum_{i=1}^n x_i}*(1-p)^{n-\\sum_{i=1}^n x_i}\\] \\[ln[ L(p)]= \\sum_{i=1}^n x_i ln(p)+\\left(n-\\sum_{i=1}^n x_i \\right) ln(1-p) \\] \\[\\frac{d}{dp}ln[ L(p)]= \\frac{\\sum_{i=1}^n x_i}{p}-\\frac{\\left(n-\\sum_{i=1}^n x_i\\right)}{1-p}=0\\] \\[ \\frac{\\sum_{i=1}^n x_i}{p}-\\frac{\\left(n-\\sum_{i=1}^n x_i\\right)}{1-p}=0\\] \\[\\hat{p}_{mv}=\\frac{\\sum_{i=1}^n x_i}{n}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim Poisson(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de máxima verosimilitud. Solución, recordar que si \\(X\\sim Poisson(\\lambda)\\) \\[\\pi(x)=P(X=x)=\\frac{e^{-\\lambda} \\lambda ^x}{x!}; \\quad X=\\{0,1,2\\ldots\\}\\] \\[L(\\lambda)=\\pi(X_1,\\lambda)*\\pi(X_2,\\lambda)*\\ldots*\\pi(X_n,\\lambda)\\] \\[L(\\lambda)=\\frac{e^{-\\lambda} \\lambda ^{x_1}}{x_1!}*\\frac{e^{-\\lambda} \\lambda ^{x_2}}{x_2!}*\\ldots*\\frac{e^{-\\lambda} \\lambda ^{x_n}}{x_n!}\\] \\[L(\\lambda)=\\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda ^{x_i}}{x_i!}=\\frac{e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n x_i}}{\\prod_{i=1}^n x_i!}\\] \\[ln [L(\\lambda)]=-n\\lambda+\\sum_{i=1}^n x_i ln \\lambda - ln \\prod_{i=1}^n x_i!\\] \\[\\frac{d}{d\\lambda}ln [L(\\lambda)]=-n+\\frac{\\sum_{i=1}^n x_i}{\\lambda}=0\\] \\[\\hat{\\lambda}=\\frac{\\sum_{i=1}^n x_i}{n}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim exp(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de máxima verosimilitud. Solución, recordar que si \\(X\\sim exp(\\lambda)\\) su función de densidad es dada por: \\[f(x)=\\lambda e^{-\\lambda x}; \\quad x\\geq0\\] \\[L(\\lambda)=\\prod_{i=1}^n \\lambda e^{-\\lambda x_i}=\\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}\\] \\[ln [L(\\lambda)]=n ln \\lambda-\\lambda \\sum_{i=1}^n x_i\\] \\[\\frac{d}{d\\lambda}ln [L(\\lambda)]=\\frac{n}{\\lambda}-\\sum_{i=1}^n x_i=0\\] \\[\\hat{\\lambda}=\\frac{1}{\\frac{\\sum_{i=1}^n x_i}{n}}=\\frac{1}{\\bar{X}}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim N(\\mu,\\sigma^2)\\) ambos parámetros desconocidos. Encontrar los estimadores de máxima verosimilitud para \\(\\mu\\) y \\(\\sigma^2\\). Solución, recordar si \\(X\\sim N(\\mu, \\sigma^2)\\) su función de densidad es dada por: \\[f(X)=\\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{1/2} }e^{-\\frac{1}{2}\\frac{\\left(x-\\mu\\right)^2}{\\sigma^2}}\\] \\[L(\\mu,\\sigma^2)=\\prod_{i=1}^n \\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{1/2} }e^{-\\frac{1}{2}\\frac{\\left(x_i-\\mu\\right)^2}{\\sigma^2}}=\\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{n/2} }e^{-\\frac{1}{2 \\sigma^2}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2}\\] \\[ln[L(\\mu,\\sigma^2)]=-\\frac{n}{2}ln (2\\pi \\sigma^2)-\\frac{1}{2 \\sigma^2}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2\\] \\[\\frac{\\partial}{\\partial \\mu} ln[L(\\mu,\\sigma^2)]=\\frac{1}{\\sigma^2} \\sum_{i=1}^n \\left(x_i-\\mu\\right)=0\\] \\[\\frac{\\partial}{\\partial \\sigma^2} ln[L(\\mu,\\sigma^2)]=-\\frac{n}{2 \\sigma^2}+\\frac{1}{2 \\sigma^4}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2=0\\] \\[\\sum_{i=1}^n x_i - n\\mu=0\\] \\[\\hat{\\mu}=\\frac{\\sum_{i=1}^n x_i}{n}=\\bar{X}\\] \\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^n \\left(x_i-\\bar{X}\\right)^2}{n}\\] 3.2.6 Método de momentos Este método fue desarrollado por 1894 por Pearson, a diferencia del método de máxima verosimilitud que fue ampliamente utilizado por Fisher a partir 1912. Recordar que para una variable aleatoria, los momentos respecto el origen son: Primer Momento: \\(\\mu_1=E[X]=\\int x f(x) dx\\) Segundo Momento: \\(\\mu_2=E[X^2]=\\int x^2 f(x) dx\\) k-ésimo momento: \\(\\mu_k=E[X^k]\\) Sea \\(X_1, X_2, \\ldots ,X_n\\) una muestra aleatorio de tamaño \\(n\\) de una va \\(X\\), definamos los primeros \\(k\\) momentos de la muestra respecto al origen como: Primer momento: \\[m_1=\\frac{\\sum_{i=1}^n x_i}{n}\\] Segundo momento: \\[m_2=\\frac{\\sum_{i=1}^n x^2_i}{n}\\] k-ésimo momento: \\[m_k=\\frac{\\sum_{i=1}^n x^k_i}{n}; \\quad k=1,2,\\ldots \\] Los momentos \\(\\mu_k\\) de la población serán funciones de los parámetros desconocidos \\(\\theta\\). Al igualar estos momentos muestrales con los poblaciones vamos a poder construir un sistema de ecuaciones de cuantas incógnitas se defina con la distribución de \\(X\\) Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim exp(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de momentos. Solución, recordar que si \\(X\\sim exp(\\lambda)\\) su función de densidad es dada por: \\[f(x)=\\lambda e^{-\\lambda x}; \\quad x\\geq0\\] \\[E[X]=m_1\\] \\[E[X]=\\int_0^\\infty x\\lambda e^{-\\lambda x}dx=\\frac{1}{\\lambda}\\] Igualando los momentos: \\[\\frac{1}{\\lambda}=\\frac{\\sum_s x_i}{n}\\] \\[\\hat{\\lambda}= \\frac{1}{\\frac{\\sum_s x_i}{n}}=\\frac{1}{\\bar{X}}\\] Ejercicio, Sea \\(X\\) una va geométrica con parámetro \\(p\\), encuentre un estimador de \\(p\\) mediante el método de momentos y el método de máxima verosimilitud. En base a una muestra aleatoria de tamaño \\(n\\) Recordar que si \\(X\\sim G(p)\\), entonces su distribución de probabilidades es: \\[\\pi(x)=P(X=x)=(1-p)^x p; \\quad x=\\{0,1,2,\\dots\\} \\] Recordar que \\(E[X]=\\frac{1-p}{p}\\), por el método de momentos: \\[\\frac{1-p}{p}=\\bar{X}\\] \\[\\hat{p}=\\frac{1}{\\bar{X}+1}\\] Por el método de máxima verosimilitud. \\[L(p)=\\prod_{i=1}^n (1-p)^{x_i} p=(1-p)^{\\sum_s x_i} p^n\\] \\[ln [L(p)]=\\sum_s x_i ln (1-p)+n ln(p)\\] \\[\\frac{d}{dp}ln [L(p)]=-\\frac{\\sum_s x_i}{1-p}+\\frac{n}{p}=0\\] \\[\\hat{p}=\\frac{1}{\\bar{X}+1}\\] Ejercicio, Sea \\(X\\) una normal con parámetro \\(\\mu\\), \\(\\sigma^2\\), encuentre un estimador de \\(\\mu\\) y \\(\\sigma^2\\) mediante el método de momentos. En base a una muestra aleatoria de tamaño \\(n\\) \\[E[X]=m_1\\rightarrow \\mu=\\bar{X}\\] \\[E[X^2]=m_2\\rightarrow E[X^2]=\\frac{\\sum_s x_i^2}{n}\\rightarrow (\\alpha)\\] Recordar que: \\[\\sigma^2=E[X^2]-E[X]^2=E[X^2]-\\mu^2 \\approx E[X^2]-\\bar{X}^2\\] Para \\((\\alpha)\\) \\[\\sigma^2+\\bar{X}^2=\\frac{\\sum_s x_i^2}{n}\\rightarrow \\hat{\\sigma}^2=\\frac{\\sum_s x_i^2}{n}-\\bar{X}^2\\] Tarea, sea \\(X\\) una va con función de densidad: \\[f(x)=\\frac{2(\\theta-x)}{\\theta^2}; \\quad 0\\leq x\\leq \\theta; \\quad \\theta&gt;0\\] Verificar que es una función de densidad Encontrar el estimador de \\(\\theta\\) por el método de momentos y máxima verosimilitud Verificar si el estimador por método de momentos y máxima verosimilitud es insesgado. Solución Verificar que es una función de densidad \\[1=\\int_{Rx} f(x)dx=\\int_{0}^\\theta \\frac{2(\\theta-x)}{\\theta^2} dx=\\frac{2}{\\theta^2}(-1) \\frac{(\\theta-x)^2}{2}/_0^\\theta\\] \\[\\frac{1}{\\theta^2}(-1)\\left(0^2-\\theta^2 \\right)=1 \\] Encontrar el estimador de \\(\\theta\\) por el método de momentos y máxima verosimilitud Suponer que tenemos una muestra aleatoria de tamaño \\(n\\), con las va \\(x_1,x_2,\\ldots,x_n\\) siente estas variables iid. Para el método de momentos se requiere resolver: \\[E[X]=m_1; \\quad E[X]=\\bar{X}\\] Se requiere calcular \\(E[X]\\) \\[E[X]=\\int_{Rx}x f(x)dx=\\int_0^\\theta x \\frac{2(\\theta-x)}{\\theta^2} dx=\\frac{\\theta}{3}\\] Entonces, \\[\\frac{\\theta}{3}=\\bar{X}\\quad \\rightarrow \\hat{\\theta}=3\\bar{X}\\] Para el método de máxima verosimilitud, sabemos que: \\[f(x_i,\\theta)=\\frac{2(\\theta-x_i)}{\\theta^2}\\] \\[L(\\theta)=\\prod_{i=1}^n \\frac{2(\\theta-x_i)}{\\theta^2}=\\frac{2^n}{\\theta^{2n}}\\prod_{i=1}^n\\left( \\theta - x_i\\right) \\] \\[ln(L(\\theta))=ln\\left[\\frac{2^n}{\\theta^{2n}}\\prod_{i=1}^n\\left( \\theta - x_i\\right)\\right]=n ln (2)-2n ln(\\theta)+ln (\\prod_{i=1}^n\\left( \\theta - x_i\\right))\\] \\[=n ln (2)-2n ln(\\theta)+\\sum_{i=1}^n ln (\\theta-x_i)\\] \\[\\frac{d ln(L(\\theta))}{d\\theta}=-\\frac{2n}{\\theta}+\\sum_{i=1}^n \\frac{1}{\\theta-x_i}=0\\] \\[\\sum_{i=1}^n \\frac{1}{\\theta-x_i}=\\frac{2n}{\\theta} \\rightarrow\\quad \\sum_{i=1}^n \\frac{\\theta}{\\theta-x_i}=\\sum_{i=1}^n 2\\rightarrow \\frac{\\theta}{\\theta-x_i}=2\\] \\[\\theta=2x_i \\rightarrow \\sum_s \\theta = 2 \\sum_s x_i\\] \\[\\hat{\\theta}=2\\bar{X}\\] Verificar si el estimador por método de momentos y máxima verosimilitud es insesgado. Para el método de momentos: \\[E[\\hat{\\theta}]=E[3\\bar{X}]=3E\\left[\\frac{\\sum_s x_i}{n}\\right]=\\frac{3}{n}\\sum_s E[x_i]=\\frac{3}{n}\\sum_s \\frac{\\theta}{3}=\\] \\[=\\frac{3n\\theta}{n3}=\\theta\\] Para el método de máxima verosimilitud \\[E[\\hat{\\theta}]=E[2\\bar{X}]=2E\\left[\\frac{\\sum_s x_i}{n}\\right]=\\frac{2}{n}\\sum_s E[x_i]=\\frac{2}{n}\\sum_s \\frac{\\theta}{3}=\\frac{2}{3}\\theta\\] 3.3 Estimación por intervalos de confianza Para construir un intervalo de confianza del parámetro desconocido \\(\\theta\\), se debe encontrar dos estadísticas \\(L\\) y \\(U\\) tales que: \\[P(L\\leq\\theta\\leq U)=1-\\alpha\\] El intervalo \\(L\\leq\\theta\\leq U\\) se llama intervalo de confianza del \\(100*(1-\\alpha)\\). A \\(L\\) se lo conoce como límite inferior y \\(U\\) como límite superior. La interpretación del intervalo de confianza es que si se coleccionan muchas muestras aleatorias y se calcula un intervalo de confianza del \\(100*(1-\\alpha)\\) por ciento en \\(\\theta\\) de cada muestra, entonces \\(100*(1-\\alpha)\\) por ciento de estos intervalos contendrán el verdadero valor de \\(\\theta\\). 3.3.1 Intervalo de confianza para la media, asumiendo varianza conocida. Sea \\(X\\) una va con media desconocida \\(\\mu\\) y varianza conocida \\(\\sigma^2\\). Suponga que se toma una muestra aleatoria de tamaño \\(n\\), \\(X_1,X_2,\\ldots,X_n\\). Puede obtenerse un intervalo de confianza del \\(100*(1-\\alpha)\\) por ciento en \\(\\mu\\) considerando la distribución de muestreo de \\(X\\) de la media muestral \\(\\bar{X}\\). Por el teorema del límite central sabemos que \\(\\bar{X}\\sim N(\\mu,\\frac{\\sigma^2}{n})\\) bajo ciertas condiciones. Así: \\[Z=\\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\] Teniendo a \\(Z\\sim N(0,1)\\), para armar el intervalo de confianza basta con trabajar sobre: \\[P(L\\leq Z\\leq U)=1-\\alpha\\] Para un intervalo de confianza \\(L \\leq \\theta \\le U\\) se debe asegurar que la precisión de los lados sea la misma, \\(\\theta-L=U-\\theta\\). curve(dnorm(x),xlim=c(-3.5,3.5),xlab=&quot;z&quot;) abline(v=0) sx&lt;-c(-3.5,seq(-3.5,qnorm(0.025),0.01),qnorm(0.025)) sy&lt;-c(0,dnorm(seq(-3.5,qnorm(0.025),0.01)),0) polygon(sx,sy,col=&quot;red&quot;) polygon(-1*sx,sy,col=&quot;red&quot;) qnorm(0.05/2) ## [1] -1.959964 Si \\(\\alpha=0.05\\) \\[P(L\\leq Z\\leq U)=0.95\\] \\[P(-Z_{\\alpha/2}\\leq Z\\leq Z_{\\alpha/2})=1-\\alpha\\] \\[P\\left(-Z_{\\alpha/2}\\leq \\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq Z_{\\alpha/2}\\right)=1-\\alpha\\] \\[P\\left(\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\leq\\mu \\leq \\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha\\] Así de esta manera tenemos identificados a \\(L\\) y \\(U\\) para \\(\\mu\\) con varianza conocida. \\[L=\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] \\[U=\\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] \\(\\alpha\\) es conocida como el nivel de significancia y \\(1-\\alpha\\) como la confiabilidad. Los valores más usuales de \\(\\alpha\\) son 0.01, 0.05 y 0.1, para estudios sobre ciencias de la salud el valor recomendado es de 0.01 o menor, para las ciencias sociales y económicas el valor recomendado es 0.05. Para la distribución normal los valores de \\(Z_{\\alpha/2}\\) son: \\(\\alpha=0.1\\), \\(Z_{\\alpha/2}=Z_{0.05}=1.64\\) (90% confiabilidad) \\(\\alpha=0.05\\), \\(Z_{\\alpha/2}=Z_{0.025}=1.96\\) (95% confiabilidad) \\(\\alpha=0.01\\), \\(Z_{\\alpha/2}=Z_{0.005}=2.58\\) (99% confiabilidad) Ejercicio, Se sabe que la vida en horas de una bombilla eléctrica de 75 watts se distribuye aproximadamente normal, con \\(\\sigma=25\\) horas. Una muestra aleatoria de 20 bombillas tiene una vida media de \\(\\bar{X}=1014\\) horas. Encontrarlos intervalos de confianza para 90, 95 y 99 % de confiabilidad Solución, como información \\(n=20\\), para elaborar los intervalos: \\[\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\leq\\mu \\leq \\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] Al 90% \\[1014-1.64 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+1.64 \\frac{25}{\\sqrt{20}}\\] \\[1004.832 \\leq \\mu \\leq 1023.168\\] Al 95% \\[1014-1.96 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+1.96 \\frac{25}{\\sqrt{20}}\\] \\[1003.043 \\leq \\mu \\leq 1024.957\\] Al 99% \\[1014-2.58 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+2.58 \\frac{25}{\\sqrt{20}}\\] \\[999.5774 \\leq \\mu \\leq 1028.423\\] 3.3.1.1 Tamaño de muestra Definamos al margen de error absoluto como: \\[\\epsilon=Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] Notar que es posible despejar \\(n\\) y esto permitirá tener una formula para definir un tamaño de muestra condicionado a: el margen de error (\\(\\epsilon\\)), desviación de los datos (\\(\\sigma\\)) y el nivel de confiabilidad (\\(Z_{\\alpha/2}\\)) \\[n=\\frac{Z_{\\alpha/2}^2*\\sigma^2}{\\epsilon^2}=\\left(\\frac{Z_{\\alpha/2}*\\sigma}{\\epsilon} \\right)^2\\] Nota: Esta formula se puede utilizar en la medida que la muestra que se seleccione sea aleatoria simple, se refiere a la selección de unidades simples. Ejemplo, se busca conocer el tiempo promedio en horas/día que pasan los estudiantes de informática de la UMSA en la computadora, para ello se planea realizar una encuesta aleatoria, que logre un 95% de confiabilidad y tenga un margen de error de 0.8 horas. Definir el tamaño de muestra necesario. Solución, como información se tiene: \\(\\epsilon=0.8\\), \\(Z_{\\alpha/2}=1.96\\), para el valor de \\(\\sigma\\) para el calculo del tamaño de muestra se realizó una piloto en la materia de estadística II a 13 estudiantes y el resultado fue \\(\\hat{\\sigma}=2.72\\) (varianza muestral). Así: \\[n=\\left(\\frac{1.96*2.72}{0.8} \\right)^2=44.41 \\approx 45\\] Ejercicio, se quiere obtener el tamaño de muestra para un estudio que busca conocer el promedio de gasto en educación mensual de los hogares de los estudiantes de informática. Se quiere un nivel de confiabilidad del 90%, asumiendo un margen de error de 300 Bs. Se sabe de un estudio similar que \\(\\hat{\\sigma}=650\\) Bs. Solución, \\[n=\\left(\\frac{1.64*650}{300} \\right)^2= 12.62\\approx 13 \\] 3.3.2 Intervalo de confianza sobre la diferencia de dos medias, conocida la varianza Tenemos dos va independientes, \\(X_1\\) con media \\(\\mu_1\\) desconocida y varianza \\(\\sigma^2_1\\) conocida y \\(X_2\\) con media \\(\\mu_2\\) desconocida y varianza \\(\\sigma^2_2\\) conocida. El objetivo es encontrar un intervalo para \\(\\mu_1-\\mu_2\\). Sea dos muestras aleatorias recopíladas para ambas va, de tal forma que \\(n_1\\) representa el tamaño de muestra para \\(X_1\\) y \\(n_2\\) para \\(X_2\\). Recordar por el teorema del limite central, esta diferencia de medias puede ser estimada por sus medias muestrales y además se aproxima a una normal, tal que: \\[\\bar{X}_1-\\bar{X}_2 \\sim N\\left(\\mu_{\\bar{X}_1-\\bar{X}_2}=\\mu_1-\\mu_2,\\sigma^2_{\\bar{X}_1-\\bar{X}_2}=\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}\\right)\\] Ahora, \\[Z=\\frac{\\bar{X}_1-\\bar{X}_2-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}}\\] Dado que \\(Z\\sim N(0,1)\\), ahora lo que queda es trabajar sobre: \\[P(-Z_{\\alpha/2} \\leq Z \\leq Z_{\\alpha/2})=1-\\alpha\\] Así, el limite inferior y superior esta dado por: \\[L=\\bar{X}_1-\\bar{X}_2-Z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\] \\[U=\\bar{X}_1-\\bar{X}_2+Z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\] Notar que en general, dado un estimador \\(\\hat{\\theta}\\) para el parámetro \\(\\theta\\), su usamos el teorema del limite central su intervalo de confianza estará dado por: \\[IC(\\theta): \\quad \\hat{\\theta} \\pm Z_{\\alpha/2} \\sqrt{V(\\hat{\\theta})} \\] Ejemplo: Se lleva a cabo pruebas de resistencia a la tensión sobre diferentes clases de largueros de aluminio utilizados en la fabricación de alas de aeroplanos comerciales. De la experiencia pasada con el proceso de fabricación de largueros y del procedimiento de prueba, se supone que las desviaciones estándar de las resistencias a la tensión son conocidas, Los datos obtenidos son: Clase de larguero 1: \\(n_1=18\\), \\(\\bar{X}_1=85.9\\), \\(\\sigma_1=1\\) Clase de larguero 2: \\(n_2=16\\), \\(\\bar{X}_2=73.3\\), \\(\\sigma_2=1.5\\) Si \\(\\mu_1\\) y \\(\\mu_2\\) son las verdaderas resistencias a la tensión de ambas clases de largueros. Encuentre intervalos de confianza al 90% y 95% de confiabilidad para la diferencia de estas medias. Solución, para el 90% de confiabilidad el intervalo esta dado por: \\[IC_{90\\%}(\\mu_1-\\mu_2): 85.9-73.3 \\pm 1.64*\\sqrt{\\frac{1^2}{18}+\\frac{1.5^2}{16}}=12.6 \\pm 0.73: [11.87\\quad 13.33]\\] \\[IC_{95\\%}(\\mu_1-\\mu_2): 85.9-73.3 \\pm 1.96*\\sqrt{\\frac{1^2}{18}+\\frac{1.5^2}{16}}=12.6 \\pm 0.87: [11.73\\quad 13.47]\\] 3.3.3 Intervalo de confianza para la media y la diferencia de medias con varianza desconocida pero muestra mayor a 30. Para los 2 anteriores intervalos definidos se suponía que la varianza es conocida, cuando no sucede esto la mejor alternativa para reemplazar a \\(\\sigma^2\\) es usando \\(\\hat{S}^2\\), esto siempre y cuando el tamaño de muestra sea grande (\\(n&gt;30\\)), para el caso de la diferencia de medias, ambos tamaños de muestra deben ser mayores a 30 \\((n_1,n_2&gt;30)\\). Esto debido al teorema del limite central. Para la media \\[IC_{100(1-\\alpha)}(\\mu): \\bar{X}\\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat{S}^2}{n}}\\] Para la diferencia de medias \\[IC_{100(1-\\alpha)}(\\mu_1-\\mu_2): \\bar{X}_1-\\bar{X}_2 \\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat{S_1}^2}{n_1}+\\frac{\\hat{S_2}^2}{n_2}}\\] Ejercicio, se tiene el dato de una muestra aleatoria de 35 estudiantes sobre la variable horas/día en la computadora, se pide calcular los intervalos de confianza al 90, 95 y 99% de confiabilidad, los datos son: n&lt;-35 set.seed(1517) x&lt;-round(rnorm(n,7.6,2.72),0) mean(x)+c(-1,1)*1.64*sqrt(var(x)/n)#90% ## [1] 6.277474 7.608240 mean(x)+c(-1,1)*1.96*sqrt(var(x)/n)#95% ## [1] 6.147643 7.738071 mean(x)+c(-1,1)*2.58*sqrt(var(x)/n)#99% ## [1] 5.896096 7.989618 Ejercicio, se tiene el dato de una muestra aleatoria de 40 personas, respecto su edad. Construya el intervalo de confianza al 95% de confiabilidad. Los datos son: set.seed(1534) x&lt;-round(runif(40,19,29)) x ## [1] 20 26 25 24 22 27 21 28 23 26 23 ## [12] 24 27 22 20 27 23 26 22 23 23 28 ## [23] 21 25 20 19 24 22 27 22 21 28 28 ## [34] 24 23 24 21 28 27 24 mean(x)+c(-1,1)*1.96*sqrt(var(x)/40) ## [1] 23.12872 24.77128 3.3.4 Intervalo de confianza para la media y la diferencia de medias con varianza desconocida pero muestra menor a 30. Para producir un intervalo de confianza valido en estos casos, se debe realizar supuestos fuertes respecto la población de la cual proviene la información. La suposición mas usual es que la población de base viene de una normal, lo que nos lleva en distribuciones muestrales a trabajar con un distribución \\(t-student\\). Para la media, sea: \\[t=\\frac{\\bar{X}-\\mu}{\\frac{\\hat{S}}{\\sqrt{n}}}\\] Ahora \\(t\\sim t-student(n-1)\\), al igual que para la normal el objetivo es encontrar: \\[P(L\\leq t \\leq U)=1-\\alpha\\] \\[P(-t_{\\alpha/2,n-1} \\leq t \\leq t_{\\alpha/2,n-1})=1-\\alpha\\] Lo que nos a: \\[IC_{100*(1-\\alpha)}(\\mu): \\bar{X} \\pm t_{\\alpha/2,n-1} \\sqrt{\\frac{\\hat{S}^2}{n}}\\] Nota: El valor de \\(t_{\\alpha/2,n-1}\\) en R. qnorm(0.05/2,lower.tail = F)#PARA LA NORMAL ## [1] 1.959964 #para la t-student alpha&lt;-c(0.1,0.05,0.01) n&lt;-20 qt(alpha/2,n-1,lower.tail = F) ## [1] 1.729133 2.093024 2.860935 Ejercicio, suponga que se tiene el dato de 15 personas sobre sus ingresos mensuales, estos datos proviene de una muestra aleatoria. Calcular el estimador de la media e intervalos de confianza al 95% y 98% de confiabilidad. n&lt;-15 set.seed(1423) x&lt;-round(runif(n,500,10000),0) x ## [1] 3415 1271 7587 5620 1277 2620 ## [7] 8805 1811 5152 4956 7823 9186 ## [13] 6785 3465 9672 Solución, bajo el supuesto que los ingresos tienen un comportamiento normal, se utilizara el intervalo de confianza con la t-student. mean(x)#estimador puntual ## [1] 5296.333 #intervalos de confianza mean(x)+c(-1,1)*qt(0.05/2,n-1,lower.tail = F)*sqrt(var(x)/n)#95% ## [1] 3680.188 6912.479 mean(x)+c(-1,1)*qt(0.02/2,n-1,lower.tail = F)*sqrt(var(x)/n)#98% ## [1] 3318.717 7273.950 Ejemplo, suponga que tenemos las mediciones de la estatura en cm de un grupo de 20 personas que fue seleccionado de forma aleatoria de una determinada población, los datos son: set.seed(1421) x&lt;-round(runif(20,140,180)) x ## [1] 171 173 180 152 148 166 177 179 ## [9] 173 167 175 145 152 142 143 155 ## [17] 179 176 173 147 Encontrar un intervalo de confianza al 95% de confiabilidad para la media poblacional. Solución, mx&lt;-mean(x) ta&lt;-qt(0.05/2,19,lower.tail = F) s2&lt;-var(x) n&lt;-20 mx+c(-1,1)*ta*sqrt(s2/n) ## [1] 157.1698 170.1302 Para la diferencia de medias, pero con varianzas iguales \\(\\sigma_1=\\sigma_2\\) Cuando \\(n_1\\) o \\(n_2\\) no superen ambas a 30, la mejor alternativa es usar el intervalo de confianza en base a la distribución \\(t\\), para la diferencia de medias el IC es: \\[IC_{100*(1-\\alpha)}(\\mu_1-\\mu_2)=\\bar{X}_1-\\bar{X}_2+t_{\\alpha/2,n_1+n_2-2} \\hat{S}_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\] Con \\(\\hat{S^2}_p\\) \\[\\hat{S}^2_p=\\frac{(n_1-1)\\hat{S}^2_1+(n_2-1)\\hat{S}^2_2}{n_1+n_2-2}\\] Para la diferencia de medias, pero con varianzas no iguales \\[IC_{100*(1-\\alpha)}(\\mu_1-\\mu_2)=\\bar{X}_1-\\bar{X}_2+t_{\\alpha/2,v} \\sqrt{\\frac{\\hat{S}_1^2}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}\\] \\[v=\\frac{\\left(\\frac{\\hat{S}_1^2}{n_1}+\\frac{\\hat{S}^2_2}{n_2} \\right)^2}{\\frac{(\\hat{S}_1^2/n_1)^2}{n_1+1}+\\frac{(\\hat{S}^2_2/n_2)^2}{n_2+1}}-2\\] Ejemplo, se tienen dos grupos donde se tomo de forma aleatoria a una muestra de ambos grupos, con el fin de estudiar la diferencia de medias para las notas de un examen, los resultados para ambos grupos fueron: set.seed(1421) x1&lt;-round(runif(10,1,100)) set.seed(1457) x2&lt;-round(runif(13,1,100)) x1 ## [1] 79 82 99 30 20 64 94 97 84 67 x2 ## [1] 73 62 57 23 59 16 65 39 81 23 74 ## [12] 22 20 Armar un intervalo de confianza al 95% de confiabilidad suponiendo que las varianzas son iguales y otro para varianzas distintas. mx1&lt;-mean(x1) mx2&lt;-mean(x2) s21&lt;-var(x1) s22&lt;-var(x2) n1&lt;-10 n2&lt;-13 # Estimador puntual de diferencia de medias mx1-mx2 ## [1] 24.36923 # Intervalo con varianzas iguales ta1&lt;-qt(0.025,n1+n2-2,lower.tail = F) s2p&lt;-((n1-1)*s21+(n2-1)*s22)/(n1+n2-2) mx1-mx2+c(-1,1)*ta1*sqrt(s2p)*sqrt(1/n1+1/n2) ## [1] 2.132725 46.605736 # Intervalo con varianzas desiguales v&lt;-((s21/n1+s22/n2)^2)/(((s21/n1)^2)/(n1+1)+((s22/n2)^2)/(n2+1))-2 ta2&lt;-qt(0.025,v,lower.tail = F) mx1-mx2+c(-1,1)*ta2*sqrt(s21/n1+s22/n2) ## [1] 1.660652 47.077810 #estimador puntual mx1-mx2 ## [1] 24.36923 3.3.5 Intervalos de confianza para la diferencia de medias de datos pareados En general, supongamos que los datos constan de \\(n\\) pares \\((X_{11}, X_{21}), (X_{12}, X_{22})...(X_{1n}, X_{2n})\\), usualmente esto sucede cuando se trabaja con observaciones en 2 puntos distintos de tiempo. Definamos las diferencias como \\(D_1=X_{11}-X_{21},D_2=X_{12}-X{22},\\ldots,D_n=X_{1n}-X_{2n}\\), estableciendo a \\(D\\) como la va, su intervalo es: \\[IC_{100*(1-\\alpha)}(\\mu_D=\\mu_1-\\mu_2): \\bar{D} \\pm t_{\\alpha/2,n-1}*\\sqrt{\\frac{\\hat{S}^2_D}{n}} \\] Si n es grande, entonces el intervalo es dado por: \\[IC_{100*(1-\\alpha)}(\\mu_D=\\mu_1-\\mu_2): \\bar{D} \\pm Z_{\\alpha/2}*\\sqrt{\\frac{\\hat{S}^2_D}{n}} \\] Ejemplo, se tiene las notas de los parciales de una muestra de 12 estudiantes, será que los estudiantes mejoraron su rendimiento del parcial 1 al parcial 2. Construya un intervalo de confianza al 90%. set.seed(1536) bd&lt;-data.frame(id=1:12,p1=round(runif(12,40,80)),p2=round(runif(12,30,90))) #estimador puntual mean(bd$p1) ## [1] 57.08333 mean(bd$p2) ## [1] 65.5 mean(bd$p2)-mean(bd$p1) ## [1] 8.416667 #creando la variable D=p2-p1 bd$D&lt;-bd$p2-bd$p1 mean(bd$D)+c(-1,1)*qt(0.1/2,11,lower.tail = F)*sqrt(var(bd$D)/12) ## [1] -2.418471 19.251805 ############################## #suponer una muestra de 100 estudiantes n&lt;-1000 set.seed(1536) bd2&lt;-data.frame(id=1:n,p1=round(runif(n,40,80)),p2=round(runif(n,30,90))) #estimador puntual mean(bd2$p1) ## [1] 60.41 mean(bd2$p2) ## [1] 59.786 mean(bd2$p2)-mean(bd2$p1) ## [1] -0.624 #creando la variable D=p2-p1 bd2$D&lt;-bd2$p2-bd2$p1 mean(bd2$D)+c(-1,1)*qnorm(0.1/2,lower.tail=F)*sqrt(var(bd2$D)/n) ## [1] -1.6782028 0.4302028 3.3.6 Intervalo de confianza para proporciones Si asumimos que el estimador del parámetro \\(P\\) (proporción), se distribuye normal, es decir, \\(\\hat{P}\\sim Normal\\), bajo ciertas condiciones como que el tamaño de muestra es grande, se puede plantear al intervalo de confianza como: \\[IC_{100*(1-\\alpha)}(P)=\\hat{P} \\pm Z_{\\alpha/2} * \\sqrt{V(\\hat{P})}\\] Donde para encontrar la \\(V(\\hat{P})\\) basta con resolver la varianza del estimador de la media para valores binarios. \\[V(\\bar{X})=\\frac{\\sigma^2}{n} =V(\\hat{P})=\\frac{\\sigma^2_p}{n}=\\frac{1}{n}\\left(\\frac{\\sum_{i=1}^N x_i^2 }{N}-\\mu^2 \\right)=\\frac{1}{n}\\left(\\frac{\\sum_{i=1}^N x_i }{N}-P^2 \\right)=\\] \\[=\\frac{1}{n}(P-P^2)=\\frac{P(1-P)}{n}\\] Notar que esta varianza esta en términos del parámetro \\(P\\), por lo que recurrimos en el caso muestral a reemplazarlo por su estimador \\(\\hat{P}\\). Así, el intervalo de confianza queda como: \\[IC_{100*(1-\\alpha)}(P)=\\hat{P} \\pm Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P}(1-\\hat{P})}{n}}\\] Ejemplo, en un curso se tomó una muestra aleatoria de 15 personas, respecto su estatura en centímetros, las mediciones son: set.seed(1431) x&lt;-round(rnorm(15,165,10),0) x ## [1] 163 167 156 156 178 172 152 160 ## [9] 140 160 168 175 170 160 164 Se pide encontrar al estimador de la proporción y su intervalo de confianza al 95% de confiabilidad para la proporción de estudiantes con estatura de 170 o más. Solución, n&lt;-15 x&lt;-(x&gt;=170)*1 x ## [1] 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 #el estimador de la proporción p&lt;-mean(x) p ## [1] 0.2666667 #en porcentaje p*100 ## [1] 26.66667 #intervalo p+c(-1,1)*1.96*sqrt(p*(1-p)/n) ## [1] 0.04287417 0.49045916 #intervalo en % (p+c(-1,1)*1.96*sqrt(p*(1-p)/n))*100 ## [1] 4.287417 49.045916 Ejercicio, la varianza del estimador de la proporción tiene la forma: \\[V(\\hat{P})=\\frac{P(1-P)}{n}\\] Si suponemos un tamaño de muestra \\(n\\) conocido, en que valor de \\(P\\) alcanza la varianza su máximo. Solución \\[\\frac{d V(\\hat{P})}{dP}=\\frac{d}{dP}\\left(\\frac{P}{n}-\\frac{P^2}{n}\\right)=\\frac{1}{n}-\\frac{2P}{n}=0 \\rightarrow P=0.5\\] También, \\[\\frac{d^2 V(\\hat{P})}{d^2P}=-\\frac{2}{n}&lt;0 \\quad (máximo)\\] vp&lt;-function(x){x*(1-x)} curve(vp,xlab=&quot;p&quot;,ylab=&quot;v(p)&quot;,main=&quot;n=1&quot;) 3.3.6.1 Tamaño de muestra para estimar proporciones Para la proporción definimos el margen de error (\\(\\epsilon\\)) como: \\[\\epsilon= Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P}(1-\\hat{P})}{n}}\\] Ahora, podemos usar esta definición como una salida para el calculo del tamaño de muestra necesario para cometer un determinado margen de error (\\(epsilon\\)), sujeto a un nivel de confiabilidad de \\(Z_{\\alpha/2}\\), basta con despejar \\(n\\) de la anterior formula. \\[n=\\left(\\frac{Z_{\\alpha/2}}{\\epsilon}\\right)^2 \\hat{P}(1-\\hat{P})\\] Dado que definir el tamaño de muestra es un paso previo a la recolección de información, notar que se tiene total control sobre el margen de error (\\(\\epsilon\\)) y el nivel de confiabilidad (\\(Z_{\\alpha/2}\\)), sin embargo, en la formula aparece el estimador \\(\\hat{P}\\) que es el de interés, la solución para saltarnos este dilema, es elegir un \\(\\hat{P}\\) basado en un estudio similar o una prueba piloto, la otra alternativa extrema es suponer un \\(\\hat{P}\\) que haga máximo a \\(n\\) con lo demás fijo. tm_p&lt;-function(z,e,p){ n&lt;-((z/e)^2)*(p*(1-p)) return(n) } ceiling(tm_p(1.96,0.05,0.5)) ## [1] 385 Ejercicio, una carrera en la universidad esta a punto de elegir a sus autoridades, se busca hacer una encuesta de intención de votos en los estudiantes para el candidato \\(Z\\), se quiere un nivel de confianza del 95%, y no errar en +- 5%. Calcular el tamaño de muestra, (1) suponiendo \\(n\\) máxima y (2) mediante un sondeo se verifico que el candidato \\(Z\\) tiene un 70% de apoyo. Solución, ceiling(tm_p(1.96,0.05,0.5))#1 ## [1] 385 ceiling(tm_p(1.96,0.05,0.7))#2 ## [1] 323 ceiling(tm_p(1.96,0.02,0.5))#epsilon 2% ## [1] 2401 ceiling(tm_p(1.96,0.01,0.5))#epsilon 1% ## [1] 9604 3.3.7 Intervalo de confianza para diferencia de proporciones Esta diferencia de proporciones son ampliamente usadas cuando se comparan dos poblaciones, respecto una característica de interés sobre dos poblaciones independientes. Así el intervalo de la diferencia de proporciones esta dado por: \\[IC_{100*(1-\\alpha)}(P_1-P_2)=(\\hat{P}_1-\\hat{P}_2) \\pm Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P_1}(1-\\hat{P}_1)}{n_1}+\\frac{\\hat{P}_2(1-\\hat{P}_2)}{n_2}}\\] Donde \\(\\hat{P}_1\\) y \\(\\hat{P}_2\\) son estimaciones de proporción para la población 1 y 2, respecto una misma característica, \\(n_1\\) y \\(n_2\\) son los tamaños de muestra es estas poblaciones. Ejercicio, en una muestra aleatoria de 20 estudiantes se midió la estatura del grupo, se tiene conocimiento del sexo de los estudiantes y se busca estimar la diferencia de proporciones por hombre y mujer de la proporción de estudiantes que superan los 170 cm de estatura. Los datos son: n&lt;-20 set.seed(1501) estatura&lt;-round(rnorm(n,165,10),0) set.seed(1501) mujer&lt;-rbinom(n,1,0.4) bd&lt;-data.frame(estatura,mujer) bd ## estatura mujer ## 1 161 0 ## 2 179 0 ## 3 171 1 ## 4 152 0 ## 5 141 1 ## 6 168 0 ## 7 168 0 ## 8 173 1 ## 9 155 0 ## 10 169 0 ## 11 160 1 ## 12 177 1 ## 13 164 1 ## 14 165 1 ## 15 159 1 ## 16 172 1 ## 17 158 0 ## 18 169 0 ## 19 172 1 ## 20 166 0 Se pide calcular el estimador puntual de la diferencia de proporciones y el intervalo de confianza al 90% de confiabilidad. Solución, algunos parámetros del ejercicio \\(n_1=10\\) , \\(n_2=10\\), las proporciones: \\[\\hat{P}_{1,h}=\\frac{\\#a}{n_1}=\\frac{1}{10}=0.1\\] \\[\\hat{P}_{2,m}=\\frac{\\#a}{n_2}=\\frac{5}{10}=0.5\\] El estimador puntual es dado por: \\[\\hat{P}_1-\\hat{P}_2=0.1-0.5=-0.4\\] Ahora, el intervalo de confianza \\[IC_{100*(1-\\alpha)}(P_1-P_2)=-0.4 \\pm 1.64 * \\sqrt{\\frac{0.1*0.9}{10}+\\frac{0.5*0.5}{10}}=-0.4 \\pm 0.3024\\] \\[IC_{100*(1-\\alpha)}(P_1-P_2)=[-0.702 \\quad -0.098]\\] 3.3.8 Intervalo de confianza para la varianza Suponga que \\(X\\sim Normal(\\mu,\\sigma)\\) ambos parámetros desconocidos. Sea \\(X_1,X_2,\\ldots,X_n\\) una muestra aleatoria de tamaño \\(n\\) de \\(X\\). Recodar que la varianza muestra sigue una distribución de muestreo tipo \\(\\chi^2\\). \\[\\chi^2=\\frac{(n-1)\\hat{S}^2}{\\sigma^2}\\] Para desarrollar el intervalo usamos esta distribución: \\[P(\\chi^2_{1-\\alpha/2,n-1} \\leq \\chi^2 \\leq \\chi^2_{\\alpha/2,n-1})=1-\\alpha\\] \\[IC_{100*(1-\\alpha)}(\\sigma^2): \\left[\\frac{(n-1)\\hat{S}^2}{\\chi^2_{\\alpha/2,n-1}} \\quad \\frac{(n-1)\\hat{S}^2}{\\chi^2_{1-\\alpha/2,n-1}}\\right]\\] 3.3.9 Intervalo de confianza para el cociente de varianzas El objetivo de esta medida es tener un intervalo para el cociente de las varianzas de dos poblaciones, esto puede servir para identificar que población (con que variable) tiene mayor variabilidad. El parámetro es: \\[\\theta=\\frac{\\sigma^2_1}{\\sigma^2_2}\\] Supongamos que \\(X_1\\) y \\(X_2\\) son va normales con media \\(\\mu_1\\) y \\(\\mu_2\\) desconocidas y varianzas \\(\\sigma^2_1\\), \\(\\sigma^2_2\\) también desconocidas. Sean dos muestras aleatorias de \\(X_1\\) y \\(X_2\\) de tamaño \\(n_1\\) y \\(n_2\\) y sean \\(\\hat{S}^2_1\\) y \\(\\hat{S}^2_2\\) las varianzas de las muestras. Para armar el intervalo de confianza recurrimos a la distribución \\(F\\). \\[F=\\frac{\\frac{\\hat{S}^2_2}{\\sigma^2_2}}{\\frac{\\hat{S}^2_1}{\\sigma^2_1}}\\] Se busca: \\[P(F_{1-\\alpha/2,n_2-1,n_1-1} \\leq F \\leq F_{\\alpha/2,n_2-1,n_1-1})=1-\\alpha\\] \\[IC_{100*(1-\\alpha)}\\left(\\frac{\\sigma^2_1}{\\sigma^2_2}\\right): \\left[\\frac{\\hat{S}^2_1}{\\hat{S}^2_2} F_{1-\\alpha/2,n_2-1,n_1-1} \\quad \\frac{\\hat{S}^2_1}{\\hat{S}^2_2} F_{\\alpha/2,n_2-1,n_1-1}\\right]\\] 3.4 Ejercicios Un ingeniero de control de calidad midió el espesor de la pared de 25 botellas de vidrio de dos litros. La media de la muestra fue 4.05 mm y la desviación estándar de la muestra \\(\\hat{S}=.08\\) mm. Determine un intervalo de confianza: al 90 % de confiabilidad (dos lados) inferior del 90 por ciento respecto al espesor de pared medio. Solución, \\[IC(\\bar{x})_{90\\%}=\\bar{x} \\pm t_{\\alpha/2,n-1} \\sqrt{\\frac{\\hat{S}^2}{n}}=4.05\\pm 1.71 \\sqrt{\\frac{0.08^2}{25}}\\] ta&lt;-qt(0.1/2,24,lower.tail = F) 4.05+c(-1,1)*ta*sqrt(0.08^2/25) ## [1] 4.022626 4.077374 \\[IC(\\bar{x})_{90\\%}=[4.02 \\quad 4.08]\\] Para el intervalo de confianza inferior. Recordar que para el intervalo de confianza de 2 lados, la formula es: \\[P(L&lt;\\theta&lt;U)=1-\\alpha\\] Mientras para un intervalo inferior, la formula es: \\[P(L&lt;\\theta)=1-\\alpha\\] \\[IC(\\bar{x})_{90\\%, inferior}=\\bar{x} - t_{\\alpha,n-1} \\sqrt{\\frac{\\hat{S}^2}{n}}=4.05- 1.32 \\sqrt{\\frac{0.08^2}{25}}\\] qt(0.1,24,lower.tail = F) ## [1] 1.317836 \\[L: 4.03mm\\] Un ingeniero industrial está interesado en estimar el tiempo medio requerido para ensamblar una tarjeta de circuito impreso. ¿Qué tan grande debe ser la muestra si el ingeniero desea tener una confianza del 95 por ciento de que el error en la estimación de la media es menor que .25 minutos? La desviación estándar del tiempo de ensamble es .45 minutos. Solución, sean los datos \\(Z_{\\alpha/2}=1.96\\), \\(\\sigma=0.45\\), \\(\\epsilon=0.25\\) \\[n=\\left(\\frac{Z_{\\alpha/2}*\\sigma}{\\epsilon} \\right)^2=\\left(\\frac{1.96*0.45}{0.25}\\right)^2=12.45\\approx13\\] 3. Dada la siguiente muestra aleatoria por hombre y mujer, respecto sus ingresos: Hombre: 2078, 3565, 3767, 3600, 2907, 1299, 3483, 8475, 2425, 3500, 3118, 3377, 3897, 21883, 9717, 2078, 8167 Mujeres: 1905, 9500, 996, 3250, 3335, 433, 2917, 616, 1625, 1104, 2050, 3000, 1625 se pide calcular intervalos de confianza para la varianza para ambas poblaciones, y para el cociente de varianzas entre \\(Hombre/Mujer\\) y \\(Mujer/Hombre\\). Estos intervalos al 95 % de confiabilidad. (Asumir normalidad de los datos de origen). Solución: hh&lt;-c(2078, 3565, 3767, 3600, 2907, 1299, 3483, 8475, 2425, 3500, 3118, 3377, 3897, 21883, 9717, 2078, 8167) mm&lt;-c(1905, 9500, 996, 3250, 3335, 433, 2917, 616, 1625, 1104, 2050, 3000, 1625) nh&lt;-17;nm&lt;-13 #estimador de varianzas var(hh)#puntual hombres S^2 ## [1] 24271163 var(mm)#puntual mujeres S^2 ## [1] 5400214 #intervalo hombres lih&lt;-(var(hh)*(nh-1))/qchisq(0.05/2,nh-1,lower.tail = F) lsh&lt;-(var(hh)*(nh-1))/qchisq(1-0.05/2,nh-1,lower.tail = F) lih;var(hh);lsh ## [1] 13462780 ## [1] 24271163 ## [1] 56218511 #intervalo mujeres limj&lt;-(var(mm)*(nm-1))/qchisq(0.05/2,nm-1,lower.tail = F) lsmj&lt;-(var(mm)*(nm-1))/qchisq(1-0.05/2,nm-1,lower.tail = F) limj;var(mm);lsmj ## [1] 2776857 ## [1] 5400214 ## [1] 14715187 ########################### #cociente de varianza H/M var(hh)/var(mm)#puntual ## [1] 4.494481 #intervalos lihm&lt;-(var(hh)/var(mm))*qf(1-0.05/2,nm-1,nh-1,lower.tail = F) lshm&lt;-(var(hh)/var(mm))*qf(0.05/2,nm-1,nh-1,lower.tail = F) lihm; var(hh)/var(mm); lshm ## [1] 1.426128 ## [1] 4.494481 ## [1] 12.98477 #99 confiabilidad lihm&lt;-(var(hh)/var(mm))*qf(1-0.01/2,nm-1,nh-1,lower.tail = F) lshm&lt;-(var(hh)/var(mm))*qf(0.01/2,nm-1,nh-1,lower.tail = F) lihm; var(hh)/var(mm); lshm ## [1] 0.9615647 ## [1] 4.494481 ## [1] 18.42446 ########################### #cociente de varianza M/H var(mm)/var(hh)#puntual ## [1] 0.2224951 #intervalos limh&lt;-(var(mm)/var(hh))*qf(1-0.05/2,nh-1,nm-1,lower.tail = F) lsmh&lt;-(var(mm)/var(hh))*qf(0.05/2,nh-1,nm-1,lower.tail = F) limh; var(mm)/var(hh); lsmh ## [1] 0.0770133 ## [1] 0.2224951 ## [1] 0.7011992 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
