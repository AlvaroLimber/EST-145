[["index.html", "Estadística II EST-145 Prefacio Audiencia Estructura del libro Software y acuerdos Bases de datos Agradecimiento", " Estadística II EST-145 Alvaro Chirino Gutierrez 2021-06-02 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Estadística II, una materia del pregrado de la carrera de Informática de la Universidad Mayor de San Ándres. Estructura del libro El libro incluye 6 capítulos, estos son: Distribuciones de probabilidad bivariada Distribuciones muestrales Distribuciones Chi cuadrado, t-student y Fisher Estimación puntual de parámetros Estimación de parámetros por intervalos de confianza Pruebas de hipótesis Software y acuerdos sessionInfo() ## R version 4.0.5 (2021-03-31) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19043) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Spanish_Bolivia.1252 ## [2] LC_CTYPE=Spanish_Bolivia.1252 ## [3] LC_MONETARY=Spanish_Bolivia.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=Spanish_Bolivia.1252 ## ## attached base packages: ## [1] stats graphics grDevices ## [4] utils datasets methods ## [7] base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.21 digest_0.6.27 ## [3] R6_2.5.0 jsonlite_1.7.2 ## [5] magrittr_2.0.1 evaluate_0.14 ## [7] highr_0.8 stringi_1.5.3 ## [9] rlang_0.4.10 rstudioapi_0.13 ## [11] jquerylib_0.1.3 bslib_0.2.4 ## [13] rmarkdown_2.7 tools_4.0.5 ## [15] stringr_1.4.0 xfun_0.22 ## [17] yaml_2.2.1 compiler_4.0.5 ## [19] htmltools_0.5.1.1 knitr_1.31 ## [21] sass_0.3.1 Bases de datos En este documento se emplearan 2 bases de datos del contexto Boliviano: Encuesta a Hogares 2019 y 2019. Vivienda y Personas Computo oficial de las elecciones del 18 de Octubre de 2020 Estas bases de datos se encuentran disponibles en formato \\(.RData\\) en el repositorio de Github del texto. Agradecimiento "],["tema-1-distribuciones-bivariadas-multivariadas.html", "1 Tema 1: Distribuciones bivariadas (multivariadas) 1.1 Variables aleatorias bivariantes 1.2 Función de distribución bivariada 1.3 Función masa de probabilidad (función de densidad) 1.4 Distribución marginal 1.5 Independencia 1.6 Valores esperados 1.7 Distribuciones condicionales 1.8 Medidas de relación entre dos variables", " 1 Tema 1: Distribuciones bivariadas (multivariadas) En el caso univariado se tenia a una va \\(X\\) definida en los reales \\(IR\\), a esta va se le asignaba una función de distribución \\(F(x)\\) y una función de densidad \\(f(x)\\). Ambas distribuciones tienen su correspondencia en lo discreto y lo continuo: Caso discreto: \\[\\sum_{Rx} P(X=x)=1\\] \\[F(t)=P(X\\leq t)=\\sum_{x\\leq t} P(X=x)\\] Caso continuo: \\[\\int_{Rx}f(x)dx=1\\] \\[F(t)=P(X\\leq t)=\\int_{-\\infty}^{t}f(x)dx\\] La idea de este capitulo es ver las propiedades en el caso bivariante y generalizar para el caso multivariante. 1.1 Variables aleatorias bivariantes Son un par de variables aleatorias con una distribución conjunta, son típicamente representadas con mayúscula \\((X, Y)\\) o \\((X_1,X_2)\\), las realizaciones de estas variables aleatorias se representan como \\((x,y)\\) o \\((x_1,x_2)\\). Definición 1. Un par de variables aleatorias bivariadas es un par numérico de resultados; una función definida en \\(IR^2\\) Ejemplos: Considerar el par (edad, estatura): \\((23,170)\\) \\((20,172)\\) \\((20,154)\\) \\((26,159)\\) \\((19,175)\\) Considerar el par: (ingreso, años experiencia) Imaginar lanzar 2 monedas simultáneamente, \\(\\Omega=\\{CC,CS,SC,SS\\}\\), si definimos a \\(Cara=1\\) y \\(Sello=0\\), \\(R_{(X,Y)}=\\{(1,1),(1,0),(0,1),(0,0)\\}\\). 1.2 Función de distribución bivariada Definición 2. La función de distribución conjunta de \\((X,Y)\\) es \\[F(x,y)=P(X\\leq x,Y\\leq y)=P\\left[\\{X\\leq x\\} \\cap \\{Y\\leq y\\} \\right]\\] Las propiedades de \\(F\\) son similares al caso univariante, \\(0\\leq F(x,y)\\leq 1\\). Ejemplo: \\[F(x,y)=(1-e^{-x})(1-e^{-y});\\hspace{2cm} x,y\\geq0 \\] * \\(F(0,0)=0\\) * \\(F(\\infty,\\infty)=1\\) Calcular: \\[P(X\\leq 10,Y\\leq 100)=F(10,100)=(1-e^{-10})(1-e^{-100})=0.9999546\\] \\[P(X\\leq 5,Y\\leq 20)=F(5,20)=0.99326\\] La distribución conjunta satisface: \\[P(a&lt;X\\leq b,c&lt; Y\\leq d)=F(b,d)-F(b,c)-F(a,d)+F(a,c)\\] Para \\(a&lt;b\\) y \\(c&lt;d\\) 1.3 Función masa de probabilidad (función de densidad) Para el par \\((X,Y)\\) con una función de distribución conjunta \\(F(x,y)\\). Para el caso continuo, Definición 3. \\[f(x,y)=\\frac{\\partial^2}{\\partial x \\partial y}F(x,y)\\] Por un tema de notación, a veces escribiremos \\(f_{X,Y}(x,y)\\) Ejercicio: encontrar \\(f(x,y)\\) para la \\(F\\) dada en el ejemplo anterior \\[f(x,y)=\\frac{\\partial^2}{\\partial x \\partial y}\\left[(1-e^{-x})(1-e^{-y})\\right]=e^{-x}e^{-y}\\] \\(f\\) satisface de forma similar las propiedades vistas en el caso univariante. \\[\\int_{Rx}\\int_{Ry}f(x,y)dydx=1\\] Para el ejercicio: \\[\\int_0^{\\infty} \\int_0^{\\infty}e^{-x}e^{-y} dx dy=\\int_0^{\\infty} e^{-y} \\left[-e^{-x}/_0^{\\infty} \\right] dy =1\\] \\[P(a&lt;X\\leq b,c&lt; Y\\leq d)=\\int_a^b \\int_c^d f(x,y)dxdy\\] Para el caso discreto podemos definirlo de la siguiente forma: \\[f(x,y)=\\pi(x,y)=P(X=x,Y=y)\\] \\[\\sum_{Rx}\\sum_{Ry}\\pi(x,y)=1\\] Ejemplo, en un restaurante de pizza se vende porciones de pizza y gaseosa, el dueño del local realizó un monitoreo del patrón de como los clientes ordenan sus pedidos, respecto la cantidad de porciones de pizza con la cantidad de gaseosas, encontrando el siguiente resultado: Cuál sera la probabilidad de: \\(P(X=3,Y=2)=0.13\\) \\(P(X\\geq 2, Y=1)=P(X=2, Y=1)+P(X=3,Y=1)=\\pi_{21}+\\pi_{31}=0.17+0.05=0.22\\) 1.4 Distribución marginal La distribución conjunta del vector aleatorio \\((X,Y)\\) describe la distribución del vector aleatorio, sin embargo, es posible a partir de la distribución conjunta, generar las distribuciones para cada componente del vector aleatorio. Definición 4, la distribución marginal de X es: \\[F_X(x)=P(X\\leq x)=P(X\\leq x, Y\\leq \\infty)=lim_{y\\rightarrow \\infty} F(x,y)\\] De manera más usual se tiene: Para el caso discreto: \\[P(X=x)=\\pi(x)=\\sum_{Ry} \\pi(x,y)\\] \\[P(Y=y)=\\pi(y)=\\sum_{Rx} \\pi(x,y)\\] Para el caso continuo: \\[f(x)=\\int_{Ry} f(x,y)dy\\] \\[f(y)=\\int_{Rx} f(x,y)dx\\] Ejercicio, para la función: \\[f(x,y)=e^{-x}e^{-y}\\] Con \\(x,y\\geq 0\\), encontrar las marginales de \\(f(x)\\) y \\(f(y)\\). Solución: \\[f(x)=\\int_0^{\\infty}e^{-x}e^{-y} dy=e^{-x}\\] \\[f(y)=\\int_0^{\\infty}e^{-x}e^{-y} dx=e^{-y}\\] Nota: las marginales deben estar en función de su propia variable aleatoria y no contener otras variables, dado que son marginales. Notar que en este ejercicio: \\[f(x,y)=e^{-x}e^{-y}=f(x)*f(y)\\] Esto no siempre sucede, este caso se da cuando las variables son independientes. Ejemplo para el caso discreto, 1.5 Independencia Definición 6, dos variables aleatorias son independientes si: \\[f(x,y)=f(x)*f(y)\\] \\[\\pi(x,y)=\\pi(x)*\\pi(y)\\] Ejercicio, verificar si la siguiente función esta bien definida y si las variables son independientes. \\[f(x,y)=\\frac{1}{4}(x+y)*xy*e^{-x-y}; \\hspace{2cm} x,y&gt;0\\] Solución, Si esta bien definida, esto significa: \\[\\int_0^{\\infty}\\int_0^{\\infty}\\frac{1}{4}(x+y)*xy*e^{-x-y}=1\\] \\[f(x)=\\int_{Ry}f(x,y)dy=\\frac{x^2+2x}{4} ( e^{-x})\\] Tarea, encontrar \\(f(y)\\) y evaluar si son o no independientes 1.6 Valores esperados En el caso univariado, sea \\(X\\) una variable aleatoria con función de probabilidad \\(\\pi(x)\\) para el caso discreto o \\(f(x)\\) para el caso continuo, el operador matemático esperanza se define como: Para el caso discreto, \\[E[g(X)]=\\sum_{Rx}g(x)P(X=x)\\] Para el caso continuo, \\[E[g(X)]=\\int_{Rx}g(x)f(x)dx\\] Definición 6, El valor esperado para la función \\(g(X,Y)\\), se define como: Para el caso discreto: \\[E[g(X,Y)]=\\sum_{Rx}\\sum_{Ry}g(x,y)\\pi(x,y)\\] Para el caso continuo: \\[E[g(X,Y)]=\\int_{Rx}\\int_{Ry}g(x,y)f(x.y)dydx\\] Nota, hay valores esperados más usuales que otros, Por ejemplo, las varianzas para cada variable \\[E[(X-E[X])^2]=V(X)\\] \\[E[(Y-E[Y])^2]=V(Y)\\] Otras medidas son \\(E[X]\\), \\(E[Y]\\) que son referencias muy similares a un promedio aritmético. Otra valor esperado bastante usado en los casos bivariados es: \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx\\] Encontrar la forma de \\(E[X]\\) usando la definición anterior. \\[E[X]=\\int_{Rx}\\int_{Ry} x f(x,y) dy dx=\\int_{Rx}xf(x) dx\\] \\[E[X]=\\sum_{Rx}\\sum_{Ry} x \\pi(x,y) =\\sum_{Rx}x \\pi(x) dx\\] Ejemplo para el caso discreto, de las pizza y las gaseosas. \\[E[X]=1*0.5+2*0.31+3*0.19=1.69\\] \\[E[Y]=0*0.12+1*0.63+2*0.25=1.13\\] \\[E[X^2]=\\sum_{x=1}^{x=3}x^2 \\pi(x)=1^2*0.5+2^2*0.31+3^2*0.19=3.45\\] \\[E[Y^2]=0^2*0.12+1^2*0.63+2^2*0.25=1.63\\] \\[E[XY]=\\sum_{Rx}\\sum_{Ry}xy \\pi(x,y)=1*0*0.04+1*1*0.42+\\ldots+3*2*0.13=2.067\\] 1.7 Distribuciones condicionales Estas distribuciones nos ayudan a entender el comportamiento de una variable, cuando fijamos a otra. Definición, una distribución condicional se define como: Caso discreto, \\(\\pi_{x/y}(X/Y=y)=\\frac{\\pi(x,y)}{\\pi(y)}\\) Caso continuo, \\[f_{X/Y}(x/y)=\\frac{f(x,y)}{f(y)}\\] \\[f_{Y/X}(y/x)=\\frac{f(x,y)}{f(x)}\\] Estas funciones condicionales cumplen todas las propiedades de una función de probabilidad. Demostrar que: \\[\\int_{Rx} f_{X/Y}(x/y) dx=1\\] \\[\\int_{Rx} f_{X/Y}(x/y) dx=\\int_{Rx} \\frac{f(x,y)}{f(y)} dx=\\frac{1}{f(y)}\\int_{Rx} f(x,y)dx=\\frac{f(y)}{f(y)}=1\\] Que sucede si \\(X\\) e \\(Y\\) son variables independientes: \\[f_{X/Y}(x/y)=\\frac{f(x,y)}{f(y)}=\\frac{f(x)f(y)}{f(y)}=f(x)\\] \\[f_{Y/X}(y/x)=\\frac{f(x,y)}{f(x)}=\\frac{f(x)f(y)}{f(x)}=f(y)\\] 1.8 Medidas de relación entre dos variables Estas medidas nos ayudan a conocer si dos variables están relacionadas y nos permite saber el tipo de relación (directa, inversa) y también podemos saber la intensidad de la relación. Las medidas son: La Covarianza \\(cov(X,Y)\\) es una medida absoluta de relación: \\[cov(X,Y)=E[(X-E[X])(Y-E[Y])]\\] Una alternativa a esta formula (versión corta). \\[cov(X,Y)=E[XY]-E[X]*E[Y]\\] Tarea, demostrar lo anterior. Otra medida importantes es la correlación entre \\(X\\) e \\(Y\\), esta es una medida relativa, que cumple la propiedad: \\(-1 \\leq corr(X,Y) \\leq 1\\), esta se define como: \\[corr(X,Y)=\\frac{cov(X,Y)}{\\sqrt{V(X)V(Y)}}=\\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}\\] * Si \\(cov_{xy}\\) o \\(corr_{xy}\\) son distintas de 0, podemos afirmar que existe relación * Si \\(cov_{xy}&gt;0\\) o \\(corr_{xy}&gt;0\\) la relación entre \\(X\\) e \\(Y\\) es directa * Si \\(cov_{xy}&lt;0\\) o \\(corr_{xy}&lt;0\\) la relación entre \\(X\\) e \\(Y\\) es inversa * La intensidad de la dirección de la relación nos la da \\(corr_{xy}\\), mientras más cercana a \\(corr_{xy}\\rightarrow 1\\) la relación directa es más fuerte, \\(corr_{xy}\\rightarrow -1\\) la relación inversa es más fuerte * Si \\(corr_{xy}\\rightarrow 0\\) podemos decir que las variables no están relacionadas (cuasi-independencia) la correlación mide principalmente relaciones lineales. Ejercicio, Demostrar que si \\(X\\) y \\(Y\\) son independientes entonces: \\[E[XY]=E[X]*E[Y]\\] Demostración, \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx=\\int_{Rx}\\int_{Ry} xy f(x)f(y) dy dx\\] \\[=\\int_{Rx} x f(x)\\left( \\int_{Ry} y f(y) dy \\right) dx=E[Y] \\int_{Rx} xf(x) dx=E[X]E[Y] \\] Como resultado de lo anterior, si \\(X\\) e \\(Y\\) son independientes: \\[cov(X,Y)=E[XY]-E[X]*E[Y]=E[X]E[Y]-E[X]E[Y]=0\\] Si dos variables son independientes la covarianza y la correlación son iguales a cero, el inverso de esta afirmación no necesariamente es cierta. Nota: \\[E[Y/X=x]=\\int_{Ry} y * f_{Y/X}(y/x) dy\\] ### Ejercicio Para \\[f(x,y)=\\frac{x(1+3y^2)}{4} \\quad 0&lt;x&lt;2, \\quad 0&lt;y&lt;1\\] Calcular \\[P(1/4&lt;X&lt;1/2 | Y=1/3)\\] Solución Se debe encontrar \\(f_{X|Y}(X|Y)=\\frac{f(x,y)}{f(y)}\\) \\[f(y)=\\int_0^2 \\frac{x(1+3y^2)}{4} dx=\\frac{(1+3y^2)}{4} \\int_0^2 x dx=\\frac{(1+3y^2)}{4} \\frac{x^2}{2}/_0^2=\\] \\[=\\frac{(1+3y^2)}{4}(2)=\\frac{(1+3y^2)}{2}\\] \\[f_{X|Y}(X|Y)=\\frac{\\frac{x(1+3y^2)}{4}}{\\frac{(1+3y^2)}{2}}=\\frac{x}{2}\\] Verificando que sea correcta \\[\\int_0^2 \\frac{x}{2}dx=\\frac{x^2}{4}/_0^2=1\\] \\[P(1/4&lt;X&lt;1/2 | Y=1/3)=\\int_{1/4}^{1/2} f_{X|Y}(X|Y) dx=\\int_{1/4}^{1/2} \\frac{x}{2} dx=\\frac{x^2}{4}/_{0.25}^{0.5}=\\] \\[=\\frac{0.5^2}{4}-\\frac{0.25^2}{4}=\\frac{3}{64}\\] "],["tema-2-distribuciones-muestrales.html", "2 Tema 2: Distribuciones muestrales 2.1 Muestras y población 2.2 Parámetros, estadísticas y estimadores. 2.3 Distribución muestral 2.4 Distribución muestral para la media 2.5 Teorema del límite central 2.6 Distribución muestral para la diferencia de medias 2.7 Distribución muestral para la proporción 2.8 Distribución muestral para la varianza 2.9 Distribución \\(\\chi^2\\) 2.10 Distribución t-student 2.11 Distribución Fisher 2.12 Ejercicios", " 2 Tema 2: Distribuciones muestrales A partir de este tema la estadística esta vinculada con la inferencia sobre los parámetros de la información/datos. 2.1 Muestras y población Definición: Una población es una colección de objetos, estos objetos tienen variables. Sea nuestra población \\(U\\), esta población puede ser finita o infinita Finitas, \\[U=\\{u_1, u_2, \\ldots , u_i,...,u_N \\}\\] Infinita, \\[U=\\{u_1, u_2, \\ldots , u_i,... \\}\\] Cada elemento de \\(U\\) tiene variables o características asociadas: \\[u_i=\\{X_{i1}, X_{i2}, \\ldots, X_{iP} \\}\\] \\[u_j=\\{X_{j1}, X_{j2}, \\ldots, X_{jP} \\}\\] Definición, Muestra: Una muestra es un subconjunto de U. Normalmente una muestra tiene un tamaño \\(n\\), el mecanismo para obtener la muestra de \\(U\\) puede ser con reposición o sin reposición, en cualquier caso podemos anotar esto de la siguiente forma, sea \\(s\\) una muestra: \\[s=\\{u_{1}^*,u_2^*, \\ldots, u_n^* \\}\\] Note que los elementos \\(u_1\\) y \\(u_1^*\\) no necesariamente son los mismos. El subconjunto \\(s\\) no es único y en realidad existen muchas muestras posibles, según el contexto, esto depende: Del tamaño de \\(N\\), \\(n\\) Del mecanismo s/rep, c/rep. Ejercicios, Sea la población \\(U=\\{a,b,c,d,e,f\\}\\), se define una muestra de \\(n=3\\), escriba todas las muestras posibles según ambos mecanismos de reposición. Solución, * (s/rep), 20: \\(s_1=\\{a,b,c\\}\\), \\(s_2=\\{a,b,d \\}\\), \\(\\ldots\\) ,\\(s_{20}=\\{d,e,f\\}\\) * (c/rep), 216: \\(s_1=\\{a,a,a\\}\\), \\(s_2={a,a,b}\\), \\(\\ldots\\), \\(s_{216}=\\{f,f,f\\}\\) En una población de 90 estudiantes, si se define una muestra de 10 estudiantes, según ambos mecanismos de selección ¿Cuantas muestras se pueden armar? s/rep: 5720645481903 c/rep: 34867844009999998976 Sin reposición: \\[Muestras_{Posibles}=\\binom{N}{n}\\] Con reposición: \\[Muestras_{Posibles}=N^n\\] Imaginemos a la primera variable de interés \\(X_1\\), para el universo esta variable tiene los elementos: \\[X_1=\\{X_{11}, X_{21}, X_{31}, \\ldots, X_{N1} \\}\\] Imaginemos que observamos a \\(X_1\\), para la muestra. \\[X_1^*=\\{X_{11}^*, X_{21}^*, X_{31}^*, \\ldots, X_{n1}^* \\}\\] Estos \\(X_{i1}^*\\) para los \\(i=1,\\ldots,n\\) son variables aleatorias. Por lo tanto \\(X_1\\) es un vector aleatorio de tamaño \\(n\\). De ahora en adelante vamos a trabajar con un solo vector aleatorio denominado \\(X\\), de tal forma que este sea la colección de \\(n\\) variables aleatorias. \\[X=\\{X_1,X_2,\\ldots,X_n \\}\\] Definición. La colección del vector aleatorio \\(X=\\{X_1,X_2,\\ldots,X_n \\}\\), son independientes e idénticamente distribuidas (iid) si la distribución conjunta de las \\(n\\) variables puede ser escrita como \\(f(x_1,x_2,\\ldots,x_n)=f(x_1)*f(x_2)*\\ldots*f(x_n)\\) y además todas las \\(x_i\\) tienen la misma función de distribución \\(F(x)\\). Definición Sea \\(N\\) el tamaño de la población y \\(n\\) el tamaño de la muestra, ambos valores para fines de este capítulo son fijas o constantes. 2.2 Parámetros, estadísticas y estimadores. El objetivo de la estadística es aprender acerca de las características de una población. Estas características las vamos a llamar parámetros. Definición, Un parámetro \\(\\theta\\) es una función sobre la población \\(U\\). \\[\\theta=f(U,X,Y,Z,\\ldots)\\] Nota: Los parámetros de una población son constantes. Ejemplo, sea el universo los 10 primeros números naturales y sus valores. \\(Y=\\{1,2,3,4,5,6,7,8,9,10\\}\\). Sobre estos valores de esta población de \\(N=10\\) se pueden calcular los siguientes parámetros. Total \\[\\theta_1=t_y=\\sum_U y_i=55\\] * Media \\[\\theta_2=\\mu_y=\\frac{t_y}{N}=\\frac{55}{10}=5.5\\] Máximo: \\(\\theta_3=max(y)=10\\) Mínimo: \\(\\theta_4=max(y)=1\\) Es posibles hacer transformaciones sobre \\(Y\\), sea \\(Z\\) una variables binaria que identifique a los números primos de \\(Y\\); \\(1=primo\\), \\(0=\\sim primo\\) \\[Z=\\{1,1,1,0,1,0,1,0,0,0 \\}\\] Calcular el promedio de \\(Z\\) \\[\\theta_5=\\mu_z=\\frac{5}{10}=0.5\\] Cuando obtenemos la media de un vector binario, obtenemos lo que se denomina un proporción \\[\\theta_5=P_a=\\frac{\\#A}{N}\\] \\[\\theta_5=P_{primos}=\\frac{\\#primos}{N}\\] Definición, estadística Se denomina estadística a una función sobre la muestra Definición, estimador Un estimador \\(\\hat{\\theta}\\) para el parámetro \\(\\theta\\) es una estadística que busca aproximar/adivinar el valor de \\(\\theta\\) Ejemplo, para la variable \\(Y=\\{1,2,3,4,5,6,7,8,9,10\\}\\), imaginar que se selecciona un muestra de tamaño \\(n=4\\) s/rep. La cantidad de muestras posibles es de 210, supongamos que realizamos 2 procesos de selección para la muestra y obtenemos: \\(s_1=\\{8, 1, 3, 7\\}\\) \\(s_2=\\{8, 2, 6, 5\\}\\) Sabemos que el parámetro del total de \\(Y\\) es \\(t_y=55\\), ¿Qué? función se puede aplicar sobre la muestra para postular a un estimador que se aproxime a \\(t_y\\) sobre las 2 muestras seleccionadas \\[\\hat{\\theta}_1=\\hat{t}_y=\\sum_s y_i\\] Para \\(s_1\\) el valor del estimador es de \\(\\hat{t}_{y,s1}=19\\), \\(\\hat{t}_{y,s2}=21\\), los valores evaluados sobre una muestra y un estimador se conoce como estimación \\[\\hat{\\theta}_2=\\hat{t}_y=\\prod_s y_i\\] Las estimaciones con el estimador propuesto \\(\\hat{t}_{y,s1}=8*1*3*7=168\\), \\(\\hat{t}_{y,s1}=480\\) \\[\\hat{\\theta}_3=\\frac{\\prod_s y_i}{3}\\] \\(\\hat{t}_{y,s1}=56\\), \\(\\hat{t}_{y,s2}=160\\) \\[\\hat{\\theta}_4=\\frac{\\sum_{s}y_i^2}{2}\\] \\(\\hat{t}_{y,s1}=61.5\\), \\(\\hat{t}_{y,s2}=64.5\\) \\[\\hat{\\theta}_5=\\frac{N}{n} \\sum_{s}y_i \\] \\(\\hat{t}_{y,s1}=47.5\\), \\(\\hat{t}_{y,s2}=52.5\\) 2.3 Distribución muestral Recordar que una estadística es una función sobre la muestra y sobre los valores que toman las variables aleatorias vinculadas a esta. Como la estadística es una función sobre las muestras aleatorias (muestras posibles) las evaluaciones que se realizan para cada una de las muestras posibles (estimadores) conforman lo que vamos a denominar una distribución muestral. Por ejemplo si planteamos al estimador del parámetro del total, recordar: \\[\\theta=t_y=\\sum_U y_i\\] Un estimador para este parámetro será: \\[\\hat{\\theta}=\\hat{t}_y=\\frac{N}{n} \\sum_s y_i\\] Este \\(\\hat{\\theta}\\) es una estadística sobre las muestras aleatorias, por lo tanto podemos decir que existe una distribución de probabilidad para este estimador, a esa distribución de probabilidad se conoce como distribución muestral. Ejemplo práctico. Supongamos que de una población de 6 personas tenemos la información de sus ingresos mensuales. \\(Y_{Ingresos}=\\{2000,3000,3500,0,6000,4500\\}\\). \\(N=6\\) Supongamos que seleccionamos una muestra de tamaño \\(n=3\\) de esta población, para ambos mecanismos de selección (s/rep, c/rep), se pide para ambos mecanismos: Conocer la cantidad de muestras posibles y mostrar estas. Para el estimador \\[\\hat{\\bar{Y}}=\\frac{1}{n}\\sum_s y_i\\] construir su distribución muestral y calcular su esperanza y su varianza * Para el estimador; \\[\\hat{t}_y=\\frac{N}{n}\\sum_s y_i\\] construir su distribución muestral y calcular su esperanza y su varianza Respuesta, (S/rep) Las muestras posibles son 20, estas muestras posibles son: Y&lt;-c(2000,3000,3500,0,6000,4500) s&lt;-combn(Y,3) s ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 2000 2000 2000 2000 2000 2000 2000 ## [2,] 3000 3000 3000 3000 3500 3500 3500 ## [3,] 3500 0 6000 4500 0 6000 4500 ## [,8] [,9] [,10] [,11] [,12] [,13] ## [1,] 2000 2000 2000 3000 3000 3000 ## [2,] 0 0 6000 3500 3500 3500 ## [3,] 6000 4500 4500 0 6000 4500 ## [,14] [,15] [,16] [,17] [,18] [,19] ## [1,] 3000 3000 3000 3500 3500 3500 ## [2,] 0 0 6000 0 0 6000 ## [3,] 6000 4500 4500 6000 4500 4500 ## [,20] ## [1,] 0 ## [2,] 6000 ## [3,] 4500 Para el estimador de la media; Tomar en cuenta que el valor del parámetro de la media poblacional es: \\(\\mu_y=\\sum_U y_i /N=3166.667\\) y&lt;-apply(s,2,sum)/3 #Distribución muestral para el estimador de la media hist(y) abline(v=mean(Y),col=&quot;red&quot;,lwd=3) # calcular la esperanza y la varianza uy&lt;-sum(y*(1/20)) # esperanza del estimador de la media sum((y-uy)^2*(1/20)) # varianza de la media muestral ## [1] 711111.1 \\[E[\\hat{\\theta}]=\\sum_{Rs} \\hat{\\theta_s} P(\\hat{\\theta}=\\hat{\\theta_s})\\] \\[V(\\hat{\\theta})=E[(\\hat{\\theta}-E[\\hat{\\theta}])^2]=\\sum_{s}(\\hat{\\theta_s}-E[\\hat{\\theta}])^2*P(\\hat{\\theta}=\\hat{\\theta})\\] Nota, Si \\(E[\\hat{\\theta}]=\\theta\\) decimos que el estimador \\(\\hat{\\theta}\\) es un estimador insesgado (sin sesgo) El estimador de la media muestral, es un estimador insesgado de la media poblacional. Para el estimador del total; Tomar en cuenta que el valor del parámetro del total poblacional es: \\(t_y=\\sum_U y_i=19000\\) ty&lt;-apply(s,2,sum)*(6/3) #Distribución muestral para el estimador del total hist(ty) abline(v=sum(Y),col=&quot;red&quot;,lwd=3) pty&lt;-sum(ty*(1/20)) # esperanza sum((ty-pty)^2*(1/20)) # varianza de la media muestral ## [1] 25600000 \\[E[\\hat{t}_y]=E[N*\\bar{Y}]=N E[\\bar{Y}]=N*u_y=N*\\frac{\\sum_U y_i}{N}=\\sum_U {y_i}=t_y\\] Repetir los cálculos para un muestreo con reposición. Muestras probables \\(6^3=N^n=216\\). Y&lt;-round(rnorm(25,30,5)) s&lt;-combn(Y,10) y&lt;-apply(s,2,sum)/10 hist(y) abline(v=mean(Y),col=&quot;red&quot;,lwd=2) 2.4 Distribución muestral para la media Recordar que para una población (\\(U\\)) con alguna variable \\(X\\) de tipo cuantitativa se puede obtener el parámetro de la media, definido como: \\[\\mu_x=\\frac{\\sum_U x_i}{N}\\] Esta variable \\(X\\) en la población por lo tanto tiene su media \\(\\mu_x\\) y también tiene su varianza, denotada por \\(\\sigma_x^2\\). Teorema: Sean \\(X_1,X_2,\\ldots,X_n\\) variables aleatorias para una muestra de tamaño \\(n\\) extraida de la población \\(U\\), donde estas \\(X_i\\) independientes e idénticamente distribuidas (iid) como: \\(X_i\\sim .(E[X_i]=\\mu_x,V(X_i)=\\sigma_x^2)\\), entonces, si: \\[\\bar{X}=\\frac{\\sum_s x_i}{n}\\] Tenemos que \\[E[\\bar{X}]=\\mu_x\\] \\[V(\\bar{X})=\\sigma^2_{\\bar{x}}=\\frac{\\sigma^2_x}{n}\\] Demostración, \\[E[\\bar{X}]=E\\left[\\frac{\\sum_s x_i}{n}\\right]=\\frac{1}{n}E[x_1+x_2+\\ldots+x_n]=\\frac{1}{n}\\left(E[x_1]+E[x_2]+\\ldots+E[x_n] \\right)=\\] \\[=\\frac{1}{n}(\\mu_x+\\mu_x+\\ldots+\\mu_x)=\\frac{n \\mu_x}{n}=\\mu_x\\] Si, \\(X\\) e \\(Y\\) son independientes \\(Cov(X,Y)=0\\). \\[V(X+Y)=V(X)+V(Y)\\] \\[V(\\bar{X})=V\\left(\\frac{\\sum_s x_i}{n}\\right)=\\frac{1}{n^2}V(x_1+x_2+\\ldots+x_n)=\\frac{1}{n^2}\\{V(x_1)+\\ldots+V(x_n)\\}=\\] \\[=\\frac{1}{n^2}(\\sigma^2_x+\\sigma^2_x+\\ldots+\\sigma^2_x)=\\frac{n \\sigma_x^2}{n^2}=\\frac{\\sigma^2_x}{n}\\] 2.5 Teorema del límite central Teorema: Si \\(\\bar{X}\\) es la media de una muestra aleatoria de tamaño \\(n\\). Tomada de una población \\(U\\) con media \\(\\mu_x\\) y varianza finita \\(\\sigma^2_x\\). Entonces la forma límite de la distribución de: \\[Z=\\frac{\\bar{X}-E[\\bar{X}]}{\\sqrt{V(\\bar{X})}}=\\frac{\\bar{X}-\\mu_x}{\\frac{\\sigma_x}{\\sqrt{n}}}\\] a medida que \\(n \\rightarrow \\infty\\), podemos asegurar que \\(Z\\sim N(0,1)\\), en este marco se puede decir a medida que \\(n\\) es más grande \\(\\bar{X}\\sim N(\\mu_x,\\frac{\\sigma^2_x}{n})\\) Nota: esta idea de \\(n\\) grande se usa tradicionalmente el valor de \\(n&gt;30\\), hay textos que plantean \\(n=20\\). Simulación del teorema del límite central: N&lt;-1000000 x&lt;-round(runif(N,0,10000),0)# ingresos mensuales de una población hist(x) n&lt;-30 choose(N,n) ## [1] 3.768348e+147 #simular 1000 muestras distintas de tamaño n y calcular su media. xbar&lt;-NULL for(i in 1:10000){ s&lt;-sample(x,n) xbar[i]&lt;-mean(s) } hist(xbar) plot(density(xbar),col=&quot;blue&quot;,lwd=2) points(density(rnorm(10^6,mean(x),sqrt(var(x)*((n-1)/n))/sqrt(n))),type=&quot;l&quot;,col=&quot;red&quot;,lwd=2) 2.6 Distribución muestral para la diferencia de medias Sean dos poblaciones \\(U_1\\) y \\(U_2\\) independientes con medias y varianzas respectivamente: \\(\\mu_{x_1}\\) y \\(\\mu_{x_2}\\), \\(\\sigma^2_{x_1}\\) y \\(\\sigma^2_{x_2}\\). Teorema: La distribución muestral de las diferencias de media \\(\\bar{X_1}-\\bar{X_2}\\) esta tiene una distribución aproximadamente normal (\\(n\\rightarrow \\infty\\)) con medias y varianzas dadas por: \\[E[\\bar{X_1}-\\bar{X_2}]=\\mu_{x_1}-\\mu_{x_2}\\] \\[V(\\bar{X_1}-\\bar{X_2})=\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\] Demostración: \\[E[\\bar{X_1}-\\bar{X_2}]=E[\\bar{X_1}]-E[\\bar{X_2}]=\\mu_{x_1}-\\mu_{x_2}\\] \\[V(\\bar{X_1}-\\bar{X_2})=V(\\bar{X_1})+V(\\bar{X_2})=\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\] 2.7 Distribución muestral para la proporción La proporción no es nada más que un caso especial de la media para \\(X\\) que toma valores binarios según alguna característica de interés. Sea \\(P_A=\\frac{\\#A}{N}=\\frac{\\sum_U x_i}{N}\\), \\(x_i=1\\) si \\(i \\in A\\) \\(x_i=0\\) eoc. la proporción de alguna característica de la población. Así la el estimador de la proporción sera: \\[\\hat{P}_A=\\frac{\\sum_s{x_i}}{n}=\\frac{\\#a}{n}\\] Teorema: Para el estadístico \\(\\hat{P}_A\\) se cumple cuando \\(n\\) tiende a infinito los siguientes resultados: \\(E[\\hat{P}_A]=P_A\\) \\(V(\\hat{P}_A)=\\frac{\\sigma^2_A}{n}\\) \\(\\hat{P}_A\\sim N(P_A,\\frac{\\sigma^2_A}{n})\\), cuando \\(n \\rightarrow \\infty\\) Tarea, encontrar la forma de \\(\\sigma^2_A\\), sabiendo que \\(x_i\\) es binaria. \\[\\sigma^2_A=\\frac{\\sum_U(x_i-\\mu_x)^2}{N}= P_A *(1-P_A)\\] 2.8 Distribución muestral para la varianza Recordar que para una población \\(U\\), si observamos a una variable de interés respecto sus características podemos obtener medidas de centralidad y también medidas de variabilidad, por ejemplo, sea \\(X\\) una variables definida para toda la población, y definamos los siguientes parámetros de \\(X\\). \\[\\mu_x=\\frac{\\sum_U x_i}{N}\\] Esta \\(\\mu_x\\) es una medida de centralidad, normalmente conocida como media, promedio de \\(X\\), la otra medida puede ser: \\[\\sigma^2_x=\\frac{\\sum_U (x_i-\\mu_x)^2}{N}\\] \\(\\sigma^2_x\\) es la varianza poblacional Ejemplo, Sea una población de \\(N=5\\) elementos con la variable \\(X=\\{10,15,20,20,35\\}\\), calcular \\(\\mu_x\\) y \\(\\sigma^2_x\\). \\(\\mu_x=20\\) \\(\\sigma^2_x=70\\) Suponer que se toman muestras aleatorias de esta población de tamaño \\(n=3\\) sin reposición. La cantidad de muestras posibles es 10. x&lt;-c(10,15,20,20,35) s&lt;-combn(x,3) s ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 10 10 10 10 10 10 15 ## [2,] 15 15 15 20 20 20 20 ## [3,] 20 20 35 20 35 35 20 ## [,8] [,9] [,10] ## [1,] 15 15 20 ## [2,] 20 20 20 ## [3,] 35 35 35 #distribución muestral de la media mean(apply(s, 2, mean)) ## [1] 20 Pensemos para el caso de la varianza en posibles estadísticos (estimadores): \\[\\hat{\\theta}_1=\\hat{\\sigma}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n}\\] \\[\\hat{\\theta}_2=\\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] x&lt;-c(10,15,20,20,35) n&lt;-3;N&lt;-5 s ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 10 10 10 10 10 10 15 ## [2,] 15 15 15 20 20 20 20 ## [3,] 20 20 35 20 35 35 20 ## [,8] [,9] [,10] ## [1,] 15 15 20 ## [2,] 20 20 20 ## [3,] 35 35 35 var(x)*((N-1)/N) ## [1] 70 theta1&lt;-apply(s,2,var)*((n-1)/n) theta2&lt;-apply(s,2,var) theta1 ## [1] 16.666667 16.666667 116.666667 ## [4] 22.222222 105.555556 105.555556 ## [7] 5.555556 72.222222 72.222222 ## [10] 50.000000 theta2 ## [1] 25.000000 25.000000 175.000000 ## [4] 33.333333 158.333333 158.333333 ## [7] 8.333333 108.333333 108.333333 ## [10] 75.000000 plot(density(theta1),xlim=c(-50,300)) points(density(theta2),col=&quot;red&quot;,type=&quot;l&quot;) mean(theta1) #E[] ## [1] 58.33333 mean(theta2) #E[] ## [1] 87.5 Notar que para el ejemplo \\(E[\\hat{\\theta_1}]\\) ni \\(E[\\hat{\\theta_2}]\\) se acercan a \\(\\sigma^2_x\\), sin embargo, \\(E[\\theta_2]=S^2_x\\). \\[S^2_x=\\frac{\\sum_U (x_i-\\mu_x)^2}{N-1}\\] &gt; Teorema Sea \\(X_1,X_2,\\ldots,X_n\\) una muestra aleatoria extraída de una población Normal \\(N(\\mu_x,\\sigma^2_x)\\), definamos al estadístico: \\[\\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] Entonces, se cumple \\[\\chi^2=\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}\\sim \\chi^2(n-1)\\] Simulación; #población de tamaño N=1000 set.seed(999) x&lt;-rnorm(1000,20,5) hist(x) #suponer que se extra una muestra de n=20 de esta población, choose(1000,20) ## [1] 3.394828e+41 n&lt;-20 #vamos a simular unas 1500 muestras posibles sigma=25 x2&lt;-NULL for(i in 1:100000){ x2[i]&lt;-(var(sample(x,20))*(n-1))/25 } x2 ## [1] 28.012821 13.303299 12.916730 ## [4] 15.444633 20.070571 18.598947 ## [7] 12.987323 26.229370 17.443031 ## [10] 29.634228 19.228462 16.847166 ## [13] 13.837075 12.306583 20.316401 ## [16] 6.691069 18.716776 17.617646 ## [19] 11.830859 13.593042 21.748010 ## [22] 11.481425 9.106947 9.473332 ## [25] 8.428679 13.683606 15.170322 ## [28] 12.459595 16.076973 11.618863 ## [31] 19.399864 18.619069 15.620865 ## [34] 23.144277 19.006885 31.386997 ## [37] 16.965785 23.948281 12.457150 ## [40] 10.199054 15.890153 14.075756 ## [43] 18.272734 16.332558 14.613984 ## [46] 25.587406 17.274712 13.451648 ## [49] 15.951381 29.599580 28.751381 ## [52] 27.883479 24.847844 28.122133 ## [55] 21.778322 13.042337 12.756593 ## [58] 13.588587 15.381787 15.173170 ## [61] 17.064814 10.215598 21.617442 ## [64] 13.529998 11.333242 11.942220 ## [67] 13.961767 11.204895 9.383787 ## [70] 16.447210 14.246598 22.518621 ## [73] 12.660786 17.883892 13.073025 ## [76] 21.860708 25.675834 11.964393 ## [79] 16.152063 33.642039 20.788343 ## [82] 25.146765 18.091672 27.518788 ## [85] 17.450027 5.250517 35.510067 ## [88] 20.579231 18.107810 7.837748 ## [91] 28.530112 20.276653 21.042629 ## [94] 17.020561 11.251209 12.107773 ## [97] 19.502901 9.780230 26.695559 ## [100] 20.650701 8.803620 17.234677 ## [103] 16.699761 14.213930 21.911950 ## [106] 23.045968 15.181081 12.116822 ## [109] 26.385342 23.237274 20.975653 ## [112] 20.512500 19.050228 14.114346 ## [115] 26.703015 15.162247 14.347395 ## [118] 16.567249 12.277332 20.445499 ## [121] 13.605211 26.066466 20.531418 ## [124] 11.402210 18.172740 19.021145 ## [127] 11.423122 16.202069 22.060284 ## [130] 26.552551 22.005707 7.734121 ## [133] 16.220572 14.392079 14.301308 ## [136] 21.769430 22.420456 15.618478 ## [139] 17.447861 17.127857 14.726064 ## [142] 25.266101 15.663966 17.745215 ## [145] 18.590808 19.813088 35.369779 ## [148] 22.437589 26.518719 5.550327 ## [151] 22.442252 22.630876 10.092094 ## [154] 11.243247 21.629633 25.316789 ## [157] 14.803836 17.506524 13.280020 ## [160] 22.070279 19.336450 39.384576 ## [163] 22.162003 16.597604 21.865927 ## [166] 22.053510 17.358355 14.287258 ## [169] 15.307627 15.212806 20.894767 ## [172] 12.342412 15.181616 16.823311 ## [175] 22.328185 16.870523 11.932493 ## [178] 14.608943 20.831203 21.081735 ## [181] 15.847040 13.459898 14.848697 ## [184] 16.029962 11.564021 14.063650 ## [187] 16.309441 19.966832 13.932327 ## [190] 9.026528 18.082221 12.720960 ## [193] 15.024540 18.718807 25.729287 ## [196] 15.350011 20.544050 18.969261 ## [199] 21.585216 31.963216 24.672649 ## [202] 25.671613 13.023054 28.851590 ## [205] 16.483956 11.303823 26.648202 ## [208] 8.478335 17.726722 15.430695 ## [211] 24.317257 32.335164 9.607395 ## [214] 26.820571 12.723560 15.235108 ## [217] 15.809233 22.814399 24.265359 ## [220] 20.872881 13.048042 12.300579 ## [223] 18.075927 11.937559 17.417532 ## [226] 18.025925 23.255247 20.109420 ## [229] 23.253181 14.372813 21.812857 ## [232] 14.024146 27.546107 15.498092 ## [235] 13.235267 14.098803 35.153901 ## [238] 18.649959 21.851561 19.819864 ## [241] 15.316509 15.795272 13.516208 ## [244] 20.682192 14.620640 14.449217 ## [247] 25.434318 26.168725 23.276445 ## [250] 28.915826 17.596923 28.010290 ## [253] 18.341387 17.879718 17.026406 ## [256] 27.204161 16.697266 26.320726 ## [259] 13.793118 24.030272 16.239461 ## [262] 30.971929 13.086218 11.798937 ## [265] 9.299183 19.134689 25.052224 ## [268] 13.346834 15.982050 23.669918 ## [271] 22.165420 18.176970 17.694919 ## [274] 20.192060 20.412274 35.497393 ## [277] 17.407584 19.655298 9.347727 ## [280] 25.572063 15.704892 13.559946 ## [283] 16.266726 14.442831 23.863729 ## [286] 17.000075 19.519409 25.114011 ## [289] 20.148385 22.266228 25.853997 ## [292] 22.662551 20.801108 10.925624 ## [295] 17.810104 26.175869 14.277400 ## [298] 13.318309 16.595470 15.266218 ## [301] 21.262551 8.580367 20.701016 ## [304] 16.503482 11.918446 13.505003 ## [307] 30.125166 13.223280 18.575549 ## [310] 34.856473 24.677999 20.881330 ## [313] 15.175015 20.602794 22.235731 ## [316] 24.510002 16.103575 14.920085 ## [319] 31.667352 25.540828 11.809267 ## [322] 18.319506 21.149436 21.635312 ## [325] 14.180294 19.957223 15.858002 ## [328] 17.166201 14.493781 14.093451 ## [331] 14.014025 22.213693 32.223125 ## [334] 17.241425 13.288698 13.250723 ## [337] 14.734604 13.995601 17.404789 ## [340] 18.592486 15.247989 17.637603 ## [343] 9.364431 26.359784 13.982073 ## [346] 20.573854 18.149615 9.110028 ## [349] 22.766342 21.045694 22.258774 ## [352] 12.546646 21.359003 15.692677 ## [355] 16.874471 20.538731 15.455086 ## [358] 25.956684 18.774150 16.489980 ## [361] 18.071091 28.006534 23.467235 ## [364] 17.147189 16.792277 15.447061 ## [367] 10.594702 12.256940 16.145375 ## [370] 17.736032 17.828512 12.531759 ## [373] 31.573850 20.210519 16.019958 ## [376] 17.578239 17.772428 15.250837 ## [379] 20.502192 14.979508 13.992929 ## [382] 21.260858 13.987037 18.380611 ## [385] 24.629809 21.920440 16.724372 ## [388] 13.391569 26.174884 10.900662 ## [391] 8.290249 9.866609 21.867045 ## [394] 19.955929 15.974358 9.058562 ## [397] 5.966568 34.844445 24.948710 ## [400] 20.310470 19.546296 11.888163 ## [403] 12.286091 9.132744 23.602785 ## [406] 20.014303 14.249253 15.506009 ## [409] 15.437651 9.281142 19.894464 ## [412] 23.939253 19.924322 15.741860 ## [415] 22.561480 28.045692 33.280957 ## [418] 22.673003 22.570943 11.497906 ## [421] 19.209914 18.966212 19.127201 ## [424] 13.153828 16.993053 21.556625 ## [427] 10.286481 14.782502 23.896517 ## [430] 13.143720 28.732325 8.044952 ## [433] 23.947529 17.891754 18.916209 ## [436] 17.088394 12.153914 23.588815 ## [439] 27.825393 18.556936 18.475666 ## [442] 18.546475 19.336447 29.765206 ## [445] 19.891351 12.766444 21.244537 ## [448] 31.254842 14.848853 19.255805 ## [451] 30.316743 15.625976 10.203716 ## [454] 17.495950 23.960264 17.154687 ## [457] 15.679591 22.816883 14.221807 ## [460] 21.113025 20.645354 20.407738 ## [463] 20.927321 16.558282 9.800061 ## [466] 14.866611 18.587171 16.813358 ## [469] 13.847721 22.178993 26.207978 ## [472] 21.315105 32.031176 20.617618 ## [475] 31.938098 10.901848 15.117016 ## [478] 8.140391 11.466804 14.176978 ## [481] 16.560051 18.967196 17.458704 ## [484] 10.764479 20.389500 12.153445 ## [487] 21.915928 16.673494 22.262874 ## [490] 23.939511 20.350276 14.509098 ## [493] 9.036172 14.584719 14.211505 ## [496] 21.439348 18.186422 21.754326 ## [499] 16.357508 16.754581 20.204783 ## [502] 9.461267 12.786030 10.312692 ## [505] 23.812264 10.630994 25.660905 ## [508] 24.008654 19.444126 13.635916 ## [511] 10.520709 20.515147 13.876175 ## [514] 15.491769 21.450161 17.486085 ## [517] 15.720568 17.524114 19.871122 ## [520] 22.700891 19.227107 17.761933 ## [523] 27.973973 19.886723 20.925573 ## [526] 23.055274 8.174269 13.171267 ## [529] 21.546352 16.652625 22.175516 ## [532] 23.707881 11.228519 11.387434 ## [535] 16.447506 24.676754 16.227232 ## [538] 11.739386 21.441527 15.707938 ## [541] 18.315900 19.424615 17.308554 ## [544] 24.028920 14.741197 20.185355 ## [547] 16.585532 26.366693 20.555072 ## [550] 12.281362 20.937916 23.546578 ## [553] 15.731429 19.801307 16.784120 ## [556] 20.765949 9.202193 11.329222 ## [559] 14.202221 11.452321 17.653245 ## [562] 18.144876 24.238262 15.465025 ## [565] 19.793484 11.315699 11.966883 ## [568] 10.439119 22.554602 15.515201 ## [571] 26.437958 26.615291 16.844931 ## [574] 26.615512 15.257409 19.586495 ## [577] 14.350989 13.182293 9.553267 ## [580] 4.491736 11.178881 22.472408 ## [583] 15.075446 18.204019 15.878175 ## [586] 15.658180 21.351499 16.204857 ## [589] 20.442849 10.947412 17.843151 ## [592] 15.867059 20.304103 17.002257 ## [595] 11.257525 20.192141 15.917608 ## [598] 14.602950 26.940746 19.137400 ## [601] 11.781175 14.305902 20.795786 ## [604] 30.453513 15.538217 15.403194 ## [607] 30.069716 7.593631 26.127048 ## [610] 15.769910 14.562004 30.980415 ## [613] 36.021805 24.049130 11.271203 ## [616] 17.409826 29.315625 23.809991 ## [619] 29.126933 14.257977 15.056230 ## [622] 23.566388 15.174209 17.382185 ## [625] 15.433936 26.392629 26.628483 ## [628] 15.262434 22.175066 19.249082 ## [631] 18.221410 17.595876 7.951111 ## [634] 17.223180 30.272167 16.662376 ## [637] 17.849205 9.536688 22.741268 ## [640] 23.950851 19.674210 14.901407 ## [643] 11.114026 14.322474 19.217378 ## [646] 18.881117 14.227646 12.417058 ## [649] 21.193379 6.238202 25.804310 ## [652] 16.128059 21.292803 13.583191 ## [655] 13.416589 16.998586 19.670594 ## [658] 19.571160 13.261077 10.385480 ## [661] 22.444720 15.186642 8.549110 ## [664] 12.592156 12.126122 21.612231 ## [667] 16.106128 7.185419 19.155048 ## [670] 14.783458 12.880983 23.028895 ## [673] 14.603729 13.465318 21.971983 ## [676] 28.639976 14.957262 16.521305 ## [679] 15.156209 16.997971 18.066716 ## [682] 10.105663 12.527995 25.242331 ## [685] 29.212772 13.279441 18.518994 ## [688] 23.081568 8.991684 18.891378 ## [691] 14.390195 19.481693 19.247036 ## [694] 15.173736 19.686150 15.104820 ## [697] 18.368197 17.131629 15.269682 ## [700] 27.958178 14.609845 18.348459 ## [703] 17.720580 17.278284 15.970391 ## [706] 31.189266 20.609505 13.667190 ## [709] 20.343798 23.676878 16.817175 ## [712] 27.798076 14.380824 23.119645 ## [715] 20.294323 12.397698 18.077482 ## [718] 15.548011 13.842401 15.732343 ## [721] 9.482712 8.778730 37.133787 ## [724] 24.199658 17.041008 16.273570 ## [727] 22.309599 14.140296 15.538825 ## [730] 19.018784 14.509132 22.133283 ## [733] 29.898216 21.899834 8.319081 ## [736] 28.376068 16.159040 14.711742 ## [739] 16.128796 17.363660 20.650008 ## [742] 20.296518 17.752446 19.662599 ## [745] 24.055001 26.253885 15.864813 ## [748] 22.608158 20.920132 11.771987 ## [751] 17.570611 20.586085 25.208490 ## [754] 20.049499 20.545025 22.385286 ## [757] 27.045100 22.773739 12.998953 ## [760] 18.532624 15.560464 17.676388 ## [763] 13.203288 18.897017 22.478345 ## [766] 19.065920 21.805359 16.342205 ## [769] 15.511562 29.192431 22.918919 ## [772] 20.299212 14.677958 19.784591 ## [775] 13.834712 16.187329 26.735234 ## [778] 14.768448 23.777946 29.613875 ## [781] 12.432792 37.751422 20.805643 ## [784] 16.105735 13.631205 14.710745 ## [787] 16.738886 18.346160 17.926659 ## [790] 11.742877 17.411051 18.677338 ## [793] 15.471975 25.991338 16.839181 ## [796] 11.470087 10.955131 26.606070 ## [799] 16.820614 24.129111 44.924757 ## [802] 15.479093 15.751811 21.996591 ## [805] 19.340129 11.096019 18.161506 ## [808] 21.631261 17.006621 27.793634 ## [811] 19.260289 9.979732 17.056580 ## [814] 20.108851 19.097732 18.232420 ## [817] 14.339916 19.669007 18.351885 ## [820] 9.760430 13.369856 17.570671 ## [823] 9.413871 16.272449 26.617002 ## [826] 14.471497 25.732663 21.930818 ## [829] 7.796016 13.842560 15.578838 ## [832] 14.404152 12.709148 37.570245 ## [835] 14.800009 19.169672 20.230360 ## [838] 18.097048 15.598180 24.112340 ## [841] 24.500908 10.507524 22.422073 ## [844] 15.871907 10.750142 15.342556 ## [847] 18.915039 21.791224 10.825029 ## [850] 18.084313 16.609825 13.699246 ## [853] 30.363761 8.009254 7.244581 ## [856] 18.225919 14.612970 16.141131 ## [859] 15.521536 10.797909 10.723517 ## [862] 20.243818 14.060388 15.937919 ## [865] 18.372922 7.746550 23.775462 ## [868] 16.209138 23.600860 16.307198 ## [871] 23.121222 12.642161 29.532925 ## [874] 27.326932 16.182682 20.959115 ## [877] 30.061046 25.554703 10.161560 ## [880] 16.051951 15.319699 13.966131 ## [883] 8.243000 16.726587 11.070633 ## [886] 21.128304 13.846221 14.991823 ## [889] 13.534118 9.887567 14.216242 ## [892] 22.186037 11.263336 29.407918 ## [895] 15.972765 8.487906 16.596630 ## [898] 12.153323 19.406587 20.387614 ## [901] 14.509980 12.219619 10.951065 ## [904] 21.920734 20.764404 27.442317 ## [907] 15.071589 24.042310 20.035989 ## [910] 7.720464 17.979013 28.385669 ## [913] 18.999167 22.648348 16.417482 ## [916] 7.843558 23.528440 21.043034 ## [919] 16.771143 19.063782 13.424514 ## [922] 13.113389 14.166799 13.271906 ## [925] 12.066717 19.910054 10.035170 ## [928] 14.076576 22.261232 14.348785 ## [931] 17.136114 38.194209 12.538656 ## [934] 19.148550 21.865993 20.316560 ## [937] 13.671468 18.326424 23.561805 ## [940] 13.496954 25.213289 23.173681 ## [943] 19.031335 12.886222 11.804907 ## [946] 7.554450 17.139227 22.100363 ## [949] 16.843482 8.286483 18.911796 ## [952] 22.663047 8.503524 22.059767 ## [955] 23.421176 13.994647 16.290766 ## [958] 19.480129 10.363203 15.488181 ## [961] 18.086993 24.379918 17.302920 ## [964] 15.080271 17.519433 19.353402 ## [967] 18.856519 17.311082 27.075140 ## [970] 34.118128 11.110204 10.452672 ## [973] 29.724010 34.261895 19.255181 ## [976] 19.463401 13.531030 19.884112 ## [979] 13.418974 9.565242 14.571297 ## [982] 15.543387 13.262746 16.333737 ## [985] 17.943791 20.294559 16.988730 ## [988] 11.853706 14.768046 19.212293 ## [991] 18.752969 14.972371 8.403704 ## [994] 22.308734 12.866281 16.714866 ## [997] 20.025841 11.190805 16.622595 ## [1000] 10.999016 ## [ reached getOption(&quot;max.print&quot;) -- omitted 99000 entries ] plot(density(x2)) points(density(rchisq(1000000,19)),col=&quot;red&quot;,type=&quot;l&quot;) 2.9 Distribución \\(\\chi^2\\) Se dice que una variable aleatoria \\(X\\) tiene una distribución Chi-cuadrado \\(\\chi^2\\) con \\(v\\) grados de libertad. Se escribe como: \\(X \\sim \\chi^2(v)\\), donde el \\(Rx=\\{x&gt;0\\}\\), si su función de densidad es: \\[f(x)=\\frac{1}{2^\\frac{v}{2} \\Gamma(\\frac{v}{2})}*x^{\\frac{v}{2}-1}*e^{-\\frac{x}{2}}\\] curve(dchisq(x,1),xlim=c(0,40),ylim=c(0,0.4)) for(v in 1:30){ curve(dchisq(x,v),add=T,col=v) } Donde, \\[E[X]=v\\] \\[V(X)=2v\\] Ejercicio, Encuentre la probabilidad de que una muestra aleatoria de \\(n=25\\) de un población normal con varianza \\(\\sigma_x^2=9\\), tenga una varianza muestral \\(\\hat{S^2}\\) entre 4 y 15. \\[P(4&lt;\\hat{S}^2_x&lt;15)=P(4*24/9&lt;\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}&lt;15*24/9)=P(10.66&lt;\\chi^2 &lt;40)\\] \\[P(10.66&lt;\\chi^2 &lt;40)=F(40)-F(10.66)=0.9786-0.0087=0.9699\\] pchisq(40,24)#F(40) para un chi2(v=24) ## [1] 0.9786132 pchisq(10.66,24)#F(10.66) para un chi2(v=24) ## [1] 0.008762108 curve(dchisq(x,24),xlim=c(0,60)) abline(v=c(10.66,40),col=&quot;red&quot;) Tomar en cuenta que: \\[\\chi^2=\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}\\sim \\chi^2(n-1)\\] #ejemplo para usar R para calcular probabilidades de la Chi2 pchisq(4,10) # F(t)=P(X&lt;t): F(4) ## [1] 0.05265302 2.10 Distribución t-student Teorema Sea \\(Z\\) una variable aleatoria normal estándar y \\(V\\) una variable aleatoria chi-cuadrado con \\(v\\) grados de libertad. Si \\(Z\\) y \\(V\\) son independientes, entonces la distribución de la variable aleatoria \\(X\\), donde: \\[X=\\frac{Z}{\\sqrt{V/v}}\\] Se comporta como una distribución \\(t\\) con \\(v\\) grados de libertad. En notación, decimos \\(X\\sim t(v)\\). \\[f(x)=\\frac{\\Gamma(\\frac{v+1}{2})}{\\Gamma{(\\frac{v}{2})}\\sqrt{v\\pi} }\\left(1+\\frac{x^2}{v} \\right)^{-(\\frac{v+1}{2})}, \\quad -\\infty&lt;x&lt;\\infty\\] Al igual que la distribución normal estándar, la \\(t\\) es simétrica al rededor del cero. Y levemente más plana que una normal. Nota: Cuando \\(v\\rightarrow \\infty\\) la \\(t \\sim N(\\mu=0,\\sigma^2=\\frac{v}{v-2})\\) v&lt;-5 curve(dnorm(x,0,sqrt(v/(v-2))),xlim=c(-5,5),ylim=c(0,0.4)) curve(dt(x,10),xlim=c(-5,5),main=&quot;t-student (v=10)&quot;,col=&quot;red&quot;,add=T) Corolario Sean \\(X_1, X_2, \\ldots, X_n\\) variables aleatorias e independientes e idénticamente distribuidas (iid) \\(X_i\\sim N(\\mu,\\sigma^2_x), \\quad i=\\{1,\\ldots,n\\}\\). Sean los estimadores: \\[\\bar{X}=\\frac{\\sum_s x_i}{n} \\quad y \\quad \\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{X})^2}{n-1}\\] Entonces, \\[ \\frac{\\bar{X}-\\mu}{\\hat{S}/\\sqrt{n}}\\sim t(v=n-1) \\] Apariencia de la \\(t\\) curve(dt(x,2),xlim=c(-5,5),ylim=c(0,0.4)) for(v in 3:30){ curve(dt(x,v),xlim=c(-5,5),col=v,add=T) } En R, para obtener \\(P(X&lt;t)=F(t)\\) con \\(X\\sim t(v)\\). # P(X&lt;2)=F(2), X ~ t(v=10) pt(2,10) ## [1] 0.963306 Ejercicio, Sea \\(X\\sim t(v=14)\\). Calcular: \\(P(X&gt;0.67)=1-P(X\\leq 0.67)=1-F(0.67)=1-0.7431=0.2569\\) \\(P(X&lt;0.5)=F(0.5)=0.6875\\) \\(P(-1.96&lt;X&lt;1.96)=F(1,96)-F(-1.96)=0.9649-0.0351=0.9298\\) 2.11 Distribución Fisher Teorema Sean \\(U\\) y \\(V\\) dos variables aleatorias independientes, con \\(U\\sim \\chi^2(v_1)\\) y \\(V \\sim \\chi^2(v_2)\\). Y sea la variable \\(X\\) definida como: \\[X=\\frac{U/v_1}{V/v_2}\\] Así, decimos que \\(X\\) se distribuye como una Fisher, \\(X\\sim F(v_1,v_2)\\), donde estas \\(v_1\\) y \\(v_2\\) son los grados de libertad de la Fisher. La forma de la distribución \\(f(x)\\) es: \\[ f(x)=\\frac{\\left(\\frac{v_1}{v_2} \\right)^{v_1/2} x^{v_1/2-1}\\Gamma{(\\frac{v_1+v_2}{2})} }{\\Gamma{(\\frac{v_1}{2})} \\Gamma{(\\frac{v_2}{2})}\\left(1+\\frac{v_1}{v_2}x \\right)^{(v_1+v_2)/2}}, \\quad x&gt;0 \\] 2.11.1 Para las varianzas muestrales Suponga que las mnuetsras aleatorias de tamaños \\(n_1\\) y \\(n_2\\) se selecciona de 2 poblaciones normales con varianzas \\(\\sigma^2_1\\) y \\(\\sigma^2_2\\) respectivamente. Sabemos: \\[\\chi^2_1=\\frac{(n_1-1)\\hat{S}_1}{\\sigma^2_1}\\sim \\chi^2(v_1=n_1-1)\\] \\[\\chi^2_2=\\frac{(n_2-1)\\hat{S}_2}{\\sigma^2_2}\\sim \\chi^2(v_2=n_2-1)\\] &gt; Teorema: Si \\(\\hat{S_1}\\) y \\(\\hat{S_2}\\) son los estimadores de la varianza de muestras aleatorias independientes entre ellas de tamaño \\(n_1\\) y \\(n_2\\), tomadas de poblaciones normales con varianzas \\(\\sigma^2_1\\) y \\(\\sigma^2_2\\) entonces: \\[\\frac{\\hat{S_1}/\\sigma^2_1}{\\hat{S_2}/\\sigma^2_2}=\\frac{\\hat{S}_1*\\sigma^2_2}{\\hat{S}_2*\\sigma^2_1}\\sim F(v_1=n_1-1,v_2=n_2-1)\\] curve(df(x,10,10),xlim=c(0,6)) En R #P(X&lt;t)=F(t), donde X ~ F(v1,v2). F(10,10), P(X&lt;1)=F(1) pf(1,10,10) ## [1] 0.5 2.12 Ejercicios La capacidad máxima de un ascensor es de 500 kilos. Si la distribución \\(X\\) de los pesos de los usuarios es \\[X\\sim N(\\mu=70,\\sigma^2=100)\\] Cuál es la probabilidad de que 8 pasajeros sobrepasen ese límite Cuál es la probabilidad de que 7 pasajeros sobrepasen ese límite Cuál es la probabilidad de que 6 pasajeros sobrepasen ese límite Solución, Sean \\(X_1, X_2, \\ldots,X_p\\) con \\(p\\) la cantidad de pasajeros en el ascensor, suponemos que estas \\(X_i\\) son iid \\(X_i\\sim N(\\mu=70,\\sigma=10)\\). Se pide: \\[P(Y=X_1+X_2+\\ldots+X_p&gt;500)\\] Notar que la suma de variables normales es también normal. \\(Y\\sim N(\\mu_y=p*\\mu_x,\\sigma^2_y=p*\\sigma^2_x )\\) \\[E[Y]=E[X_1+\\ldots+X_p]=E[X_1]+\\ldots+E[X_p]=\\mu_x+\\ldots+\\mu_x=p*\\mu_x\\] \\[V(Y)=V(X_1+\\ldots+X_p)=V(X_1)+\\ldots+V(X_p)=\\sigma^2_x+\\ldots+\\sigma^2_x=p*\\sigma^2_x\\] Cuál es la probabilidad de que 8 pasajeros sobrepasen ese límite; \\(Y\\sim N(\\mu_y=8*70=560,\\sigma^2_y=8*100=800)\\) \\[P(Y&gt;500)=P(Z&gt;\\frac{500-560}{\\sqrt{800}})=P(Z&gt;-2.12)=1-P(Z\\leq -2.12)=1-\\phi(-2.12)=\\] \\[=1-0.017=0.983\\] Cuál es la probabilidad de que 7 pasajeros sobrepasen ese límite, \\(Y\\sim N(\\mu_y=490,\\sigma^2_y=700)\\) \\[P(Y&gt;500)=P(Z&gt;0.3779)=1-\\phi(0.3779)=1-0.647=0.353\\] Cuál es la probabilidad de que 6 pasajeros sobrepasen ese límite. (Ejercicio) El viaje en un autobús especial para ir de un campus de una universidad al campus de otra en una ciudad toma, en promedio, 28 minutos, con una desviación estándar de 5 minutos. En cierta semana un autobús hizo el viaje 40 veces. ¿Cuál es la probabilidad de que el tiempo promedio del viaje sea mayor a 30 minutos? Solución, \\(n=40\\), Sea \\(X\\) una va. que explica el tiempo de viaje entre los dos campus. \\(X \\sim .(\\mu_x=28,\\sigma=5)\\), nos pide: \\[P(\\bar{X}&gt;30)\\] Recordar por el teorema del límite central que \\(\\bar{X} \\sim N(\\mu_{\\bar{x}}=28,\\sigma^2_{\\bar{x}}= \\sigma^2_x/n=25/40)\\) cuando \\(n&gt;30\\). \\[P(\\bar{X}&gt;30)=P\\left(\\frac{\\bar{X}-\\mu_x}{\\sigma_x/\\sqrt{n}} &gt;\\frac{30-28}{5/\\sqrt{40}}\\right)=P(Z&gt;2.52)=1-P(Z\\leq 2.52)\\approx 1-\\phi(2.52)=\\] \\[=1-0.9941=0.0059\\] La calificación promedio de los estudiantes de primer año en un examen de aptitudes en cierta universidad es 540, con una desviación estándar de 50. Suponga que las medias se miden con cualquier grado de precisión. ¿Cuál es la probabilidad de que dos grupos seleccionados al azar, que constan de 32 y 50 estudiantes, respectivamente, difieran en sus calificaciones promedio por: más de 20 puntos? una cantidad entre 5 y 10 puntos? _ Solución, como información se tienen 2 muestras, \\(n_1=32\\), \\(n_2=50\\). Sea \\(X\\sim . (\\mu=540,\\sigma=50)\\), nos piden analizar la diferencia de las medias de los dos grupos, \\(\\bar{X_1}\\), \\(\\bar{X}_2\\): más de 20 puntos? \\[P( |\\bar{X_1}-\\bar{X}_2| &gt;20)=1-P( |\\bar{X_1}-\\bar{X}_2| \\leq 20)=1-P( -20\\leq \\bar{X_1}-\\bar{X}_2 \\leq 20)\\] Para la diferencia de medias \\(\\bar{X_1}-\\bar{X}_2\\sim N(\\mu_{\\bar{X_1}-\\bar{X}_2}=\\mu_x-\\mu_x=0,\\sigma^2_{\\bar{X_1}-\\bar{X}_2}=\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}=50^2/32+50^2/50)\\), entonces, \\(Y=\\bar{X_1}-\\bar{X}_2\\sim N(\\mu_y=0,\\sigma^2_y=128.125)\\), bajo el supuesto que las \\(\\bar{X}_1\\) y \\(\\bar{X}_2\\) tienden a ser normales por el teorema del límite central. \\[P( -20\\leq \\bar{X_1}-\\bar{X}_2 \\leq 20)=P(-20/11.31\\leq Z \\leq 20/11.31)\\approx \\phi(1.77)-\\phi(-1.77)=\\] \\[=0.962-0.038=0.9232\\] \\[P( |\\bar{X_1}-\\bar{X}_2| &gt;20)=1-0.9232=0.0768\\] una cantidad entre 5 y 10 puntos? (ejercicio) Suponga que las varianzas muestrales son mediciones continuas. Calcule la probabilidad de que una muestra aleatoria de 25 observaciones, de una población normal con varianza \\(\\sigma^2 = 6\\), tenga una varianza muestral \\(\\hat{S}^2\\) mayor que 9.1 entre 3.462 y 10.745. \\[P(\\hat{S}^2&gt;9.1)=P\\left(\\frac{(n-1)\\hat{S^2}}{\\sigma^2_x}&gt; 24*9.1/6 \\right)=P(\\chi^2&gt;36.4)=1-P(\\chi^2\\leq36.4)=\\] \\[=1-0.9498=0.0502\\] Teniendo en cuenta que \\(\\chi^2\\sim \\chi^2(v=24)\\) Un fabricante de cierta marca de barras de cereal con bajo contenido de grasa afirma que el contenido promedio de grasa saturada en éstas es de 0.5 gramos. En una muestra aleatoria de 8 barras de cereal de esta marca se encontró que su contenido de grasa saturada era de 0.6, 0.7, 0.7, 0.3, 0.4, 0.5, 0.4 y 0.2. ¿Estaría de acuerdo con tal afirmación? Suponga una distribución normal. (\\(t\\)) 2.12.1 Ejercicios del examen CI=1 # P(3&lt;x&lt;19)=F(19)-F(3) v=16 pchisq(19,16)-pchisq(3,16) ## [1] 0.7311673 CI=1 \\[P(X&gt;0.68)=1-P(X&lt;0.68)=1-F(0.68)\\] #v=22 1-pt(0.68,22) ## [1] 0.2517989 \\(X\\sim F(v_1,v_2)\\) \\[P(X=1.36)=0\\] curve(dnorm(x),xlim=c(-3,3)) "],["estimaciones-de-una-y-dos-muestrales.html", "3 Estimaciones de una y dos muestrales 3.1 Inferencia estadística 3.2 Estimadores puntuales 3.3 Estimación por intervalos de confianza", " 3 Estimaciones de una y dos muestrales 3.1 Inferencia estadística El proceso por el cual, mediante una muestra estadísticamente seleccionada se busca describir a la población/universo de la cual esta proviene. Podemos clasificar a la inferencia estadística en: Inferencia descriptiva: Tiene el único objetivo de describir a la población mediante la muestra, tradicionalmente se enfoca en estimaciones comúnes como; la media, la varianza, total, un porcentaje, diferencia de medias, diferencia de proporciones. Estimación puntual: \\(\\hat{\\theta}\\) Estimación por intervalos: \\([ \\hat{\\theta}_{LI},\\hat{\\theta}_{LS}]\\) Pruebas de hipótesis: \\(\\hat{\\theta}=k\\), \\(\\hat{\\theta}&gt;k\\), \\(\\hat{\\theta}&lt;k\\) Tamaño de la muestra (\\(n\\)): \\(n=f(U,V(\\hat{\\theta}),\\hat{\\theta},\\ldots)\\) Inferencia predictiva: Tiene una idea de estudiar; por un lado, la evolución de las estimaciones y sus posibles valores futuros (series de tiempo), por otro lado, le interesa conocer las relaciones (no causales) entre las variables. Series de tiempo Modelos lineales Técnicas multivariantes etc. Inferencia causal: Tiene el objetivo de medir la relación causal entre variables. \\(X \\rightarrow Y\\) Diseños experimentales Diseños cuasi-experimentales Modelos estructurales Etc. Tarea: Indagar a que se refiere la inferencia bayesiana. 3.2 Estimadores puntuales Recordemos que tenemos un universo \\(U\\) de tamaño \\(N\\). \\[U=\\{u_1, u_2,\\ldots,u_N\\}\\] Donde cada unidad del universo tiene variables (características) asociadas, pensemos en \\(p\\) características. \\[u_i=\\{X_{i1},X_{i2}, \\ldots , X_{ip}\\}\\] Un párametro es una función sobre el universo y sus variables, lo denotamos por \\(\\theta\\) \\[\\theta=f(U,X)\\] Un estimador se construye a partir de la definición de una estadística (\\(\\Theta\\)) y tiene el objetivo de aproximar de la mejor forma a un parámetro. \\(\\hat{\\theta}\\rightarrow \\theta\\). El estimador \\(\\hat{\\theta}\\) se construye a partir de una muestra aleatoria (\\(s\\)) de tamaño \\(n\\) obtenida de \\(U\\). Nota: Para un parámetro \\(\\theta\\), pueden existir muchos estimadores candidatos: \\(\\hat{\\theta}_1,\\hat{\\theta}_2, \\hat{\\theta}_3,...\\), la pregunta es ¿Cuál es mejor?. Existen al menos dos criterios: 3.2.1 Estimador insesgado \\[E[\\hat{\\theta}]=\\theta\\] 3.2.2 Estimador eficiente Supongamos que tenemos dos estimadores para \\(\\theta\\), \\(\\hat{\\theta}_1\\), \\(\\hat{\\theta}_2\\), el estimador más eficiente entre los dos será quien tenga el valor más pequeño en su varianza. \\(min(V(\\hat{\\theta_1}),V(\\hat{\\theta_2})) \\rightarrow \\hat{\\theta}\\)$ Ejemplo. Sea el vector \\(X=\\{10,10,20,25,30\\}\\) de una población con \\(N=5\\), se define una muestra de \\(n=3\\) y se busca estimar la media de \\(X\\): \\(\\mu_x\\). A partir de los estimadores de la media y la mediana muestral. Determinar: Son estimadores insesgados Cuál estimador es más eficiente Suponga un muestreo sin reposición. Solución. x&lt;-c(10,10,20,25,30) choose(5,3) ## [1] 10 s&lt;-combn(x,3) #distribución muestral de la media muestral dmedia&lt;-apply(s,2,mean) #distribución muestral de la mediana muestral dmediana&lt;-apply(s,2,median) # Son estimadores insesgados #media sum(dmedia*(1/10)) # E[] ## [1] 19 mean(dmedia) ## [1] 19 #media sum(dmediana*(1/10)) # E[] ## [1] 18.5 mean(dmediana) ## [1] 18.5 #parámetro media poblacional mean(x) ## [1] 19 # Cuál estimador es más eficiente sum((dmedia-mean(dmedia))^2*1/10) # media muestral ## [1] 10.66667 sum((dmediana-mean(dmediana))^2*1/10) # mediana muestral ## [1] 35.25 Para este ejercicio, la media muestral es insesgado y más eficiente que la mediana muestral. Nota Los principales problemas de estimación ocurren con frecuencia para estimar: El promedio o media de una población \\(\\mu\\) \\[\\mu_X=\\frac{\\sum_U X_i}{N}\\] La varianza poblacional \\(\\sigma^2\\) \\[\\sigma^2=\\frac{\\sum_U (X_i-\\mu_X)^2}{N}\\] La proporción de una característica en la población \\(P\\) \\[P_A=\\frac{\\#A}{N}=\\frac{\\sum_{U} X_i}{N}; \\quad \\{X_i=1, \\quad i \\in A, X_i=0,\\quad eoc\\}\\] La diferencia de medias de dos poblaciones \\(\\mu_1-\\mu_2\\) La diferencia de proporciones de dos poblaciones \\(P_1-P_2\\) Estimaciones puntuales razonables de estos parámetros son las siguientes: Para \\(\\mu\\), la estimación es \\(\\hat{\\mu_x}=\\bar{X}\\) la media muestral \\[\\bar{X}=\\frac{\\sum_s X_i}{n}\\] * Para \\(\\sigma^2\\), la estimación es \\(\\hat{\\sigma}^2=\\hat{S^2}\\), la varianza muestral \\[\\hat{S}^2=\\frac{\\sum_s (X_i-\\bar{X})^2}{n-1}\\] * Para \\(P\\), la estimación es \\(\\hat{P}\\), la proporción muestral \\[\\hat{P}_A=\\frac{\\#_sA}{N}=\\frac{\\sum_{s} X_i}{N}; \\quad \\{X_i=1, \\quad i \\in A, X_i=0,\\quad eoc\\}\\] * Para \\(\\mu_1-\\mu_2\\), la estimación es \\(\\hat{\\mu_1}-\\hat{\\mu}_2=\\bar{X}_1-\\bar{X}_2\\), la diferencia entre las medias de las muestras de dos muestras aleatorias independientes. * Para \\(P_1-P_2\\), la estimación es \\(\\hat{P_1}-\\hat{P}_2\\), la diferencia entre las proporciones de las muestras de dos muestras aleatorias independientes. Ejercicio, Suponga que \\(X\\) es una variable aleatoria con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Sea \\(X_1, X_2, \\ldots, X_n\\) una muestra aleatoria de tamaño \\(n\\) de \\(X\\). DEMOSTRAR que la media de muestra \\(\\bar{X}\\) y la varianza muestral \\(\\hat{S}^2\\) son estimadores insesgados de \\(\\mu\\) y \\(\\sigma^2\\), respectivamente. Como información; \\(E[X]=\\mu\\), \\[V(X)=\\sigma^2=E[X^2]-E[X]^2\\]. También recordar que; \\[V(\\bar{X})=\\frac{\\sigma^2}{n}=E[\\bar{X}^2]-E[\\bar{X}]^2=E[\\bar{X}^2]-\\mu^2\\] Solución, \\[E[\\bar{X}]=E\\left[\\frac{\\sum_s X_i}{n} \\right]=\\frac{1}{n}\\left(\\sum_s E[X_i]\\right)=\\frac{1}{n}\\left(\\sum_{i=1}^n \\mu\\right)=\\mu \\] \\[E[\\hat{S^2}]=E\\left[\\frac{\\sum_s (X_i-\\bar{X})^2}{n-1} \\right]=\\frac{1}{n-1} E\\left[\\sum_s (X_i^2-2X_i \\bar{X}+\\bar{X}^2) \\right]=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2\\bar{X} \\sum_sX_i +n\\bar{X}^2 \\right]= \\] \\[=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2\\bar{X}n\\frac{\\sum_s X_i}{n} +n\\bar{X}^2 \\right]=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-2n\\bar{X}^2 +n\\bar{X}^2 \\right]=\\frac{1}{n-1}E\\left[\\sum_s X_i^2-n\\bar{X}^2\\right]=\\] \\[=\\frac{1}{n-1}\\left(\\sum_s E[X_i^2]-nE[\\bar{X}^2] \\right)= \\alpha\\] Notar \\(\\sigma^2=E[X^2]-E[X]^2=E[X^2]-\\mu^2\\) para un \\(X_i\\), \\(\\sigma^2=E[X_i^2]-E[X_i]^2=E[X_i^2]-\\mu^2\\), entonces, \\(E[X_i]=\\sigma^2+\\mu^2\\). Por otro lado \\(E[\\bar{X}^2]=\\frac{\\sigma^2}{n}+\\mu^2\\), Así: \\[\\alpha=\\frac{1}{n-1}\\left[\\sum_s (\\sigma^2+\\mu) -n \\left(\\frac{\\sigma^2}{n}+\\mu \\right) \\right]=\\frac{1}{n-1}\\left[ n \\sigma^2+n\\mu -\\sigma^2-n\\mu \\right]=\\frac{\\sigma^2(n-1)}{n-1}=\\sigma^2\\] 3.2.3 Error cuadrático medio (ECM) Este se define para un estimador como: \\[ECM(\\hat{\\theta})=E\\left[(\\hat{\\theta}-\\theta)^2\\right]\\] Recordar que \\(V(\\hat{\\theta})=E\\left[(\\hat{\\theta}-E[\\hat{\\theta}])^2\\right]\\). \\[ECM(\\hat{\\theta})=E\\left[\\left[(\\hat{\\theta}-E[\\hat{\\theta}])-(\\theta-E[\\hat\\theta]) \\right]^2\\right]=E\\left[(\\hat{\\theta}-E[\\hat{\\theta}])^2-2(\\hat{\\theta}-E[\\hat{\\theta}])(\\theta-E[\\hat\\theta])+ (\\theta-E[\\hat\\theta])^2 \\right]=\\] \\[=E[(\\hat{\\theta}-E[\\hat{\\theta}])^2]-2(\\theta-E[\\hat\\theta])E\\left[\\hat{\\theta}-E[\\hat{\\theta}]\\right]+E[(\\theta-E[\\hat\\theta])^2]=V(\\hat\\theta)=V(\\hat{\\theta})+E[(\\theta-E[\\hat\\theta])^2]=\\] \\[=V(\\hat{\\theta})+sesgo(\\hat\\theta)^2\\] 3.2.4 Cota de Cramer Rao Es posible obtener una cota inferior de la varianza de todos los estimadores (\\(\\hat{\\theta}_1, \\hat{\\theta}_2,\\ldots\\)) insesgados de \\(\\theta\\). Sea \\(\\hat{\\theta}\\) un estimador insesgado del parámetro \\(\\theta\\), basado en una muestra aleatorio de \\(n\\) observaciones, y considérese que \\(f(x,\\theta)\\) denota la función de distribución de probabilidades de una variable aleatoria \\(X\\). Entonces una cota inferior en la varianza de \\(\\hat{\\theta}\\) es: \\[V(\\hat{\\theta})\\geq\\frac{1}{nE\\left\\{ \\left[\\frac{d}{d\\theta }ln f(X,\\theta) \\right]^2 \\right\\}}\\] Esta desigualdad se denomina cota de Cramer Rao. Si un estimador insesgado \\(\\hat{\\theta}\\) satisface la desigualdad, se tratará del estimador insesgado de varianza mínima de \\(\\theta\\). Ejemplo, Demostrar que la media muestra \\(\\bar{X}\\) es el estimador insesgado de varianza mínima de la media de una distribución normal con varianza conocida. Sea \\(X\\sim N(\\mu,\\sigma^2)\\), sabemos \\(E[\\bar{X}]=\\mu\\) \\[f(X,\\mu)=\\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2}\\] \\[ln f(X,\\mu)=ln\\left( \\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2} \\right)=-ln\\left( \\sqrt{2\\pi} \\sigma \\right) -\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^2\\] \\[E\\left\\{\\left[ \\frac{d}{d\\mu} ln f(X,\\mu)\\right]^2 \\right\\}=E\\left\\{ \\left[ \\frac{(X-\\mu)}{\\sigma^2} \\right]^2\\right\\} =E\\left[\\frac{(X-\\mu)^2}{\\sigma^4} \\right]=\\frac{E[(X-\\mu)^2]}{\\sigma^4}=\\] \\[=\\frac{\\sigma^2}{\\sigma^4}=\\frac{1}{\\sigma^2}\\] Finalmente, para la cota de Cramer-Rao \\[V(\\bar{X})\\geq \\frac{1}{\\frac{n}{\\sigma^2}}=\\frac{\\sigma^2}{n}=V(\\bar{X})\\] 3.2.5 Método de Maxima verosimilitud Suponga que \\(X\\) es una va, con distribución \\(f(X,\\theta)\\), donde \\(\\theta\\) es un parámetro desconocido. Sean \\(X_1, X_2,\\ldots, X_n\\) va. iid. como \\(X\\), la muestra de tamaño \\(n\\). La función de probabilidad de las \\(n\\) va. se escribe como: \\[f(X_1,X_2, \\ldots,X_n,\\theta)=f(X_1,\\theta)*f(X_2,\\theta)*\\ldots*f(X_n,\\theta)=L(\\theta)\\] El estimador de máxima verosimilitud de \\(\\theta\\) es el valor que maximiza la función de probabilidad \\(L(\\theta)\\). Pasos para obtener el estimador de máxima verosimilitud para un parámetro \\(\\theta\\) Obtener \\(L(\\theta)\\) Calcular \\(ln [L(\\theta)]\\) Resolver la ecuación: \\[\\frac{d}{d\\theta} ln [L(\\theta)]=0\\] En el caso de que tengamos más de un parámetro, los pasos son: Obtener \\(L(\\theta_1,\\theta_2,\\ldots)=f(X_1,\\theta_1,\\theta_2,\\ldots)*\\ldots*f(X_n,\\theta_1,\\theta_2,\\ldots)\\) Calcular \\(ln [L(\\theta_1,\\theta_2,\\ldots)]\\) Resolver el sistema de ecuaciones: \\[\\frac{\\partial }{\\partial \\theta_1} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] \\[\\frac{\\partial }{\\partial \\theta_2} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] \\[\\frac{\\partial }{\\partial \\theta_p} ln [L(\\theta_1,\\theta_2,\\ldots)]=0\\] Ejemplo, Sea \\(X\\sim Bernoulli(p)\\), la función de probabilidad es: \\[P(X=x)=\\pi(x)=p^x (1-p)^{1-x} \\quad ; x=\\{0,1\\}\\] Si \\(p\\) es el parámetro de interés que se busca estimar, ¿qué forma tendrá el estimador de máxima verosimilitud? Solución, Supongamos que se extrae una muestra de tamaño \\(n\\), así: \\[L(p)=f(X_1,p)*f(X_2,p)*\\ldots*f(X_n,p)=p^{x_1} (1-p)^{1-x_1}*p^{x_2} (1-p)^{1-x_2}*\\ldots*p^{x_n} (1-p)^{1-x_n}\\] \\[L(p)=p^{\\sum_{i=1}^n x_i}*(1-p)^{n-\\sum_{i=1}^n x_i}\\] \\[ln[ L(p)]= \\sum_{i=1}^n x_i ln(p)+\\left(n-\\sum_{i=1}^n x_i \\right) ln(1-p) \\] \\[\\frac{d}{dp}ln[ L(p)]= \\frac{\\sum_{i=1}^n x_i}{p}-\\frac{\\left(n-\\sum_{i=1}^n x_i\\right)}{1-p}=0\\] \\[ \\frac{\\sum_{i=1}^n x_i}{p}-\\frac{\\left(n-\\sum_{i=1}^n x_i\\right)}{1-p}=0\\] \\[\\hat{p}_{mv}=\\frac{\\sum_{i=1}^n x_i}{n}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim Poisson(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de máxima verosimilitud. Solución, recordar que si \\(X\\sim Poisson(\\lambda)\\) \\[\\pi(x)=P(X=x)=\\frac{e^{-\\lambda} \\lambda ^x}{x!}; \\quad X=\\{0,1,2\\ldots\\}\\] \\[L(\\lambda)=\\pi(X_1,\\lambda)*\\pi(X_2,\\lambda)*\\ldots*\\pi(X_n,\\lambda)\\] \\[L(\\lambda)=\\frac{e^{-\\lambda} \\lambda ^{x_1}}{x_1!}*\\frac{e^{-\\lambda} \\lambda ^{x_2}}{x_2!}*\\ldots*\\frac{e^{-\\lambda} \\lambda ^{x_n}}{x_n!}\\] \\[L(\\lambda)=\\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda ^{x_i}}{x_i!}=\\frac{e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n x_i}}{\\prod_{i=1}^n x_i!}\\] \\[ln [L(\\lambda)]=-n\\lambda+\\sum_{i=1}^n x_i ln \\lambda - ln \\prod_{i=1}^n x_i!\\] \\[\\frac{d}{d\\lambda}ln [L(\\lambda)]=-n+\\frac{\\sum_{i=1}^n x_i}{\\lambda}=0\\] \\[\\hat{\\lambda}=\\frac{\\sum_{i=1}^n x_i}{n}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim exp(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de máxima verosimilitud. Solución, recordar que si \\(X\\sim exp(\\lambda)\\) su función de densidad es dada por: \\[f(x)=\\lambda e^{-\\lambda x}; \\quad x\\geq0\\] \\[L(\\lambda)=\\prod_{i=1}^n \\lambda e^{-\\lambda x_i}=\\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}\\] \\[ln [L(\\lambda)]=n ln \\lambda-\\lambda \\sum_{i=1}^n x_i\\] \\[\\frac{d}{d\\lambda}ln [L(\\lambda)]=\\frac{n}{\\lambda}-\\sum_{i=1}^n x_i=0\\] \\[\\hat{\\lambda}=\\frac{1}{\\frac{\\sum_{i=1}^n x_i}{n}}=\\frac{1}{\\bar{X}}\\] Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim N(\\mu,\\sigma^2)\\) ambos parámetros desconocidos. Encontrar los estimadores de máxima verosimilitud para \\(\\mu\\) y \\(\\sigma^2\\). Solución, recordar si \\(X\\sim N(\\mu, \\sigma^2)\\) su función de densidad es dada por: \\[f(X)=\\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{1/2} }e^{-\\frac{1}{2}\\frac{\\left(x-\\mu\\right)^2}{\\sigma^2}}\\] \\[L(\\mu,\\sigma^2)=\\prod_{i=1}^n \\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{1/2} }e^{-\\frac{1}{2}\\frac{\\left(x_i-\\mu\\right)^2}{\\sigma^2}}=\\frac{1}{\\left(2\\pi \\sigma^2 \\right)^{n/2} }e^{-\\frac{1}{2 \\sigma^2}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2}\\] \\[ln[L(\\mu,\\sigma^2)]=-\\frac{n}{2}ln (2\\pi \\sigma^2)-\\frac{1}{2 \\sigma^2}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2\\] \\[\\frac{\\partial}{\\partial \\mu} ln[L(\\mu,\\sigma^2)]=\\frac{1}{\\sigma^2} \\sum_{i=1}^n \\left(x_i-\\mu\\right)=0\\] \\[\\frac{\\partial}{\\partial \\sigma^2} ln[L(\\mu,\\sigma^2)]=-\\frac{n}{2 \\sigma^2}+\\frac{1}{2 \\sigma^4}\\sum_{i=1}^n \\left(x_i-\\mu\\right)^2=0\\] \\[\\sum_{i=1}^n x_i - n\\mu=0\\] \\[\\hat{\\mu}=\\frac{\\sum_{i=1}^n x_i}{n}=\\bar{X}\\] \\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^n \\left(x_i-\\bar{X}\\right)^2}{n}\\] 3.2.6 Método de momentos Este método fue desarrollado por 1894 por Pearson, a diferencia del método de máxima verosimilitud que fue ampliamente utilizado por Fisher a partir 1912. Recordar que para una variable aleatoria, los momentos respecto el origen son: Primer Momento: \\(\\mu_1=E[X]=\\int x f(x) dx\\) Segundo Momento: \\(\\mu_2=E[X^2]=\\int x^2 f(x) dx\\) k-ésimo momento: \\(\\mu_k=E[X^k]\\) Sea \\(X_1, X_2, \\ldots ,X_n\\) una muestra aleatorio de tamaño \\(n\\) de una va \\(X\\), definamos los primeros \\(k\\) momentos de la muestra respecto al origen como: Primer momento: \\[m_1=\\frac{\\sum_{i=1}^n x_i}{n}\\] Segundo momento: \\[m_2=\\frac{\\sum_{i=1}^n x^2_i}{n}\\] k-ésimo momento: \\[m_k=\\frac{\\sum_{i=1}^n x^k_i}{n}; \\quad k=1,2,\\ldots \\] Los momentos \\(\\mu_k\\) de la población serán funciones de los parámetros desconocidos \\(\\theta\\). Al igualar estos momentos muestrales con los poblaciones vamos a poder construir un sistema de ecuaciones de cuantas incógnitas se defina con la distribución de \\(X\\) Ejemplo, Sea \\(X_1, X_2, \\ldots,X_n\\), va iid, tal que \\(X_i\\sim exp(\\lambda)\\). Encontrar el estimador de \\(\\lambda\\) empleando el método de momentos. Solución, recordar que si \\(X\\sim exp(\\lambda)\\) su función de densidad es dada por: \\[f(x)=\\lambda e^{-\\lambda x}; \\quad x\\geq0\\] \\[E[X]=m_1\\] \\[E[X]=\\int_0^\\infty x\\lambda e^{-\\lambda x}dx=\\frac{1}{\\lambda}\\] Igualando los momentos: \\[\\frac{1}{\\lambda}=\\frac{\\sum_s x_i}{n}\\] \\[\\hat{\\lambda}= \\frac{1}{\\frac{\\sum_s x_i}{n}}=\\frac{1}{\\bar{X}}\\] Ejercicio, Sea \\(X\\) una va geométrica con parámetro \\(p\\), encuentre un estimador de \\(p\\) mediante el método de momentos y el método de máxima verosimilitud. En base a una muestra aleatoria de tamaño \\(n\\) Recordar que si \\(X\\sim G(p)\\), entonces su distribución de probabilidades es: \\[\\pi(x)=P(X=x)=(1-p)^x p; \\quad x=\\{0,1,2,\\dots\\} \\] Recordar que \\(E[X]=\\frac{1-p}{p}\\), por el método de momentos: \\[\\frac{1-p}{p}=\\bar{X}\\] \\[\\hat{p}=\\frac{1}{\\bar{X}+1}\\] Por el método de máxima verosimilitud. \\[L(p)=\\prod_{i=1}^n (1-p)^{x_i} p=(1-p)^{\\sum_s x_i} p^n\\] \\[ln [L(p)]=\\sum_s x_i ln (1-p)+n ln(p)\\] \\[\\frac{d}{dp}ln [L(p)]=-\\frac{\\sum_s x_i}{1-p}+\\frac{n}{p}=0\\] \\[\\hat{p}=\\frac{1}{\\bar{X}+1}\\] Tarea, Sea \\(X\\) una normal con parámetro \\(\\mu\\), \\(\\sigma^2\\), encuentre un estimador de \\(\\mu\\) y \\(\\sigma^2\\) mediante el método de momentos. En base a una muestra aleatoria de tamaño \\(n\\) \\[\\mu=\\bar{X}\\] \\[E[X^2]=\\frac{\\sum_s x_i^2}{n}\\] Recordar que \\(\\sigma^2=E[X^2]-E[X]^2=E[X^2]-\\mu^2 \\approx E[X^2]-\\bar{X}^2\\). 3.3 Estimación por intervalos de confianza Para construir un intervalo de confianza del parámetro desconocido \\(\\theta\\), se debe encontrar dos estadísticas \\(L\\) y \\(U\\) tales que: \\[P(L\\leq\\theta\\leq U)=1-\\alpha\\] El intervalo \\(L\\leq\\theta\\leq U\\) se llama intervalo de confianza del \\(100*(1-\\alpha)\\). A \\(L\\) se lo conoce como límite inferior y \\(U\\) como límite superior. La interpretación del intervalo de confianza es que si se coleccionan muchas muestras aleatorias y se calcula un intervalo de confianza del \\(100*(1-\\alpha)\\) por ciento en \\(\\theta\\) de cada muestra, entonces \\(100*(1-\\alpha)\\) por ciento de estos intervalos contendrán el verdadero valor de \\(\\theta\\). 3.3.1 Intervalo de confianza para la media, asumiendo varianza conocida. Sea \\(X\\) una va con media desconocida \\(\\mu\\) y varianza conocida \\(\\sigma^2\\). Suponga que se toma una muestra aleatoria de tamaño \\(n\\), \\(X_1,X_2,\\ldots,X_n\\). Puede obtenerse un intervalo de confianza del \\(100*(1-\\alpha)\\) por ciento en \\(\\mu\\) considerando la distribución de muestreo de \\(X\\) de la media muestral \\(\\bar{X}\\). Por el teorema del límite central sabemos que \\(\\bar{X}\\sim N(\\mu,\\frac{\\sigma^2}{n})\\) bajo ciertas condiciones. Así: \\[Z=\\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\] Teniendo a \\(Z\\sim N(0,1)\\), para armar el intervalo de confianza basta con trabajar sobre: \\[P(L\\leq Z\\leq U)=1-\\alpha\\] Para un intervalo de confianza \\(L \\leq \\theta \\le U\\) se debe asegurar que la precisión de los lados sea la misma, \\(\\theta-L=U-\\theta\\). curve(dnorm(x),xlim=c(-3.5,3.5),xlab=&quot;z&quot;) abline(v=0) sx&lt;-c(-3.5,seq(-3.5,qnorm(0.025),0.01),qnorm(0.025)) sy&lt;-c(0,dnorm(seq(-3.5,qnorm(0.025),0.01)),0) polygon(sx,sy,col=&quot;red&quot;) polygon(-1*sx,sy,col=&quot;red&quot;) qnorm(0.05/2) ## [1] -1.959964 Si \\(\\alpha=0.05\\) \\[P(L\\leq Z\\leq U)=0.95\\] \\[P(-Z_{\\alpha/2}\\leq Z\\leq Z_{\\alpha/2})=1-\\alpha\\] \\[P\\left(-Z_{\\alpha/2}\\leq \\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq Z_{\\alpha/2}\\right)=1-\\alpha\\] \\[P\\left(\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\leq\\mu \\leq \\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha\\] Así de esta manera tenemos identificados a \\(L\\) y \\(U\\) para \\(\\mu\\) con varianza conocida. \\[L=\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] \\[U=\\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] \\(\\alpha\\) es conocida como el nivel de significancia y \\(1-\\alpha\\) como la confiabilidad. Los valores más usuales de \\(\\alpha\\) son 0.01, 0.05 y 0.1, para estudios sobre ciencias de la salud el valor recomendado es de 0.01 o menor, para las ciencias sociales y económicas el valor recomendado es 0.05. Para la distribución normal los valores de \\(Z_{\\alpha/2}\\) son: \\(\\alpha=0.1\\), \\(Z_{\\alpha/2}=Z_{0.05}=1.64\\) (90% confiabilidad) \\(\\alpha=0.05\\), \\(Z_{\\alpha/2}=Z_{0.025}=1.96\\) (95% confiabilidad) \\(\\alpha=0.01\\), \\(Z_{\\alpha/2}=Z_{0.005}=2.58\\) (99% confiabilidad) Ejercicio, Se sabe que la vida en horas de una bombilla eléctrica de 75 watts se distribuye aproximadamente normal, con \\(\\sigma=25\\) horas. Una muestra aleatoria de 20 bombillas tiene una vida media de \\(\\bar{X}=1014\\) horas. Encontrarlos intervalos de confianza para 90, 95 y 99 % de confiabilidad Solución, como información \\(n=20\\), para elaborar los intervalos: \\[\\bar{X}-Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\leq\\mu \\leq \\bar{X}+Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] Al 90% \\[1014-1.64 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+1.64 \\frac{25}{\\sqrt{20}}\\] \\[1004.832 \\leq \\mu \\leq 1023.168\\] Al 95% \\[1014-1.96 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+1.96 \\frac{25}{\\sqrt{20}}\\] \\[1003.043 \\leq \\mu \\leq 1024.957\\] Al 99% \\[1014-2.58 \\frac{25}{\\sqrt{20}}\\leq\\mu \\leq 1014+2.58 \\frac{25}{\\sqrt{20}}\\] \\[999.5774 \\leq \\mu \\leq 1028.423\\] 3.3.1.1 Tamaño de muestra Definamos al margen de error absoluto como: \\[\\epsilon=Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\] Notar que es posible despejar \\(n\\) y esto permitirá tener una formula para definir un tamaño de muestra condicionado a: el margen de error (\\(\\epsilon\\)), desviación de los datos (\\(\\sigma\\)) y el nivel de confiabilidad (\\(Z_{\\alpha/2}\\)) \\[n=\\frac{Z_{\\alpha/2}^2*\\sigma^2}{\\epsilon^2}=\\left(\\frac{Z_{\\alpha/2}*\\sigma}{\\epsilon} \\right)^2\\] Nota: Esta formula se puede utilizar en la medida que la muestra que se seleccione sea aleatoria simple, se refiere a la selección de unidades simples. Ejemplo, se busca conocer el tiempo promedio en horas/día que pasan los estudiantes de informática de la UMSA en la computadora, para ello se planea realizar una encuesta aleatoria, que logre un 95% de confiabilidad y tenga un margen de error de 0.8 horas. Definir el tamaño de muestra necesario. Solución, como información se tiene: \\(\\epsilon=0.8\\), \\(Z_{\\alpha/2}=1.96\\), para el valor de \\(\\sigma\\) para el calculo del tamaño de muestra se realizó una piloto en la materia de estadística II a 10 estudiantes y el resultado fue \\(\\sigma=3.335\\). Así: \\[n=\\left(\\frac{1.96*3.335}{0.8} \\right)^2=66.8\\approx 67\\] ### Intervalo de confianza sobre la diferencia de dos medias, conocida la varianza Tenemos dos va independientes, \\(X_1\\) con media \\(\\mu_1\\) desconocida y varianza \\(\\sigma^2_1\\) conocida y \\(X_2\\) con media \\(\\mu_2\\) desconocida y varianza \\(\\sigma^2_2\\) conocida. El objetivo es encontrar un intervalo para \\(\\mu_1-\\mu2\\). Sea dos muestras aleatorias recopíladas para ambas va, de tal forma que \\(n_1\\) representa el tamaño de muestra para \\(X_1\\) y \\(n_2\\) para \\(X_2\\). Recordar por el teorema del limite central, esta diferencia de medias puede ser estimada por sus medias muestrales y además se aproxima a una normal, tal que: \\[\\bar{X}_1-\\bar{X}_2 \\sim N\\left(\\mu_{\\bar{X}_1-\\bar{X}_2}=\\mu_1-\\mu_2,\\sigma^2_{\\bar{X}_1-\\bar{X}_2}=\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}\\right)\\] Ahora, \\[Z=\\frac{\\bar{X}_1-\\bar{X}_2-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}}\\] Dado que \\(Z\\sim N(0,1)\\), ahora lo que queda es trabajar sobre: \\[P(-Z_{\\alpha/2} \\leq Z \\leq Z_{\\alpha/2})=1-\\alpha\\] Así, el limite inferior y superior esta dado por: \\[L=\\bar{X}_1-\\bar{X}_2-Z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\] \\[U=\\bar{X}_1-\\bar{X}_2+Z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\] Notar que en general, dado un estimador \\(\\hat{\\theta}\\) para el parámetro \\(\\theta\\), su usamos el teorema del limite central su intervalo de confianza estará dado por: \\[IC(\\theta): \\quad \\hat{\\theta} \\pm Z_{\\alpha/2} \\sqrt{V(\\hat{\\theta})} \\] Ejercicio: Se lleva a cabo pruebas de resistencia a la tensión sobre diferentes clases de largueros de aluminio utilizados en la fabricación de alas de aeroplanos comerciales. De la experiencia pasada con el proceso de fabricación de largueros y del procedimiento de prueba, se supone que las desviaciones estándar de las resistencias a la tensión son conocidas, Los datos obtenidos son: Clase de larguero 1: \\(n_1=18\\), \\(\\bar{X}_1=85.9\\), \\(\\sigma_1=1\\) Clase de larguero 2: \\(n_2=16\\), \\(\\bar{X}_2=73.3\\), \\(\\sigma_2=1.5\\) Si \\(\\mu_1\\) y \\(\\mu_2\\) son las verdaderas resistencias a la tensión de ambas clases de largueros. Encuentre intervalos de confianza al 90% y 95% de confiabilidad para la diferencia de estas medias. Solución, para el 90% de confiabilidad el intervalo esta dado por: \\[IC_{90\\%}(\\mu_1-\\mu_2): 85.9-73.3 \\pm 1.64*\\sqrt{\\frac{1^2}{18}+\\frac{1.5^2}{16}}=12.6 \\pm 0.73: [11.87\\quad 13.33]\\] \\[IC_{90\\%}(\\mu_1-\\mu_2): 85.9-73.3 \\pm 1.96*\\sqrt{\\frac{1^2}{18}+\\frac{1.5^2}{16}}=12.6 \\pm 0.87: [11.73\\quad 13.47]\\] 3.3.2 Intervalo de confianza para la media y la diferencia de medias con varianza desconocida pero muestra mayor a 30. Para los 2 anteriores intervalos definidos se suponía que la varianza es conocida, cuando no sucede esto la mejor alternativa para reemplazar a \\(\\sigma^2\\) es usando \\(\\hat{S}^2\\), esto siempre y cuando el tamaño de muestra sea grande (\\(n&gt;30\\)), para el caso de la diferencia de medias, ambos tamaños de muestra deben ser mayores a 30 \\((n_1,n_2&gt;30)\\). Esto debido al teorema del limite central. Para la media \\[IC_{100(1-\\alpha)}(\\mu): \\bar{X}\\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat{S}^2}{n}}\\] Para la diferencia de medias \\[IC_{100(1-\\alpha)}(\\mu_1-\\mu_2): \\bar{X}_1-\\bar{X}_2 \\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat{S_1}^2}{n_1}+\\frac{\\hat{S_2}^2}{n_2}}\\] Ejercicio, se tiene el dato de una muestra aleatoria de 40 personas, respecto su edad. Construya el intervalo de confianza al 95% de confiabilidad. Los datos son: set.seed(1534) x&lt;-round(runif(40,19,29)) x ## [1] 20 26 25 24 22 27 21 28 23 26 23 24 ## [13] 27 22 20 27 23 26 22 23 23 28 21 25 ## [25] 20 19 24 22 27 22 21 28 28 24 23 24 ## [37] 21 28 27 24 mean(x)+c(-1,1)*1.96*sqrt(var(x)/40) ## [1] 23.12872 24.77128 3.3.3 Intervamuestra menor a 30.lo de confianza para la media y la diferencia de medias con varianza desconocida pero Para producir un intervalo de confianza valido en estos casos, se debe realizar supuestos fuertes respecto la población de la cual proviene la información. La suposición mas usual es que la población de base viene de una normal, lo que nos lleva en distribuciones muestrales a trabajar con un distribución \\(t-student\\). Para la media, sea: \\[t=\\frac{\\bar{X}-\\mu}{\\frac{\\hat{S}}{\\sqrt{n}}}\\] Ahora \\(t\\sim t-student(n-1)\\), al igual que para la normal el objetivo es encontrar: \\[P(L\\leq t \\leq U)=1-\\alpha\\] \\[P(-t_{\\alpha/2,n-1} \\leq t \\leq t_{\\alpha/2,n-1})=1-\\alpha\\] Lo que nos a: \\[IC_{100*(1-\\alpha)}(\\mu): \\bar{X} \\pm t_{\\alpha/2,n-1} \\sqrt{\\frac{\\hat{S}^2}{n}}\\] Ejemplo, suponga que tenemos las mediciones de la estatura en cm de un grupo de 20 personas que fue seleccionado de forma aleatoria de una determinada población, los datos son: set.seed(1421) x&lt;-round(runif(20,140,180)) x ## [1] 171 173 180 152 148 166 177 179 173 ## [10] 167 175 145 152 142 143 155 179 176 ## [19] 173 147 Encontrar un intervalo de confianza al 95% de confiabilidad para la media poblacional. Solución, mx&lt;-mean(x) ta&lt;-qt(0.05/2,19,lower.tail = F) s2&lt;-var(x) n&lt;-20 mx+c(-1,1)*ta*sqrt(s2/n) ## [1] 157.1698 170.1302 Para la diferencia de medias, pero con varianzas iguales \\(\\sigma_1=\\sigma_2\\) Cuando \\(n_1\\) o \\(n_2\\) no superen ambas a 30, la mejor alternativa es usar el intervalo de confianza en base a la distribución \\(t\\), para la diferencia de medias el IC es: \\[IC_{100*(1-\\alpha)}(\\mu_1-\\mu_2)=\\bar{X}_1-\\bar{X}_2+t_{\\alpha/2,n_1+n_2-2} \\hat{S}_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\] Con \\(\\hat{S^2}_p\\) \\[\\hat{S}^2_p=\\frac{(n_1-1)\\hat{S}^2_1+(n_2-1)\\hat{S}^2_2}{n_1+n_2-2}\\] Para la diferencia de medias, pero con varianzas no iguales \\[IC_{100*(1-\\alpha)}(\\mu_1-\\mu_2)=\\bar{X}_1-\\bar{X}_2+t_{\\alpha/2,v} \\sqrt{\\frac{\\hat{S}_1^2}{n_1}+\\frac{\\hat{S}^2_2}{n_2}}\\] \\[v=\\frac{\\left(\\frac{\\hat{S}_1^2}{n_1}+\\frac{\\hat{S}^2_2}{n_2} \\right)^2}{\\frac{(\\hat{S}_1^2/n_1)^2}{n_1+1}+\\frac{(\\hat{S}^2_2/n_2)^2}{n_2+1}}\\] Ejemplo, se tienen dos grupos donde se tomo de forma aleatoria a una muestra de ambos grupos, con el fin de estudiar la diferencia de medias para las notas de un examen, los resultados para ambos grupos fueron: set.seed(1421) x1&lt;-round(runif(10,1,100)) set.seed(1457) x2&lt;-round(runif(13,1,100)) x1 ## [1] 79 82 99 30 20 64 94 97 84 67 x2 ## [1] 73 62 57 23 59 16 65 39 81 23 74 22 ## [13] 20 Armar un intervalo de confianza al 95% de confiabilidad suponiendo que las varianzas son iguales y otro para varianzas distintas. mx1&lt;-mean(x1) mx2&lt;-mean(x2) s21&lt;-var(x1) s22&lt;-var(x2) n1&lt;-10 n2&lt;-13 ta1&lt;-qt(0.025,n1+n2-2,lower.tail = F) s2p&lt;-((n1-1)*s21+(n2-1)*s22)/(n1+n2-2) v&lt;-((s21/n1+s22/n2)^2)/(((s21/n1)^2)/(n1+1)+((s22/n2)^2)/(n2+1)) ta2&lt;-qt(0.025,v,lower.tail = F) # Intervalo con varianzas iguales mx1-mx2+c(-1,1)*ta1*sqrt(s2p)*sqrt(1/n1+1/n2) ## [1] 2.132725 46.605736 # Intervalo con varianzas desiguales mx1-mx2+c(-1,1)*ta2*sqrt(s21/n1+s22/n2) ## [1] 1.793795 46.944667 #estimador puntual mx1-mx2 ## [1] 24.36923 3.3.4 Intervalos de confianza para la diferencia de medias de datos pareados En general, supongamos que los datos constan de \\(n\\) pares \\((X_{11}, X_{21}), (X_{12}, X_{22})...(X_{1n}, X_{2n})\\), usualmente esto sucede cuando se trabaja con observaciones en 2 puntos distintos de tiempo. Definamos las diferencias como \\(D_1=X_{11}-X_{21},D_2=X_{12}-X{22},\\ldots,D_n=X_{1n}-X_{2n}\\), estableciendo a \\(D\\) como la va, su intervalo es: \\[IC_{100*(1-\\alpha)}(\\mu_D=\\mu_1-\\mu_2): \\bar{D} \\pm t_{\\alpha/2,n-1}*\\sqrt{\\frac{\\hat{S}^2_D}{n}} \\] Si n es grande, entonces el intervalo es dado por: \\[IC_{100*(1-\\alpha)}(\\mu_D=\\mu_1-\\mu_2): \\bar{D} \\pm Z_{\\alpha/2}*\\sqrt{\\frac{\\hat{S}^2_D}{n}} \\] Ejemplo, se tiene las notas de los parciales de una muestra de 12 estudiantes, será que los estudiantes mejoraron su rendimiento del parcial 1 al parcial 2. Construya un intervalo de confianza al 90%. bd&lt;-data.frame(id=1:12,p1=round(runif(12,40,80)),p2=round(runif(12,30,90))) bd$D&lt;-bd$p1-bd$p2 mean(bd$D)+c(-1,1)*qt(0.05,11,lower.tail = F)*sqrt(var(bd$D)/12) ## [1] -12.359681 9.193015 3.3.5 Intervalo de confianza para proporciones Si asumimos que el estimador del parámetro \\(P\\) (proporción), se distribuye normal, es decir, \\(\\hat{P}\\sim Normal\\), bajo ciertas condiciones como que el tamaño de muestra es grande, se puede plantear al intervalo de confianza como: \\[IC_{100*(1-\\alpha)}(P)=\\hat{P} \\pm Z_{\\alpha/2} * \\sqrt{V(\\hat{P})}\\] Donde para encontrar la \\(V(\\hat{P})\\) basta con resolver la varianza del estimador de la media para valores binarios. \\[V(\\bar{X})=\\frac{\\sigma^2}{n} =V(\\hat{P})=\\frac{\\sigma^2_p}{n}=\\frac{1}{n}\\left(\\frac{\\sum_{i=1}^N x_i^2 }{N}-\\mu^2 \\right)=\\frac{1}{n}\\left(\\frac{\\sum_{i=1}^N x_i }{N}-P^2 \\right)=\\] \\[=\\frac{1}{n}(P-P^2)=\\frac{P(1-P)}{n}\\] Notar que esta varianza esta en términos del parámetro \\(P\\), por lo que recurrimos en el caso muestral a reemplazarlo por su estimador \\(\\hat{P}\\). Así, el intervalo de confianza queda como: \\[IC_{100*(1-\\alpha)}(P)=\\hat{P} \\pm Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P}(1-\\hat{P})}{n}}\\] Ejemplo, en un curso se tomo una muestra aleatoria de 15 personas, respecto su estatura en centímetros, las mediciones son: set.seed(1501) x&lt;-round(rnorm(15,165,10),0) x ## [1] 161 179 171 152 141 168 168 173 155 ## [10] 169 160 177 164 165 159 Se pide encontrar al estimador de la proporción y su intervalo de confianza al 95% de confiabilidad para la proporción de estudiantes con estatura de 170 o más. Solución, n&lt;-15 x&lt;-(x&gt;=170)*1 x ## [1] 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 #el estimador de la proporción p&lt;-mean(x) p ## [1] 0.2666667 #en porcentaje mean(x)*100 ## [1] 26.66667 #intervalo p+c(-1,1)*1.96*sqrt(p*(1-p)/n) ## [1] 0.04287417 0.49045916 #p-1.96*sqrt(p*(1-p)/n) #p+1.96*sqrt(p*(1-p)/n) #intervalo en % (p+c(-1,1)*1.96*sqrt(p*(1-p)/n))*100 ## [1] 4.287417 49.045916 3.3.5.1 Tamaño de muestra para estimar proporciones Para la proporción definimos el margen de error (\\(\\epsilon\\)) como: \\[\\epsilon= Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P}(1-\\hat{P})}{n}}\\] Ahora, podemos usar esta definición como una salida para el calculo del tamaño de muestra necesario para cometer un determinado margen de error (\\(epsilon\\)), sujeto a un nivel de confiabilidad de \\(Z_{\\alpha/2}\\), basta con despejar \\(n\\) de la anterior formula. \\[n=\\left(\\frac{Z_{\\alpha/2}}{\\epsilon}\\right)^2 \\hat{P}(1-\\hat{P})\\] Dado que definir el tamaño de muestra es un paso previo a la recolección de información, notar que se tiene total control sobre el margen de error (\\(\\epsilon\\)) y el nivel de confiabilidad (\\(Z_{\\alpha/2}\\)), sin embargo, en la formula aparece el estimador \\(\\hat{P}\\) que es el de interés, la solución para saltarnos este dilema, es elegir un \\(\\hat{P}\\) basado en un estudio similar o una prueba piloto, la otra alternativa extrema es suponer un \\(\\hat{P}\\) que haga máximo a \\(n\\) con lo demás fijo. Ejercicio, una carrera en la universidad esta a punto de elegir a sus autoridades, se busca hacer una encuesta de intención de votos en los estudiantes para el candidato \\(Z\\), se quiere un nivel de confianza del 95%, y no errar en +- 5%. Calcular el tamaño de muestra, (1) suponiendo \\(n\\) máxima y (2) mediante un sondeo se verifico que el candidato \\(Z\\) tiene un 70% de apoyo. 3.3.6 Intervalo de confianza para diferencia de proporciones Esta diferencia de proporciones son ampliamente usadas cuando se comparan dos poblaciones, respecto una característica de interés sobre dos poblaciones independientes. Así el intervalo de la diferencia de proporciones esta dado por: \\[IC_{100*(1-\\alpha)}(P_1-P_2)=(\\hat{P}_1-\\hat{P}_2) \\pm Z_{\\alpha/2} * \\sqrt{\\frac{\\hat{P_1}(1-\\hat{P}_1)}{n_1}+\\frac{\\hat{P}_2(1-\\hat{P}_2)}{n_2}}\\] Donde \\(\\hat{P}_1\\) y \\(\\hat{P}_2\\) son estimaciones de proporción para la población 1 y 2, respecto una misma característica, \\(n_1\\) y \\(n_2\\) son los tamaños de muestra es estas poblaciones. Ejercicio, en una muestra aleatoria de 20 estudiantes se midió la estatura del grupo, se tiene conocimiento del sexo de los estudiantes y se busca estimar la diferencia de proporciones por hombre y mujer de la proporción de estudiantes que superan los 170 cm de estatura. Los datos son: n&lt;-20 set.seed(1501) estatura&lt;-round(rnorm(n,165,10),0) set.seed(1501) mujer&lt;-rbinom(n,1,0.4) bd&lt;-data.frame(estatura,mujer) bd ## estatura mujer ## 1 161 0 ## 2 179 0 ## 3 171 1 ## 4 152 0 ## 5 141 1 ## 6 168 0 ## 7 168 0 ## 8 173 1 ## 9 155 0 ## 10 169 0 ## 11 160 1 ## 12 177 1 ## 13 164 1 ## 14 165 1 ## 15 159 1 ## 16 172 1 ## 17 158 0 ## 18 169 0 ## 19 172 1 ## 20 166 0 Se pide calcular el estimador puntual de la diferencia de proporciones y el intervalo de confianza al 90% de confiabilidad. Solución, algunos parámetros del ejercicio \\(n_1=10\\) , \\(n_2=10\\), las proporciones: \\[\\hat{P}_{1,h}=\\frac{\\#a}{n_1}=\\frac{1}{10}=0.1\\] \\[\\hat{P}_{2,m}=\\frac{\\#a}{n_2}=\\frac{5}{10}=0.5\\] El estimador puntual es dado por: \\[\\hat{P}_1-\\hat{P}_2=0.1-0.5=-0.4\\] Ahora, el intervalo de confianza \\[IC_{100*(1-\\alpha)}(P_1-P_2)=-0.4 \\pm 1.64 * \\sqrt{\\frac{0.1*0.9}{10}+\\frac{0.5*0.5}{10}}=-0.4 \\pm 0.3024\\] \\[IC_{100*(1-\\alpha)}(P_1-P_2)=[-0.702 \\quad -0.098]\\] 3.3.7 Intervalo de confianza para la varianza Suponga que \\(X\\sim Normal(\\mu,\\sigma)\\) ambos parámetros desconocidos. Sea \\(X_1,X_2,\\ldots,X_n\\) una muestra aleatoria de tamaño \\(n\\) de \\(X\\). Recodar que la varianza muestra sigue una distribución de muestreo tipo \\(\\chi^2\\). \\[\\chi^2=\\frac{(n-1)\\hat{S}^2}{\\sigma^2}\\] Para desarrollar el intervalo usamos esta distribución: \\[P(\\chi^2_{1-\\alpha/2,n-1} \\leq \\chi^2 \\leq \\chi^2_{\\alpha/2,n-1})=1-\\alpha\\] \\[IC_{100*(1-\\alpha)}(\\sigma^2): \\left[\\frac{(n-1)\\hat{S}^2}{\\chi^2_{\\alpha/2,n-1}} \\quad \\frac{(n-1)\\hat{S}^2}{\\chi^2_{1-\\alpha/2,n-1}}\\right]\\] 3.3.8 Intervalo de confianza para el cociente de varianzas El objetivo de esta medida es tener un intervalo para el cociente de las varianzas de dos poblaciones, esto puede servir para identificar que población (con que variable) tiene mayor variabilidad. El parámetro es: \\[\\theta=\\frac{\\sigma^2_1}{\\sigma^2_2}\\] Supongamos que \\(X_1\\) y \\(X_2\\) son va normales con media \\(\\mu_1\\) y \\(\\mu_2\\) desconocidas y varianzas \\(\\sigma^2_1\\), \\(\\sigma^2_2\\) también desconocidas. Sean dos muestras aleatorias de \\(X_1\\) y \\(X_2\\) de tamaño \\(n_1\\) y \\(n_2\\) y sean \\(\\hat{S}^2_1\\) y \\(\\hat{S}^2_2\\) las varianzas de las muestras. Para armar el intervalo de confianza recurrimos a la distribución \\(F\\). \\[F=\\frac{\\frac{\\hat{S}^2_2}{\\sigma^2_2}}{\\frac{\\hat{S}^2_1}{\\sigma^2_1}}\\] Se busca: \\[P(F_{1-\\alpha/2,n_1-1,n_2-1} \\leq F \\leq F_{\\alpha/2,n_1-1,n_2-1})=1-\\alpha\\] \\[IC_{100*(1-\\alpha)}\\left(\\frac{\\sigma^2_1}{\\sigma^2_2}\\right): \\left[\\frac{\\hat{S}^2_1}{\\hat{S}^2_2} F_{1-\\alpha/2,n_1-1,n_2-1} \\quad \\frac{\\hat{S}^2_1}{\\hat{S}^2_2} F_{\\alpha/2,n_1-1,n_2-1}\\right]\\] "],["prueba-de-hipótesis.html", "4 Prueba de Hipótesis 4.1 Introducción 4.2 Prueba de hipótesis sobre la media 4.3 Prueba de hipótesis sobre la diferencia de medias 4.4 Prueba de hipótesis sobre la proporción 4.5 Prueba de hipótesis sobre la diferencia de Proporciones 4.6 Tamaño de muestra y su relación con el error de tipo I y tipo II", " 4 Prueba de Hipótesis Dado que el principal objetivo de la inferencia estadística es aproximarse al valor de los parámetros (\\(\\theta\\)) mediante un estimador muestral \\(\\hat{\\theta}\\) que viene de una distribución muestral y por lo tanto es una variable aleatoria también. Otra estrategia a parte de la estimación (puntual o por intervalos) del parámetro \\(\\theta\\) es la de plantear hipótesis al rededor de los valores del parámetro. A esto le vamos a denominar una prueba de hipótesis, donde solamente existen dos posibles resultados; Rechazamos la hipótesis o no la rechazamos. En este tema se explora las pruebas de hipótesis estadísticas sobre parámetros comunes, vistos anteriormente: Media Diferencia de medias Proporción Diferencia de proporciones Datos pareados (colección de información de la misma unidad, en diversos momentos del tiempo) Varianza Igualdad de varianza Adicionalmente, al final del tema se vera la pruebas de bondad de ajuste, que se sirven para plantear cuando una serie de datos tiene una determinada distribución Todas las pruebas que se verán están enmarcadas en lo paramétrico, es decir, se realizarán supuestos respecto la distribución de la información y se trabaja con distribuciones conocidas como; la normal, t-student, \\(\\chi^2\\), F. Mencionar que existen las pruebas de hipótesis no paramétricas, estas funcionan sobre las distribuciones libres. 4.1 Introducción 4.1.1 Hipótesis estadística Tener presente que la hipótesis que se defina debe estar siempre en términos del parámetro, no de la muestra. Cuando elaboramos una hipótesis esta tiene dos elementos; la hipótesis que se plantea (hipótesis nula) y el complemento de esta hipótesis (hipótesis alternativa), esta última puede ser de un lado o de dos lados. \\[H_0: \\theta = k\\] \\[H_1: \\theta \\neq k \\quad(\\text{2 lados})\\] \\[H_1: \\theta &lt; k \\quad(\\text{1 lado})\\] \\[H_1: \\theta &gt; k \\quad(\\text{1 lado})\\] 4.1.2 Región de aceptación y región de rechazo Al plantear un hipótesis estadística sobre un parámetro de alguna población, la manera de verificar dicha hipótesis pasara por estudiar una muestra aleatoria sobre la cual se establecera una regla que nos permita decir si la hipótesis es correcta o no. Tradicionalmente se definirá un estadístico de prueba que nos permitirá decidir. Por ejemplo, estamos interesados en conocer el promedio de ingresos laborales mensuales de las personas que viven en el municipio de La Paz. Planteamos la hipótesis nula: \\[H_0: \\mu_{ingreso}=3000.Bs\\] La hipótesis alternativa \\[H_1: \\mu_{ingreso} \\neq 3000.Bs\\] Imaginemos que tomamos una muestra aleatoria de personas del municipio de La Paz y les consultamos acerca de su ingreso laboral mensual, el resultado del promedio muestral es de \\[\\bar{x}=?\\] Una regla para contrastar una hipótesis es definir la región de aceptación y la región de rechazo, en este ejemplo podemos momentaneamente de forma arbitraria podemos decir que aceptamos la \\(H_0\\) si \\[2700\\leq\\bar{x}\\leq 3100\\] En otro caso, se rechaza la hipótesis y la región que esta fuera de la región de aceptación se conoce como región de rechazo (\\(\\bar{x}&lt;2700\\) ó \\(\\bar{x}&gt;3100\\)). Nota: En la práctica, normalmente la región de aceptación para pruebas de 2 lados coincide con el intervalo de confianza. Tener presente los conceptos de: Hipótesis nula Hipótesis alternativa (1 lado, 2 lados) Estadístico de prueba Región de aceptación Región de rechazo Error de tipo I Error de tipo II 4.1.3 Errores de tipo I y errores de tipo II (investigar de que se trata y buscar ejemplos) (Falso positivo) Error de tipo I (\\(\\alpha\\)) (Falso negativos) Error de tipo II (\\(\\beta\\)) 4.1.4 Pruebas bilaterales \\[H_0: \\theta=k\\] \\[H_1: \\theta\\neq k\\] 4.1.5 Pruebas unilaterales Existen de 2 tipos, 4.1.6 Pasos para la prueba hipótesis estadística Plantear la prueba de hipótesis; definir \\(H_0\\) y \\(H_1\\) Establecer el nivel de significancia de la prueba; el error de tipo I a tolerar, normalmente los valores más usuales son \\(\\alpha=\\{0.01,0.05,0.1\\}\\) Seleccionar y construir el estadístico de prueba adecuado; se obtiene usando la información de una muestra aleatoria Determinar las regiones de aceptación y rechazo; estas regiones se construyen usando como insumo el paso 2 y 3 Tomar una decisión en base al estadístico de prueba y las regiones de aceptación y rechazo identificadas. 4.2 Prueba de hipótesis sobre la media Para las pruebas al rededor de la media, vamos a suponer que los datos de interés se distribuyen como una normal o al menos que la muestra aleatoria para el estadístico de prueba es grande y por lo tanto podemos usar el teorema del limite central. Existen dos variaciones para esta prueba; cuando se conoce la varianza y cuando no se conoce 4.2.1 Con varianza conocida Hipótesis \\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\] Nivel de significancia; es \\(\\alpha\\), dado que es una prueba bilateral existen dos regiones de rechazo, cada una de estas 2 regiones con probabilidad \\(\\alpha/2\\) Estadístico de prueba; Se cuenta con una muestra aleatoria (\\(X_1,X_2,\\ldots,X_n\\)) iid de tamaño \\(n\\), de tal forma que cada una de estas variables \\(X_i\\sim .(E(X_i)=\\mu,\\sigma(conocido))\\). La estadística de prueba es: \\[Z_0=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] Este estadístico supone que \\(Z_0\\sim N(0,1)\\), en el supuesto que \\(E[\\bar{X}]=\\mu=\\mu_0\\), este supuesto se cumple siempre y cuando \\(H_0\\) sea cierta Región de aceptación y rechazo curve(dnorm(x),xlim=c(-4,4),main=&quot;significancia del 10%&quot;) # abline(v=c(-1.64,1.64),col=&quot;red&quot;) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) # al 5 % curve(dnorm(x),xlim=c(-4,4),main=&quot;significancia del 5%&quot;) abline(v=c(-1.96,1.96),col=&quot;red&quot;) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) # al 1 % curve(dnorm(x),xlim=c(-4,4),main=&quot;significancia del 1%&quot;) abline(v=c(-2.58,2.58),col=&quot;red&quot;) text(c(-3.5,0,3.5),rep(0.2,3),c(&quot;Rechazo&quot;,&quot;Aceptación&quot;,&quot;Rechazo&quot;)) #qnorm(0.01/2) La decisión; Se rechaza la \\(H_0\\) cuando: \\[Z_0&gt;Z_{\\alpha/2} \\quad ó \\quad Z_0&lt;-Z_{\\alpha/2}\\] Los valores usuales para los \\(Z_{\\alpha/2}\\) son: al 10 de significanca: \\(Z_{\\alpha/2}=1.64\\) al 5 de significanca: \\(Z_{\\alpha/2}=1.96\\) al 1 de significanca: \\(Z_{\\alpha/2}=2.58\\) Ejercicio. Se sabe que los diámetros de tornillos tienen una desviación estándar de 0.0001 plg. Una muestra aleatoria de 35 tornillos produce un diámetro promedio de 0.2546 plg. Prueba la hipótesis de que el diámetro medio real es de 0.255 plg. empleando un \\(\\alpha=0.05\\). \\[H_0: \\mu=0.255\\] \\[H_1: \\mu\\neq0.255\\] \\[Z_0=\\frac{0.2546-0.255}{\\frac{0.0001}{\\sqrt{35}}}=-23.66\\] Se rechaza \\(H_0\\) si: \\[-23.66&gt;1.96 \\quad ó \\quad -23.66&lt;-1.96\\] Por lo tanto rechazamos la hipótesis nula de que la media de diámetro de los tornillos sea de 0.255 plg. Para las pruebas unilaterales se utiliza el mismo estadístico de prueba \\(Z_0\\), con las siguientes hipótesis y región de rechazo. \\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu&gt;\\mu_0\\] Se rechaza \\(H_0\\) si \\(Z_0&gt;Z{\\alpha}\\). \\[H_0: \\mu=\\mu_0\\] \\[H_1: \\mu &lt; \\mu_0\\] Se rechaza \\(H_0\\) si \\(Z_0&lt;-Z{\\alpha}\\) Los valores usuales para los \\(Z_\\{alpha}\\) de pruebas unilaterales. \\(Z_{0.1}=1.28\\) \\(Z_{0.05}=1.64\\) \\(Z_{0.01}=2.33\\) 4.2.2 Con varianza desconocida Si el tamaño de muestra fuera mayor 30, todo lo visto anteriormente se mantiene, el único cambio se da al momento de calcular el estadístico de prueba donde en lugar de \\(\\sigma\\) tomamos la varianza muestral \\(\\hat{S}\\) \\[Z_0=\\frac{\\bar{X}-\\mu_0}{\\frac{\\hat{S}}{\\sqrt{n}}}\\] Si la muestra no es mayor 30, podemos suponer que los datos son normales y la aproximación para la región de aceptación y rechazo vienen de una t-student. Estadístico de prueba \\[t_0=\\frac{\\bar{X}-\\mu_0}{\\frac{\\hat{S}}{\\sqrt{n}}}\\] Región de aceptación y rechazo; si \\(H_0\\) es cierta, entonces \\(t_0\\sim t(n-1)\\). La decisión; Se rechaza la \\(H_0\\) cuando: \\[t_0&gt;t_{\\alpha/2,n-1} \\quad ó \\quad t_0&lt;-t_{\\alpha/2,n-1}\\] 4.3 Prueba de hipótesis sobre la diferencia de medias Esta prueba se utiliza principalmente cuando se contrasta valores de 2 poblaciones independientes. Las hipótesis son: \\[H_0: \\mu_1=\\mu_2\\] \\[H_1: \\mu_1\\neq\\mu_2\\] Suponemos que se extrae muestras aleatorias de cada una de las poblaciones de tamaño \\(n_1\\) y \\(n_2\\) respectivamente. Se supone que cada una de las muestras las poblaciones se distribuyen iid con media desconocida y varianza conocida. Si suponemos que la muestra en ambas poblaciones son grandes \\(n_1,n_2&gt;30\\) usando el teorema central del limite podemos afirmar en base a los capitulos anteriores \\[\\bar{X_1}-\\bar{X}_2\\sim N \\left(\\mu_1-\\mu_2,\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}\\right)\\] Por lo tanto se puede plantear como estadístico de prueba si \\(H_0\\) es cierta: \\[Z_0=\\frac{\\bar{X_1}-\\bar{X}_2}{\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}}\\sim N(0,1)\\] Finalmente, en base a la distribución de \\(Z_0\\), se rechaza la igualdad de medias cuando: \\[Z_0&gt;Z_{\\alpha/2} \\quad ó \\quad Z_0&lt; -Z_{\\alpha/2} \\] Cuando la varianza no es conocida pero el tamaño de muestra es mayor a 30 para ambas poblaciones, el estadístico de prueba es el único que cambia \\[Z_0=\\frac{\\bar{X_1}-\\bar{X}_2}{\\sqrt{\\frac{\\hat{S}_1^2}{n_1}+\\frac{\\hat{S}_2^2}{n_2}}}\\sim N(0,1)\\] Cuando \\(H_1: \\mu_1&gt;\\mu2\\) se rechaza la \\(H_0\\) cuando: \\[Z_0&gt;Z_{\\alpha}\\] Cuando \\(H_1: \\mu_1&lt;\\mu2\\) se rechaza la \\(H_0\\) cuando: \\[Z_0&lt;-Z_{\\alpha}\\] Cuando la muestra es menor a 30 en una o ambas poblaciones, se debe hacer el supuesto que los datos son normales y aproximar la distribución a una \\(t-student\\). En este caso el estadístico de prueba es: \\[t_0=\\frac{\\bar{X_1}-\\bar{X}_2}{\\sqrt{\\frac{\\hat{S}_1^2}{n_1}+\\frac{\\hat{S}_2^2}{n_2}}}\\] Lo anterior para el estadístico \\(t_0\\) es valido bajo el supuesto de normalidad de los datos y las varianzas de las poblaciones no son iguales \\(\\sigma_1 \\neq \\sigma_2\\). En el caso donde suponemos igualdad de varianzas \\(\\sigma_1 = \\sigma_2\\). El estadístico de prueba es: \\[t_0=\\frac{\\bar{X_1}-\\bar{X}_2}{\\hat{S}_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\] Donde: \\[\\hat{S}^2_p=\\frac{(n_1-1)\\hat{S}_1^2+(n_2-1)\\hat{S}_2^2}{n_1+n_2-2}\\] Entonces, para el caso de igualdad de varianza se rechaza la \\(H_0\\) cuando: \\[t_0&gt;t_{\\alpha/2,n_1+n_2-2} \\quad ó \\quad t_0&lt;-t_{\\alpha/2,n_1+n_2-2}\\] Cuando las varianzas no son iguales, se rechaza la \\(H_0\\) cuando: \\[t_0&gt;t_{\\alpha/2,v} \\quad ó \\quad t_0&lt;-t_{\\alpha/2,v}\\] Donde: \\[v=\\frac{\\frac{\\hat{S}_1^2}{n_1}+\\frac{\\hat{S}_2^2}{n_2}}{\\frac{(\\hat{S}^2_1/n_1)^2}{n_1+1}+\\frac{(\\hat{S}^2_2/n_2)^2}{n_2+1}}-2\\] Ejemplo. Dos tipos de plásticos son apropiados para que los utilice un fabricante de componentes electrónicos. La resistencia al rompimiento de estos plásticos es importante, Se sabe que \\(\\sigma_1=\\sigma_2=1.0\\) psi. De una muestra aleatoria de tamaño \\(n_1 = 10\\) y \\(n_2 = 12\\) obtenemos \\(\\bar{X}_1 = 162.5\\) y \\(\\bar{X}_2 = 155.0\\). La compañía no adoptará el plástico 1 a menos que su resistencia al rompimiento exceda la del plástico 2 al menos por 10 psi. De acuerdo con la información de la muestra, ¿debe utilizarse el plástico I? Solución, la variable es el resistencia al rompimiento de dos tipos de plásticos. Sea \\(\\mu_1\\) y \\(\\mu_2\\) los promedios de resistencia de los plásticos 1 y 2 respectivamente. \\[H_0: \\mu_1=\\mu_2+10\\] Vamos a definir una tercera variable en base al plástico 2 que sea \\(y=x_2+10\\), de tal forme que \\(\\mu_y=E[y]=\\mu_2+10\\) , también, \\(\\sigma^2_y=V(Y)=V(X_2)=\\sigma_2^2\\) , así la hipótesis es: \\[H_0: \\mu_1=\\mu_y\\] \\[H_1: \\mu_1 \\neq \\mu_y\\] \\[H_1: \\mu_1 &lt; \\mu_y\\] \\[Z_0=\\frac{162.5-(155.0+10)}{\\sqrt{\\frac{1}{10}+\\frac{1}{12}}}=-5.8387\\] Evaluando en la región de rechazo con una significancia del 5% \\[-5.84&lt; -1.96 = -Z_{\\alpha/2}\\] Por lo tanto rechazamos \\(H_0\\) Ahora el prueba unilateral \\[-5.84&lt; -1.64 = -Z_{\\alpha}\\] Para ambas \\(H_1\\) se rechaza la \\(H_0\\). Por lo tanto estadísticamente se recomienda no usar el plástico I ya que no supera en 10 psi al plástico 2 Si contrastamos: \\[H_1: \\mu_1&gt;\\mu_y\\] Se rechaza la \\(H_0\\) si: \\[-5.84&gt; 1.64 = Z_{\\alpha}\\] 4.4 Prueba de hipótesis sobre la proporción Partimos de las siguientes hipótesis: \\[H_0: P=P_0\\] \\[H_1: P \\neq P_0\\] \\[H_1: P &lt; P_0\\] \\[H_1: P &gt; P_0\\] Como se vio en el capítulo anterior podemos suponer que tenemos una muestra aleatoria de tamaño \\(n\\) donde las variables de esta muestra \\(X\\sim N(nP,n*P*(1-P))\\), entonces si la hipótesis nula es cierta entonces \\(X\\sim N(nP_0,n*P_0*(1-P_0))\\). Así el estadístico de prueba es la estandarización de \\(X\\). \\[Z_0=\\frac{X-nP_0}{\\sqrt{n*P_0*(1-P_0)}}\\sim N(0,1)\\] Donde \\(X\\) representa el número de casos en la muestra aleatoria que pertenecen a la clase de interés. De esta forma en una prueba bilateral se rechaza la \\(H_0\\) si \\[Z_0&lt; - Z_{\\alpha/2} \\quad ó \\quad Z_0&gt; Z_{\\alpha/2}\\] Para las pruebas unilaterales, con \\(H_1: P&gt; P_0\\), se rechaza \\(H_0\\) cuando: \\[Z_0&gt;Z_\\alpha\\] Con \\(H_1: P&lt; P_0\\), se rechaza \\(H_0\\) cuando: \\[Z_0&lt;-Z_\\alpha\\] Ejercicio, De 1000 casos seleccionados de cáncer en el pulmón, 823 terminaron en muerte. Se podría decir que el porcentaje de muertes es del 90%, o mayor a este valor. Solución, el parámetro de interés es la proporción de muertes por el cáncer de pulmón (\\(P\\)) \\[H_0: P=0.9\\] \\[H_1: P\\neq0.9\\] \\[H_2: P &gt; 0.9\\] El estadístico de prueba: \\[Z_0=\\frac{823-1000*0.9}{\\sqrt{1000*0.9*0.1}}=-8.11\\] Para la prueba bilateral a un \\(\\alpha=0.05\\): \\[-8.11&lt; - 1.96 \\quad ó \\quad -8.11&gt; 1.96\\] Por lo tanto se rechaza la \\(H_0\\). Mientras que para la prueba unilateral \\[-8.11&gt;1.64\\] No hay evidencia estadística para rechazar \\(H_0\\) y aceptar \\(H_2\\). 4.5 Prueba de hipótesis sobre la diferencia de Proporciones En este caso el supuesto principal es que se trabajan con 2 poblaciones independientes y sobre ellas se busca contrastar el valor de sus proporciones de alguna clase en particular. \\[H_0: P_1=P_2\\] \\[H_1: P_1 \\neq P_2\\] \\[H_1: P_1 &gt; P_2\\] \\[H_1: P_1 &lt; P_2\\] Se parte con dos muestras aleatorias de tamaños \\(n_1\\) y \\(n_2\\) de ambas poblaciones, y sea \\(X_1\\), \\(X_2\\) el número de observaciones que pertenecen a la clase de interés en las muestras. Si aproximamos a \\(X_1\\) y \\(X_2\\) como normales al ser estas binomiales, si se trabaja a nivel de los estimadores, sean \\(\\hat{P}_1\\) y \\(\\hat{P}_2\\) se sabe que: \\[Z_0=\\frac{\\hat{P_1}-\\hat{P_2}}{\\sqrt{\\hat{P}(1-\\hat{P})*\\left[\\frac{1}{n_1}+\\frac{1}{n_2} \\right]}}\\sim N(0,1)\\] \\[\\hat{P}=\\frac{X_1+X_2}{n_1+n_2}\\] De esta forma en una prueba bilateral se rechaza la \\(H_0\\) si \\[Z_0&lt; - Z_{\\alpha/2} \\quad ó \\quad Z_0&gt; Z_{\\alpha/2}\\] Para las pruebas unilaterales, con \\(H_1: P_1&gt; P_2\\), se rechaza \\(H_0\\) cuando: \\[Z_0&gt;Z_\\alpha\\] Con \\(H_1: P_1&lt; P_2\\), se rechaza \\(H_0\\) cuando: \\[Z_0&lt;-Z_\\alpha\\] Ejercicio, del libro guía del capítulo resolver el ejercicio 11.41 y 11.42 4.6 Tamaño de muestra y su relación con el error de tipo I y tipo II Recordar que el error de tipo 1 denotado por \\(\\alpha\\) es el falso positivo o también establece el riesgo de rechazar algo que es verdadero. Mientras que el error de tipo 2, denotado por \\(\\beta\\) es el falso negativo, que establece el riesgo de aceptar algo que es falso. Para la prueba de hipótesis para la media, la formula para el tamaño de muestra es: Bilateral, \\[n=\\frac{(Z_{\\alpha/2}+Z_\\beta)^2 \\sigma^2}{\\delta^2 }\\] Unilaterales \\[n=\\frac{(Z_{\\alpha}+Z_\\beta)^2 \\sigma^2}{\\delta^2 }\\] Ejercicio, 11.2 del libro guía. Se está estudiando el rendimiento de un proceso químico. De la experiencia previa se sabe que la varianza del rendimiento con este proceso es 5. Los últimos cinco días de operación de la planta han dado como resultado los siguientes rendimientos (en porcentajes): \\[91.6,88.75,90.8,89.95,91.3.\\] ¿Hay razón para creer que el rendimiento es menor al 90%? ¿Que tamaño de muestra se requeriría para detectar un rendimiento medio verdadero de 85% con probabilidad de 0.95? Solución, \\[H_0: \\mu=90\\] \\[H_1: \\mu&lt;90\\] \\[Z_0=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}=\\frac{90.48-90}{\\frac{2.24}{\\sqrt{5}}}=0.48\\] Se rechaza \\(H_0\\) si (a un \\(\\alpha=0.05\\)): \\[0.48&lt;-Z_{\\alpha}=-1.64\\] Por lo tanto no hay evidencia estadística para rechazar \\(H_0\\). \\[n=\\frac{(Z_{\\alpha}+Z_\\beta)^2 \\sigma^2}{\\delta^2 }=\\frac{(1.64+1.64)^2 5}{(-5)^2 }=2.15 \\approx 3\\] De forma análoga en las otras pruebas estadísticas se tienen los tamaños de muestra: Para la diferencia de medias Bilateral \\[n=\\frac{(Z_{\\alpha/2}+Z_\\beta)^2 (\\sigma_1^2+\\sigma_2^2)}{\\delta^2 }\\] \\[n=\\frac{(Z_{\\alpha}+Z_\\beta)^2 (\\sigma_1^2+\\sigma_2^2)}{\\delta^2 }\\] Para la proporción, Bilateral \\[n=\\left(\\frac{Z_{\\alpha/2} \\sqrt{P_0(1-P_0)}+Z_{\\beta} \\sqrt{P(1-P)}}{P-P_0} \\right)^2\\] Donde \\(P\\) es el valor propuesto para \\(H_1\\). Unilateral \\[n=\\left(\\frac{Z_{\\alpha} \\sqrt{P_0(1-P_0)}+Z_{\\beta} \\sqrt{P(1-P)}}{P-P_0} \\right)^2\\] Tareas, revisar el libro guía el capítulo 11, en los apartados: 11-10.2. Tamaño de muestra para la diferencia de proporciones Revisar la tabla 11.7 Revisar la Prueba de hipótesis sobre la varianza Revisar la Prueba de hipótesis sobre la igualdad de varianzas Se recomienda ir resolviendo los ejercicios del capitulo 11 Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2020. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown. R Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Xie, Yihui. 2019. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://CRAN.R-project.org/package=knitr. . 2020. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown. Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. How to Read This Book. Transforming Climate Finance and Green Investment with Blockchains, 1. https://doi.org/10.1016/b978-0-12-814447-3.00041-0. "]]
