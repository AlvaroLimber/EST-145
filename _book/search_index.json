[["index.html", "Estadística II EST-145 Prefacio Audiencia Estructura del libro Software y acuerdos Bases de datos Agradecimiento", " Estadística II EST-145 Alvaro Chirino Gutierrez 2021-03-17 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Estadística II, una materia del pregrado de la carrera de Informática de la Universidad Mayor de San Ándres. Estructura del libro El libro incluye 6 capítulos, estos son: Distribuciones de probabilidad bivariada Distribuciones muestrales Distribuciones Chi cuadrado, t-student y Fisher Estimación puntual de parámetros Estimación de parámetros por intervalos de confianza Pruebas de hipótesis Software y acuerdos sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Spanish_Bolivia.1252 LC_CTYPE=Spanish_Bolivia.1252 ## [3] LC_MONETARY=Spanish_Bolivia.1252 LC_NUMERIC=C ## [5] LC_TIME=Spanish_Bolivia.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.4 magrittr_2.0.1 bookdown_0.21 htmltools_0.5.1.1 ## [5] tools_4.0.4 rstudioapi_0.13 yaml_2.2.1 stringi_1.5.3 ## [9] rmarkdown_2.6 highr_0.8 knitr_1.31 stringr_1.4.0 ## [13] xfun_0.19 digest_0.6.27 packrat_0.5.0 rlang_0.4.10 ## [17] evaluate_0.14 Bases de datos En este documento se emplearan 2 bases de datos del contexto Boliviano: Encuesta a Hogares 2019 y 2019. Vivienda y Personas Computo oficial de las elecciones del 18 de Octubre de 2020 Estas bases de datos se encuentran disponibles en formato \\(.RData\\) en el repositorio de Github del texto. Agradecimiento "],["tema-1-distribuciones-bivariadas-multivariadas.html", "1 Tema 1: Distribuciones bivariadas (multivariadas) 1.1 Variables aleatorias bivariantes 1.2 Función de distribución bivariada 1.3 Función masa de probabilidad (función de densidad) 1.4 Distribución marginal 1.5 Independencia 1.6 Valores esperados 1.7 Distribuciones condicionales 1.8 Medidas de relación entre dos variables", " 1 Tema 1: Distribuciones bivariadas (multivariadas) En el caso univariado se tenia a una va \\(X\\) definida en los reales \\(IR\\), a esta va se le asignaba una función de distribución \\(F(x)\\) y una función de densidad \\(f(x)\\). Ambas distribuciones tienen su correspondencia en lo discreto y lo continuo: Caso discreto: \\[\\sum_{Rx} P(X=x)=1\\] \\[F(t)=P(X\\leq t)=\\sum_{x\\leq t} P(X=x)\\] Caso continuo: \\[\\int_{Rx}f(x)dx=1\\] \\[F(t)=P(X\\leq t)=\\int_{-\\infty}^{t}f(x)dx\\] La idea de este capitulo es ver las propiedades en el caso bivariante y generalizar para el caso multivariante. 1.1 Variables aleatorias bivariantes Son un par de variables aleatorias con una distribución conjunta, son típicamente representadas con mayúscula \\((X, Y)\\) o \\((X_1,X_2)\\), las realizaciones de estas variables aleatorias se representan como \\((x,y)\\) o \\((x_1,x_2)\\). Definición 1. Un par de variables aleatorias bivariadas es un par numérico de resultados; una función definida en \\(IR^2\\) Ejemplos: Considerar el par (edad, estatura): \\((23,170)\\) \\((20,172)\\) \\((20,154)\\) \\((26,159)\\) \\((19,175)\\) Considerar el par: (ingreso, años experiencia) Imaginar lanzar 2 monedas simultáneamente, \\(\\Omega=\\{CC,CS,SC,SS\\}\\), si definimos a \\(Cara=1\\) y \\(Sello=0\\), \\(R_{(X,Y)}=\\{(1,1),(1,0),(0,1),(0,0)\\}\\). 1.2 Función de distribución bivariada Definición 2. La función de distribución conjunta de \\((X,Y)\\) es \\[F(x,y)=P(X\\leq x,Y\\leq y)=P\\left[\\{X\\leq x\\} \\cap \\{Y\\leq y\\} \\right]\\] Las propiedades de \\(F\\) son similares al caso univariante, \\(0\\leq F(x,y)\\leq 1\\). Ejemplo: \\[F(x,y)=(1-e^{-x})(1-e^{-y});\\hspace{2cm} x,y\\geq0 \\] * \\(F(0,0)=0\\) * \\(F(\\infty,\\infty)=1\\) Calcular: \\[P(X\\leq 10,Y\\leq 100)=F(10,100)=(1-e^{-10})(1-e^{-100})=0.9999546\\] \\[P(X\\leq 5,Y\\leq 20)=F(5,20)=0.99326\\] La distribución conjunta satisface: \\[P(a&lt;X\\leq b,c&lt; Y\\leq d)=F(b,d)-F(b,c)-F(a,d)+F(a,c)\\] Para \\(a&lt;b\\) y \\(c&lt;d\\) 1.3 Función masa de probabilidad (función de densidad) Para el par \\((X,Y)\\) con una función de distribución conjunta \\(F(x,y)\\). Para el caso continuo, Definición 3. \\[f(x,y)=\\frac{\\partial^2}{\\partial x \\partial y}F(x,y)\\] Por un tema de notación, a veces escribiremos \\(f_{X,Y}(x,y)\\) Ejercicio: encontrar \\(f(x,y)\\) para la \\(F\\) dada en el ejemplo anterior \\[f(x,y)=\\frac{\\partial^2}{\\partial x \\partial y}\\left[(1-e^{-x})(1-e^{-y})\\right]=e^{-x}e^{-y}\\] \\(f\\) satisface de forma similar las propiedades vistas en el caso univariante. \\[\\int_{Rx}\\int_{Ry}f(x,y)dydx=1\\] Para el ejercicio: \\[\\int_0^{\\infty} \\int_0^{\\infty}e^{-x}e^{-y} dx dy=\\int_0^{\\infty} e^{-y} \\left[-e^{-x}/_0^{\\infty} \\right] dy =1\\] \\[P(a&lt;X\\leq b,c&lt; Y\\leq d)=\\int_a^b \\int_c^d f(x,y)dxdy\\] Para el caso discreto podemos definirlo de la siguiente forma: \\[f(x,y)=\\pi(x,y)=P(X=x,Y=y)\\] \\[\\sum_{Rx}\\sum_{Ry}\\pi(x,y)=1\\] Ejemplo, en un restaurante de pizza se vende porciones de pizza y gaseosa, el dueño del local realizó un monitoreo del patrón de como los clientes ordenan sus pedidos, respecto la cantidad de porciones de pizza con la cantidad de gaseosas, encontrando el siguiente resultado: Cuál sera la probabilidad de: \\(P(X=3,Y=2)=0.13\\) \\(P(X\\geq 2, Y=1)=P(X=2, Y=1)+P(X=3,Y=1)=\\pi_{21}+\\pi_{31}=0.17+0.05=0.22\\) 1.4 Distribución marginal La distribución conjunta del vector aleatorio \\((X,Y)\\) describe la distribución del vector aleatorio, sin embargo, es posible a partir de la distribución conjunta, generar las distribuciones para cada componente del vector aleatorio. Definición 4, la distribución marginal de X es: \\[F_X(x)=P(X\\leq x)=P(X\\leq x, Y\\leq \\infty)=lim_{y\\rightarrow \\infty} F(x,y)\\] De manera más usual se tiene: Para el caso discreto: \\[P(X=x)=\\pi(x)=\\sum_{Ry} \\pi(x,y)\\] \\[P(Y=y)=\\pi(y)=\\sum_{Rx} \\pi(x,y)\\] Para el caso continuo: \\[f(x)=\\int_{Ry} f(x,y)dy\\] \\[f(y)=\\int_{Rx} f(x,y)dx\\] Ejercicio, para la función: \\[f(x,y)=e^{-x}e^{-y}\\] Con \\(x,y\\geq 0\\), encontrar las marginales de \\(f(x)\\) y \\(f(y)\\). Solución: \\[f(x)=\\int_0^{\\infty}e^{-x}e^{-y} dy=e^{-x}\\] \\[f(y)=\\int_0^{\\infty}e^{-x}e^{-y} dx=e^{-y}\\] Nota: las marginales deben estar en función de su propia variable aleatoria y no contener otras variables, dado que son marginales. Notar que en este ejercicio: \\[f(x,y)=e^{-x}e^{-y}=f(x)*f(y)\\] Esto no siempre sucede, este caso se da cuando las variables son independientes. Ejemplo para el caso discreto, 1.5 Independencia Definición 6, dos variables aleatorias son independientes si: \\[f(x,y)=f(x)*f(y)\\] \\[\\pi(x,y)=\\pi(x)*\\pi(y)\\] Ejercicio, verificar si la siguiente función esta bien definida y si las variables son independientes. \\[f(x,y)=\\frac{1}{4}(x+y)*xy*e^{-x-y}; \\hspace{2cm} x,y&gt;0\\] Solución, Si esta bien definida, esto significa: \\[\\int_0^{\\infty}\\int_0^{\\infty}\\frac{1}{4}(x+y)*xy*e^{-x-y}=1\\] \\[f(x)=\\int_{Ry}f(x,y)dy=\\frac{x^2+2x}{4} ( e^{-x})\\] Tarea, encontrar \\(f(y)\\) y evaluar si son o no independientes 1.6 Valores esperados En el caso univariado, sea \\(X\\) una variable aleatoria con función de probabilidad \\(\\pi(x)\\) para el caso discreto o \\(f(x)\\) para el caso continuo, el operador matemático esperanza se define como: Para el caso discreto, \\[E[g(X)]=\\sum_{Rx}g(x)P(X=x)\\] Para el caso continuo, \\[E[g(X)]=\\int_{Rx}g(x)f(x)dx\\] Definición 6, El valor esperado para la función \\(g(X,Y)\\), se define como: Para el caso discreto: \\[E[g(X,Y)]=\\sum_{Rx}\\sum_{Ry}g(x,y)\\pi(x,y)\\] Para el caso continuo: \\[E[g(X,Y)]=\\int_{Rx}\\int_{Ry}g(x,y)f(x.y)dydx\\] Nota, hay valores esperados más usuales que otros, Por ejemplo, las varianzas para cada variable \\[E[(X-E[X])^2]=V(X)\\] \\[E[(Y-E[Y])^2]=V(Y)\\] Otras medidas son \\(E[X]\\), \\(E[Y]\\) que son referencias muy similares a un promedio aritmético. Otra valor esperado bastante usado en los casos bivariados es: \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx\\] Encontrar la forma de \\(E[X]\\) usando la definición anterior. \\[E[X]=\\int_{Rx}\\int_{Ry} x f(x,y) dy dx=\\int_{Rx}xf(x) dx\\] \\[E[X]=\\sum_{Rx}\\sum_{Ry} x \\pi(x,y) =\\sum_{Rx}x \\pi(x) dx\\] Ejemplo para el caso discreto, de las pizza y las gaseosas. \\[E[X]=1*0.5+2*0.31+3*0.19=1.69\\] \\[E[Y]=0*0.12+1*0.63+2*0.25=1.13\\] \\[E[X^2]=\\sum_{x=1}^{x=3}x^2 \\pi(x)=1^2*0.5+2^2*0.31+3^2*0.19=3.45\\] \\[E[Y^2]=0^2*0.12+1^2*0.63+2^2*0.25=1.63\\] \\[E[XY]=\\sum_{Rx}\\sum_{Ry}xy \\pi(x,y)=1*0*0.04+1*1*0.42+\\ldots+3*2*0.13=2.067\\] 1.7 Distribuciones condicionales Estas distribuciones nos ayudan a entender el comportamiento de una variable, cuando fijamos a otra. Definición, una distribución condicional se define como: Caso discreto, \\(\\pi_{x/y}(X/Y=y)=\\frac{\\pi(x,y)}{\\pi(y)}\\) Caso continuo, \\[f_{X/Y}(x/y)=\\frac{f(x,y)}{f(y)}\\] \\[f_{Y/X}(y/x)=\\frac{f(x,y)}{f(x)}\\] Estas funciones condicionales cumplen todas las propiedades de una función de probabilidad. Demostrar que: \\[\\int_{Rx} f_{X/Y}(x/y) dx=1\\] \\[\\int_{Rx} f_{X/Y}(x/y) dx=\\int_{Rx} \\frac{f(x,y)}{f(y)} dx=\\frac{1}{f(y)}\\int_{Rx} f(x,y)dx=\\frac{f(y)}{f(y)}=1\\] Que sucede si \\(X\\) e \\(Y\\) son variables independientes: \\[f_{X/Y}(x/y)=\\frac{f(x,y)}{f(y)}=\\frac{f(x)f(y)}{f(y)}=f(x)\\] \\[f_{Y/X}(y/x)=\\frac{f(x,y)}{f(x)}=\\frac{f(x)f(y)}{f(x)}=f(y)\\] 1.8 Medidas de relación entre dos variables Estas medidas nos ayudan a conocer si dos variables están relacionadas y nos permite saber el tipo de relación (directa, inversa) y también podemos saber la intensidad de la relación. Las medidas son: La Covarianza \\(cov(X,Y)\\) es una medida absoluta de relación: \\[cov(X,Y)=E[(X-E[X])(Y-E[Y])]\\] Una alternativa a esta formula (versión corta). \\[cov(X,Y)=E[XY]-E[X]*E[Y]\\] Tarea, demostrar lo anterior. Otra medida importantes es la correlación entre \\(X\\) e \\(Y\\), esta es una medida relativa, que cumple la propiedad: \\(-1 \\leq corr(X,Y) \\leq 1\\), esta se define como: \\[corr(X,Y)=\\frac{cov(X,Y)}{\\sqrt{V(X)V(Y)}}=\\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}\\] * Si \\(cov_{xy}\\) o \\(corr_{xy}\\) son distintas de 0, podemos afirmar que existe relación * Si \\(cov_{xy}&gt;0\\) o \\(corr_{xy}&gt;0\\) la relación entre \\(X\\) e \\(Y\\) es directa * Si \\(cov_{xy}&lt;0\\) o \\(corr_{xy}&lt;0\\) la relación entre \\(X\\) e \\(Y\\) es inversa * La intensidad de la dirección de la relación nos la da \\(corr_{xy}\\), mientras más cercana a \\(corr_{xy}\\rightarrow 1\\) la relación directa es más fuerte, \\(corr_{xy}\\rightarrow -1\\) la relación inversa es más fuerte * Si \\(corr_{xy}\\rightarrow 0\\) podemos decir que las variables no están relacionadas (cuasi-independencia) la correlación mide principalmente relaciones lineales. Ejercicio, Demostrar que si \\(X\\) y \\(Y\\) son independientes entonces: \\[E[XY]=E[X]*E[Y]\\] Demostración, \\[E[XY]=\\int_{Rx}\\int_{Ry} xy f(x,y) dy dx=\\int_{Rx}\\int_{Ry} xy f(x)f(y) dy dx\\] \\[=\\int_{Rx} x f(x)\\left( \\int_{Ry} y f(y) dy \\right) dx=E[Y] \\int_{Rx} xf(x) dx=E[X]E[Y] \\] Como resultado de lo anterior, si \\(X\\) e \\(Y\\) son independientes: \\[cov(X,Y)=E[XY]-E[X]*E[Y]=E[X]E[Y]-E[X]E[Y]=0\\] Si dos variables son independientes la covarianza y la correlación son iguales a cero, el inverso de esta afirmación no necesariamente es cierta. Nota: \\[E[Y/X=x]=\\int_{Ry} y * f_{Y/X}(y/x) dy\\] "],["tema-2-distribuciones-muestrales.html", "2 Tema 2: Distribuciones muestrales 2.1 Muestras y población 2.2 Parámetros, estadísticas y estimadores. 2.3 Distribución muestral 2.4 Distribución muestral para la media 2.5 Teorema del límite central 2.6 Distribución muestral para la diferencia de medias 2.7 Distribución muestral para la proporción 2.8 Distribución muestral para la varianza 2.9 Distribución \\(\\chi^2\\) 2.10 Distribución t-student 2.11 Distribución Fisher 2.12 Ejercicios", " 2 Tema 2: Distribuciones muestrales A partir de este tema la estadística esta vinculada con la inferencia sobre los parámetros de la información/datos. 2.1 Muestras y población Definición: Una población es una colección de objetos, estos objetos tienen variables. Sea nuestra población \\(U\\), esta población puede ser finita o infinita Finitas, \\[U=\\{u_1, u_2, \\ldots , u_i,...,u_N \\}\\] Infinita, \\[U=\\{u_1, u_2, \\ldots , u_i,... \\}\\] Cada elemento de \\(U\\) tiene variables o características asociadas: \\[u_i=\\{X_{i1}, X_{i2}, \\ldots, X_{iP} \\}\\] \\[u_j=\\{X_{j1}, X_{j2}, \\ldots, X_{jP} \\}\\] Definición, Muestra: Una muestra es un subconjunto de U. Normalmente una muestra tiene un tamaño \\(n\\), el mecanismo para obtener la muestra de \\(U\\) puede ser con reposición o sin reposición, en cualquier caso podemos anotar esto de la siguiente forma, sea \\(s\\) una muestra: \\[s=\\{u_{1}^*,u_2^*, \\ldots, u_n^* \\}\\] Note que los elementos \\(u_1\\) y \\(u_1^*\\) no necesariamente son los mismos. El subconjunto \\(s\\) no es único y en realidad existen muchas muestras posibles, según el contexto, esto depende: Del tamaño de \\(N\\), \\(n\\) Del mecanismo s/rep, c/rep. Ejercicios, Sea la población \\(U=\\{a,b,c,d,e,f\\}\\), se define una muestra de \\(n=3\\), escriba todas las muestras posibles según ambos mecanismos de reposición. Solución, * (s/rep), 20: \\(s_1=\\{a,b,c\\}\\), \\(s_2=\\{a,b,d \\}\\), \\(\\ldots\\) ,\\(s_{20}=\\{d,e,f\\}\\) * (c/rep), 216: \\(s_1=\\{a,a,a\\}\\), \\(s_2={a,a,b}\\), \\(\\ldots\\), \\(s_{216}=\\{f,f,f\\}\\) En una población de 90 estudiantes, si se define una muestra de 10 estudiantes, según ambos mecanismos de selección ¿Cuantas muestras se pueden armar? s/rep: 5720645481903 c/rep: 34867844009999998976 Sin reposición: \\[Muestras_{Posibles}=\\binom{N}{n}\\] Con reposición: \\[Muestras_{Posibles}=N^n\\] Imaginemos a la primera variable de interés \\(X_1\\), para el universo esta variable tiene los elementos: \\[X_1=\\{X_{11}, X_{21}, X_{31}, \\ldots, X_{N1} \\}\\] Imaginemos que observamos a \\(X_1\\), para la muestra. \\[X_1^*=\\{X_{11}^*, X_{21}^*, X_{31}^*, \\ldots, X_{n1}^* \\}\\] Estos \\(X_{i1}^*\\) para los \\(i=1,\\ldots,n\\) son variables aleatorias. Por lo tanto \\(X_1\\) es un vector aleatorio de tamaño \\(n\\). De ahora en adelante vamos a trabajar con un solo vector aleatorio denominado \\(X\\), de tal forma que este sea la colección de \\(n\\) variables aleatorias. \\[X=\\{X_1,X_2,\\ldots,X_n \\}\\] Definición. La colección del vector aleatorio \\(X=\\{X_1,X_2,\\ldots,X_n \\}\\), son independientes e idénticamente distribuidas (iid) si la distribución conjunta de las \\(n\\) variables puede ser escrita como \\(f(x_1,x_2,\\ldots,x_n)=f(x_1)*f(x_2)*\\ldots*f(x_n)\\) y además todas las \\(x_i\\) tienen la misma función de distribución \\(F(x)\\). Definición Sea \\(N\\) el tamaño de la población y \\(n\\) el tamaño de la muestra, ambos valores para fines de este capítulo son fijas o constantes. 2.2 Parámetros, estadísticas y estimadores. El objetivo de la estadística es aprender acerca de las características de una población. Estas características las vamos a llamar parámetros. Definición, Un parámetro \\(\\theta\\) es una función sobre la población \\(U\\). \\[\\theta=f(U,X,Y,Z,\\ldots)\\] Nota: Los parámetros de una población son constantes. Ejemplo, sea el universo los 10 primeros números naturales y sus valores. \\(Y=\\{1,2,3,4,5,6,7,8,9,10\\}\\). Sobre estos valores de esta población de \\(N=10\\) se pueden calcular los siguientes parámetros. Total \\[\\theta_1=t_y=\\sum_U y_i=55\\] * Media \\[\\theta_2=\\mu_y=\\frac{t_y}{N}=\\frac{55}{10}=5.5\\] Máximo: \\(\\theta_3=max(y)=10\\) Mínimo: \\(\\theta_4=max(y)=1\\) Es posibles hacer transformaciones sobre \\(Y\\), sea \\(Z\\) una variables binaria que identifique a los números primos de \\(Y\\); \\(1=primo\\), \\(0=\\sim primo\\) \\[Z=\\{1,1,1,0,1,0,1,0,0,0 \\}\\] Calcular el promedio de \\(Z\\) \\[\\theta_5=\\mu_z=\\frac{5}{10}=0.5\\] Cuando obtenemos la media de un vector binario, obtenemos lo que se denomina un proporción \\[\\theta_5=P_a=\\frac{\\#A}{N}\\] \\[\\theta_5=P_{primos}=\\frac{\\#primos}{N}\\] Definición, estadística Se denomina estadística a una función sobre la muestra Definición, estimador Un estimador \\(\\hat{\\theta}\\) para el parámetro \\(\\theta\\) es una estadística que busca aproximar/adivinar el valor de \\(\\theta\\) Ejemplo, para la variable \\(Y=\\{1,2,3,4,5,6,7,8,9,10\\}\\), imaginar que se selecciona un muestra de tamaño \\(n=4\\) s/rep. La cantidad de muestras posibles es de 210, supongamos que realizamos 2 procesos de selección para la muestra y obtenemos: \\(s_1=\\{8, 1, 3, 7\\}\\) \\(s_2=\\{8, 2, 6, 5\\}\\) Sabemos que el parámetro del total de \\(Y\\) es \\(t_y=55\\), ¿Qué? función se puede aplicar sobre la muestra para postular a un estimador que se aproxime a \\(t_y\\) sobre las 2 muestras seleccionadas \\[\\hat{\\theta}_1=\\hat{t}_y=\\sum_s y_i\\] Para \\(s_1\\) el valor del estimador es de \\(\\hat{t}_{y,s1}=19\\), \\(\\hat{t}_{y,s2}=21\\), los valores evaluados sobre una muestra y un estimador se conoce como estimación \\[\\hat{\\theta}_2=\\hat{t}_y=\\prod_s y_i\\] Las estimaciones con el estimador propuesto \\(\\hat{t}_{y,s1}=8*1*3*7=168\\), \\(\\hat{t}_{y,s1}=480\\) \\[\\hat{\\theta}_3=\\frac{\\prod_s y_i}{3}\\] \\(\\hat{t}_{y,s1}=56\\), \\(\\hat{t}_{y,s2}=160\\) \\[\\hat{\\theta}_4=\\frac{\\sum_{s}y_i^2}{2}\\] \\(\\hat{t}_{y,s1}=61.5\\), \\(\\hat{t}_{y,s2}=64.5\\) \\[\\hat{\\theta}_5=\\frac{N}{n} \\sum_{s}y_i \\] \\(\\hat{t}_{y,s1}=47.5\\), \\(\\hat{t}_{y,s2}=52.5\\) 2.3 Distribución muestral Recordar que una estadística es una función sobre la muestra y sobre los valores que toman las variables aleatorias vinculadas a esta. Como la estadística es una función sobre las muestras aleatorias (muestras posibles) las evaluaciones que se realizan para cada una de las muestras posibles (estimadores) conforman lo que vamos a denominar una distribución muestral. Por ejemplo si planteamos al estimador del parámetro del total, recordar: \\[\\theta=t_y=\\sum_U y_i\\] Un estimador para este parámetro será: \\[\\hat{\\theta}=\\hat{t}_y=\\frac{N}{n} \\sum_s y_i\\] Este \\(\\hat{\\theta}\\) es una estadística sobre las muestras aleatorias, por lo tanto podemos decir que existe una distribución de probabilidad para este estimador, a esa distribución de probabilidad se conoce como distribución muestral. Ejemplo práctico. Supongamos que de una población de 6 personas tenemos la información de sus ingresos mensuales. \\(Y_{Ingresos}=\\{2000,3000,3500,0,6000,4500\\}\\). \\(N=6\\) Supongamos que seleccionamos una muestra de tamaño \\(n=3\\) de esta población, para ambos mecanismos de selección (s/rep, c/rep), se pide para ambos mecanismos: Conocer la cantidad de muestras posibles y mostrar estas. Para el estimador \\[\\hat{\\bar{Y}}=\\frac{1}{n}\\sum_s y_i\\] construir su distribución muestral y calcular su esperanza y su varianza * Para el estimador; \\[\\hat{t}_y=\\frac{N}{n}\\sum_s y_i\\] construir su distribución muestral y calcular su esperanza y su varianza Respuesta, (S/rep) Las muestras posibles son 20, estas muestras posibles son: Y&lt;-c(2000,3000,3500,0,6000,4500) s&lt;-combn(Y,3) s ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## [1,] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 3000 3000 3000 3000 ## [2,] 3000 3000 3000 3000 3500 3500 3500 0 0 6000 3500 3500 3500 0 ## [3,] 3500 0 6000 4500 0 6000 4500 6000 4500 4500 0 6000 4500 6000 ## [,15] [,16] [,17] [,18] [,19] [,20] ## [1,] 3000 3000 3500 3500 3500 0 ## [2,] 0 6000 0 0 6000 6000 ## [3,] 4500 4500 6000 4500 4500 4500 Para el estimador de la media; Tomar en cuenta que el valor del parámetro de la media poblacional es: \\(\\mu_y=\\sum_U y_i /N=3166.667\\) y&lt;-apply(s,2,sum)/3 #Distribución muestral para el estimador de la media hist(y) abline(v=mean(Y),col=&quot;red&quot;,lwd=3) # calcular la esperanza y la varianza uy&lt;-sum(y*(1/20)) # esperanza del estimador de la media sum((y-uy)^2*(1/20)) # varianza de la media muestral ## [1] 711111.1 \\[E[\\hat{\\theta}]=\\sum_{Rs} \\hat{\\theta_s} P(\\hat{\\theta}=\\hat{\\theta_s})\\] \\[V(\\hat{\\theta})=E[(\\hat{\\theta}-E[\\hat{\\theta}])^2]=\\sum_{s}(\\hat{\\theta_s}-E[\\hat{\\theta}])^2*P(\\hat{\\theta}=\\hat{\\theta})\\] Nota, Si \\(E[\\hat{\\theta}]=\\theta\\) decimos que el estimador \\(\\hat{\\theta}\\) es un estimador insesgado (sin sesgo) El estimador de la media muestral, es un estimador insesgado de la media poblacional. Para el estimador del total; Tomar en cuenta que el valor del parámetro del total poblacional es: \\(t_y=\\sum_U y_i=19000\\) ty&lt;-apply(s,2,sum)*(6/3) #Distribución muestral para el estimador del total hist(ty) abline(v=sum(Y),col=&quot;red&quot;,lwd=3) pty&lt;-sum(ty*(1/20)) # esperanza sum((ty-pty)^2*(1/20)) # varianza de la media muestral ## [1] 25600000 \\[E[\\hat{t}_y]=E[N*\\bar{Y}]=N E[\\bar{Y}]=N*u_y=N*\\frac{\\sum_U y_i}{N}=\\sum_U {y_i}=t_y\\] Repetir los cálculos para un muestreo con reposición. Muestras probables \\(6^3=N^n=216\\). Y&lt;-round(rnorm(25,30,5)) s&lt;-combn(Y,10) y&lt;-apply(s,2,sum)/10 hist(y) abline(v=mean(Y),col=&quot;red&quot;,lwd=2) 2.4 Distribución muestral para la media Recordar que para una población (\\(U\\)) con alguna variable \\(X\\) de tipo cuantitativa se puede obtener el parámetro de la media, definido como: \\[\\mu_x=\\frac{\\sum_U x_i}{N}\\] Esta variable \\(X\\) en la población por lo tanto tiene su media \\(\\mu_x\\) y también tiene su varianza, denotada por \\(\\sigma_x^2\\). Teorema: Sean \\(X_1,X_2,\\ldots,X_n\\) variables aleatorias para una muestra de tamaño \\(n\\) extraida de la población \\(U\\), donde estas \\(X_i\\) independientes e idénticamente distribuidas (iid) como: \\(X_i\\sim .(E[X_i]=\\mu_x,V(X_i)=\\sigma_x^2)\\), entonces, si: \\[\\bar{X}=\\frac{\\sum_s x_i}{n}\\] Tenemos que \\[E[\\bar{X}]=\\mu_x\\] \\[V(\\bar{X})=\\sigma^2_{\\bar{x}}=\\frac{\\sigma^2_x}{n}\\] Demostración, \\[E[\\bar{X}]=E\\left[\\frac{\\sum_s x_i}{n}\\right]=\\frac{1}{n}E[x_1+x_2+\\ldots+x_n]=\\frac{1}{n}\\left(E[x_1]+E[x_2]+\\ldots+E[x_n] \\right)=\\] \\[=\\frac{1}{n}(\\mu_x+\\mu_x+\\ldots+\\mu_x)=\\frac{n \\mu_x}{n}=\\mu_x\\] Si, \\(X\\) e \\(Y\\) son independientes \\(Cov(X,Y)=0\\). \\[V(X+Y)=V(X)+V(Y)\\] \\[V(\\bar{X})=V\\left(\\frac{\\sum_s x_i}{n}\\right)=\\frac{1}{n^2}V(x_1+x_2+\\ldots+x_n)=\\frac{1}{n^2}\\{V(x_1)+\\ldots+V(x_n)\\}=\\] \\[=\\frac{1}{n^2}(\\sigma^2_x+\\sigma^2_x+\\ldots+\\sigma^2_x)=\\frac{n \\sigma_x^2}{n^2}=\\frac{\\sigma^2_x}{n}\\] 2.5 Teorema del límite central Teorema: Si \\(\\bar{X}\\) es la media de una muestra aleatoria de tamaño \\(n\\). Tomada de una población \\(U\\) con media \\(\\mu_x\\) y varianza finita \\(\\sigma^2_x\\). Entonces la forma límite de la distribución de: \\[Z=\\frac{\\bar{X}-E[\\bar{X}]}{\\sqrt{V(\\bar{X})}}=\\frac{\\bar{X}-\\mu_x}{\\frac{\\sigma_x}{\\sqrt{n}}}\\] a medida que \\(n \\rightarrow \\infty\\), podemos asegurar que \\(Z\\sim N(0,1)\\), en este marco se puede decir a medida que \\(n\\) es más grande \\(\\bar{X}\\sim N(\\mu_x,\\frac{\\sigma^2_x}{n})\\) Nota: esta idea de \\(n\\) grande se usa tradicionalmente el valor de \\(n&gt;30\\), hay textos que plantean \\(n=20\\). Simulación del teorema del límite central: N&lt;-1000000 x&lt;-round(runif(N,0,10000),0)# ingresos mensuales de una población hist(x) n&lt;-30 choose(N,n) ## [1] 3.768348e+147 #simular 1000 muestras distintas de tamaño n y calcular su media. xbar&lt;-NULL for(i in 1:10000){ s&lt;-sample(x,n) xbar[i]&lt;-mean(s) } hist(xbar) plot(density(xbar),col=&quot;blue&quot;,lwd=2) points(density(rnorm(10^6,mean(x),sqrt(var(x)*((n-1)/n))/sqrt(n))),type=&quot;l&quot;,col=&quot;red&quot;,lwd=2) 2.6 Distribución muestral para la diferencia de medias Sean dos poblaciones \\(U_1\\) y \\(U_2\\) independientes con medias y varianzas respectivamente: \\(\\mu_{x_1}\\) y \\(\\mu_{x_2}\\), \\(\\sigma^2_{x_1}\\) y \\(\\sigma^2_{x_2}\\). Teorema: La distribución muestral de las diferencias de media \\(\\bar{X_1}-\\bar{X_2}\\) esta tiene una distribución aproximadamente normal (\\(n\\rightarrow \\infty\\)) con medias y varianzas dadas por: \\[E[\\bar{X_1}-\\bar{X_2}]=\\mu_{x_1}-\\mu_{x_2}\\] \\[V(\\bar{X_1}-\\bar{X_2})=\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\] Demostración: \\[E[\\bar{X_1}-\\bar{X_2}]=E[\\bar{X_1}]-E[\\bar{X_2}]=\\mu_{x_1}-\\mu_{x_2}\\] \\[V(\\bar{X_1}-\\bar{X_2})=V(\\bar{X_1})+V(\\bar{X_2})=\\frac{\\sigma^2_{x_1}}{n_1}+\\frac{\\sigma^2_{x_2}}{n_2}\\] 2.7 Distribución muestral para la proporción La proporción no es nada más que un caso especial de la media para \\(X\\) que toma valores binarios según alguna característica de interés. Sea \\(P_A=\\frac{\\#A}{N}=\\frac{\\sum_U x_i}{N}\\), \\(x_i=1\\) si \\(i \\in A\\) \\(x_i=0\\) eoc. la proporción de alguna característica de la población. Así la el estimador de la proporción sera: \\[\\hat{P}_A=\\frac{\\sum_s{x_i}}{n}=\\frac{\\#a}{n}\\] Teorema: Para el estadístico \\(\\hat{P}_A\\) se cumple cuando \\(n\\) tiende a infinito los siguientes resultados: \\(E[\\hat{P}_A]=P_A\\) \\(V(\\hat{P}_A)=\\frac{\\sigma^2_A}{n}\\) \\(\\hat{P}_A\\sim N(P_A,\\frac{\\sigma^2_A}{n})\\), cuando \\(n \\rightarrow \\infty\\) Tarea, encontrar la forma de \\(\\sigma^2_A\\), sabiendo que \\(x_i\\) es binaria. \\[\\sigma^2_A=\\frac{\\sum_U(x_i-\\mu_x)^2}{N}= P_A *(1-P_A)\\] 2.8 Distribución muestral para la varianza Recordar que para una población \\(U\\), si observamos a una variable de interés respecto sus características podemos obtener medidas de centralidad y también medidas de variabilidad, por ejemplo, sea \\(X\\) una variables definida para toda la población, y definamos los siguientes parámetros de \\(X\\). \\[\\mu_x=\\frac{\\sum_U x_i}{N}\\] Esta \\(\\mu_x\\) es una medida de centralidad, normalmente conocida como media, promedio de \\(X\\), la otra medida puede ser: \\[\\sigma^2_x=\\frac{\\sum_U (x_i-\\mu_x)^2}{N}\\] \\(\\sigma^2_x\\) es la varianza poblacional Ejemplo, Sea una población de \\(N=5\\) elementos con la variable \\(X=\\{10,15,20,20,35\\}\\), calcular \\(\\mu_x\\) y \\(\\sigma^2_x\\). \\(\\mu_x=20\\) \\(\\sigma^2_x=70\\) Suponer que se toman muestras aleatorias de esta población de tamaño \\(n=3\\) sin reposición. La cantidad de muestras posibles es 10. x&lt;-c(10,15,20,20,35) s&lt;-combn(x,3) s ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 10 10 10 10 10 10 15 15 15 20 ## [2,] 15 15 15 20 20 20 20 20 20 20 ## [3,] 20 20 35 20 35 35 20 35 35 35 #distribución muestral de la media mean(apply(s, 2, mean)) ## [1] 20 Pensemos para el caso de la varianza en posibles estadísticos (estimadores): \\[\\hat{\\theta}_1=\\hat{\\sigma}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n}\\] \\[\\hat{\\theta}_2=\\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] x&lt;-c(10,15,20,20,35) n&lt;-3;N&lt;-5 s ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 10 10 10 10 10 10 15 15 15 20 ## [2,] 15 15 15 20 20 20 20 20 20 20 ## [3,] 20 20 35 20 35 35 20 35 35 35 var(x)*((N-1)/N) ## [1] 70 theta1&lt;-apply(s,2,var)*((n-1)/n) theta2&lt;-apply(s,2,var) theta1 ## [1] 16.666667 16.666667 116.666667 22.222222 105.555556 105.555556 5.555556 ## [8] 72.222222 72.222222 50.000000 theta2 ## [1] 25.000000 25.000000 175.000000 33.333333 158.333333 158.333333 8.333333 ## [8] 108.333333 108.333333 75.000000 plot(density(theta1),xlim=c(-50,300)) points(density(theta2),col=&quot;red&quot;,type=&quot;l&quot;) mean(theta1) #E[] ## [1] 58.33333 mean(theta2) #E[] ## [1] 87.5 Notar que para el ejemplo \\(E[\\hat{\\theta_1}]\\) ni \\(E[\\hat{\\theta_2}]\\) se acercan a \\(\\sigma^2_x\\), sin embargo, \\(E[\\theta_2]=S^2_x\\). \\[S^2_x=\\frac{\\sum_U (x_i-\\mu_x)^2}{N-1}\\] &gt; Teorema Sea \\(X_1,X_2,\\ldots,X_n\\) una muestra aleatoria extraída de una población Normal \\(N(\\mu_x,\\sigma^2_x)\\), definamos al estadístico: \\[\\hat{S}^2_x=\\frac{\\sum_s (x_i-\\bar{x})^2}{n-1}\\] Entonces, se cumple \\[\\chi^2=\\frac{(n-1)\\hat{S}^2_x}{\\sigma^2_x}=\\frac{\\sum_s (x_i-\\bar{x})^2}{\\sigma^2_x}\\sim \\chi^2(n-1)\\] Simulación; #población de tamaño N=1000 set.seed(999) x&lt;-rnorm(1000,20,5) hist(x) #suponer que se extra una muestra de n=20 de esta población, choose(1000,20) ## [1] 3.394828e+41 n&lt;-20 #vamos a simular unas 1500 muestras posibles sigma=25 x2&lt;-NULL for(i in 1:100000){ x2[i]&lt;-(var(sample(x,20))*(n-1))/25 } x2 ## [1] 28.012821 13.303299 12.916730 15.444633 20.070571 18.598947 12.987323 ## [8] 26.229370 17.443031 29.634228 19.228462 16.847166 13.837075 12.306583 ## [15] 20.316401 6.691069 18.716776 17.617646 11.830859 13.593042 21.748010 ## [22] 11.481425 9.106947 9.473332 8.428679 13.683606 15.170322 12.459595 ## [29] 16.076973 11.618863 19.399864 18.619069 15.620865 23.144277 19.006885 ## [36] 31.386997 16.965785 23.948281 12.457150 10.199054 15.890153 14.075756 ## [43] 18.272734 16.332558 14.613984 25.587406 17.274712 13.451648 15.951381 ## [50] 29.599580 28.751381 27.883479 24.847844 28.122133 21.778322 13.042337 ## [57] 12.756593 13.588587 15.381787 15.173170 17.064814 10.215598 21.617442 ## [64] 13.529998 11.333242 11.942220 13.961767 11.204895 9.383787 16.447210 ## [71] 14.246598 22.518621 12.660786 17.883892 13.073025 21.860708 25.675834 ## [78] 11.964393 16.152063 33.642039 20.788343 25.146765 18.091672 27.518788 ## [85] 17.450027 5.250517 35.510067 20.579231 18.107810 7.837748 28.530112 ## [92] 20.276653 21.042629 17.020561 11.251209 12.107773 19.502901 9.780230 ## [99] 26.695559 20.650701 8.803620 17.234677 16.699761 14.213930 21.911950 ## [106] 23.045968 15.181081 12.116822 26.385342 23.237274 20.975653 20.512500 ## [113] 19.050228 14.114346 26.703015 15.162247 14.347395 16.567249 12.277332 ## [120] 20.445499 13.605211 26.066466 20.531418 11.402210 18.172740 19.021145 ## [127] 11.423122 16.202069 22.060284 26.552551 22.005707 7.734121 16.220572 ## [134] 14.392079 14.301308 21.769430 22.420456 15.618478 17.447861 17.127857 ## [141] 14.726064 25.266101 15.663966 17.745215 18.590808 19.813088 35.369779 ## [148] 22.437589 26.518719 5.550327 22.442252 22.630876 10.092094 11.243247 ## [155] 21.629633 25.316789 14.803836 17.506524 13.280020 22.070279 19.336450 ## [162] 39.384576 22.162003 16.597604 21.865927 22.053510 17.358355 14.287258 ## [169] 15.307627 15.212806 20.894767 12.342412 15.181616 16.823311 22.328185 ## [176] 16.870523 11.932493 14.608943 20.831203 21.081735 15.847040 13.459898 ## [183] 14.848697 16.029962 11.564021 14.063650 16.309441 19.966832 13.932327 ## [190] 9.026528 18.082221 12.720960 15.024540 18.718807 25.729287 15.350011 ## [197] 20.544050 18.969261 21.585216 31.963216 24.672649 25.671613 13.023054 ## [204] 28.851590 16.483956 11.303823 26.648202 8.478335 17.726722 15.430695 ## [211] 24.317257 32.335164 9.607395 26.820571 12.723560 15.235108 15.809233 ## [218] 22.814399 24.265359 20.872881 13.048042 12.300579 18.075927 11.937559 ## [225] 17.417532 18.025925 23.255247 20.109420 23.253181 14.372813 21.812857 ## [232] 14.024146 27.546107 15.498092 13.235267 14.098803 35.153901 18.649959 ## [239] 21.851561 19.819864 15.316509 15.795272 13.516208 20.682192 14.620640 ## [246] 14.449217 25.434318 26.168725 23.276445 28.915826 17.596923 28.010290 ## [253] 18.341387 17.879718 17.026406 27.204161 16.697266 26.320726 13.793118 ## [260] 24.030272 16.239461 30.971929 13.086218 11.798937 9.299183 19.134689 ## [267] 25.052224 13.346834 15.982050 23.669918 22.165420 18.176970 17.694919 ## [274] 20.192060 20.412274 35.497393 17.407584 19.655298 9.347727 25.572063 ## [281] 15.704892 13.559946 16.266726 14.442831 23.863729 17.000075 19.519409 ## [288] 25.114011 20.148385 22.266228 25.853997 22.662551 20.801108 10.925624 ## [295] 17.810104 26.175869 14.277400 13.318309 16.595470 15.266218 21.262551 ## [302] 8.580367 20.701016 16.503482 11.918446 13.505003 30.125166 13.223280 ## [309] 18.575549 34.856473 24.677999 20.881330 15.175015 20.602794 22.235731 ## [316] 24.510002 16.103575 14.920085 31.667352 25.540828 11.809267 18.319506 ## [323] 21.149436 21.635312 14.180294 19.957223 15.858002 17.166201 14.493781 ## [330] 14.093451 14.014025 22.213693 32.223125 17.241425 13.288698 13.250723 ## [337] 14.734604 13.995601 17.404789 18.592486 15.247989 17.637603 9.364431 ## [344] 26.359784 13.982073 20.573854 18.149615 9.110028 22.766342 21.045694 ## [351] 22.258774 12.546646 21.359003 15.692677 16.874471 20.538731 15.455086 ## [358] 25.956684 18.774150 16.489980 18.071091 28.006534 23.467235 17.147189 ## [365] 16.792277 15.447061 10.594702 12.256940 16.145375 17.736032 17.828512 ## [372] 12.531759 31.573850 20.210519 16.019958 17.578239 17.772428 15.250837 ## [379] 20.502192 14.979508 13.992929 21.260858 13.987037 18.380611 24.629809 ## [386] 21.920440 16.724372 13.391569 26.174884 10.900662 8.290249 9.866609 ## [393] 21.867045 19.955929 15.974358 9.058562 5.966568 34.844445 24.948710 ## [400] 20.310470 19.546296 11.888163 12.286091 9.132744 23.602785 20.014303 ## [407] 14.249253 15.506009 15.437651 9.281142 19.894464 23.939253 19.924322 ## [414] 15.741860 22.561480 28.045692 33.280957 22.673003 22.570943 11.497906 ## [421] 19.209914 18.966212 19.127201 13.153828 16.993053 21.556625 10.286481 ## [428] 14.782502 23.896517 13.143720 28.732325 8.044952 23.947529 17.891754 ## [435] 18.916209 17.088394 12.153914 23.588815 27.825393 18.556936 18.475666 ## [442] 18.546475 19.336447 29.765206 19.891351 12.766444 21.244537 31.254842 ## [449] 14.848853 19.255805 30.316743 15.625976 10.203716 17.495950 23.960264 ## [456] 17.154687 15.679591 22.816883 14.221807 21.113025 20.645354 20.407738 ## [463] 20.927321 16.558282 9.800061 14.866611 18.587171 16.813358 13.847721 ## [470] 22.178993 26.207978 21.315105 32.031176 20.617618 31.938098 10.901848 ## [477] 15.117016 8.140391 11.466804 14.176978 16.560051 18.967196 17.458704 ## [484] 10.764479 20.389500 12.153445 21.915928 16.673494 22.262874 23.939511 ## [491] 20.350276 14.509098 9.036172 14.584719 14.211505 21.439348 18.186422 ## [498] 21.754326 16.357508 16.754581 20.204783 9.461267 12.786030 10.312692 ## [505] 23.812264 10.630994 25.660905 24.008654 19.444126 13.635916 10.520709 ## [512] 20.515147 13.876175 15.491769 21.450161 17.486085 15.720568 17.524114 ## [519] 19.871122 22.700891 19.227107 17.761933 27.973973 19.886723 20.925573 ## [526] 23.055274 8.174269 13.171267 21.546352 16.652625 22.175516 23.707881 ## [533] 11.228519 11.387434 16.447506 24.676754 16.227232 11.739386 21.441527 ## [540] 15.707938 18.315900 19.424615 17.308554 24.028920 14.741197 20.185355 ## [547] 16.585532 26.366693 20.555072 12.281362 20.937916 23.546578 15.731429 ## [554] 19.801307 16.784120 20.765949 9.202193 11.329222 14.202221 11.452321 ## [561] 17.653245 18.144876 24.238262 15.465025 19.793484 11.315699 11.966883 ## [568] 10.439119 22.554602 15.515201 26.437958 26.615291 16.844931 26.615512 ## [575] 15.257409 19.586495 14.350989 13.182293 9.553267 4.491736 11.178881 ## [582] 22.472408 15.075446 18.204019 15.878175 15.658180 21.351499 16.204857 ## [589] 20.442849 10.947412 17.843151 15.867059 20.304103 17.002257 11.257525 ## [596] 20.192141 15.917608 14.602950 26.940746 19.137400 11.781175 14.305902 ## [603] 20.795786 30.453513 15.538217 15.403194 30.069716 7.593631 26.127048 ## [610] 15.769910 14.562004 30.980415 36.021805 24.049130 11.271203 17.409826 ## [617] 29.315625 23.809991 29.126933 14.257977 15.056230 23.566388 15.174209 ## [624] 17.382185 15.433936 26.392629 26.628483 15.262434 22.175066 19.249082 ## [631] 18.221410 17.595876 7.951111 17.223180 30.272167 16.662376 17.849205 ## [638] 9.536688 22.741268 23.950851 19.674210 14.901407 11.114026 14.322474 ## [645] 19.217378 18.881117 14.227646 12.417058 21.193379 6.238202 25.804310 ## [652] 16.128059 21.292803 13.583191 13.416589 16.998586 19.670594 19.571160 ## [659] 13.261077 10.385480 22.444720 15.186642 8.549110 12.592156 12.126122 ## [666] 21.612231 16.106128 7.185419 19.155048 14.783458 12.880983 23.028895 ## [673] 14.603729 13.465318 21.971983 28.639976 14.957262 16.521305 15.156209 ## [680] 16.997971 18.066716 10.105663 12.527995 25.242331 29.212772 13.279441 ## [687] 18.518994 23.081568 8.991684 18.891378 14.390195 19.481693 19.247036 ## [694] 15.173736 19.686150 15.104820 18.368197 17.131629 15.269682 27.958178 ## [701] 14.609845 18.348459 17.720580 17.278284 15.970391 31.189266 20.609505 ## [708] 13.667190 20.343798 23.676878 16.817175 27.798076 14.380824 23.119645 ## [715] 20.294323 12.397698 18.077482 15.548011 13.842401 15.732343 9.482712 ## [722] 8.778730 37.133787 24.199658 17.041008 16.273570 22.309599 14.140296 ## [729] 15.538825 19.018784 14.509132 22.133283 29.898216 21.899834 8.319081 ## [736] 28.376068 16.159040 14.711742 16.128796 17.363660 20.650008 20.296518 ## [743] 17.752446 19.662599 24.055001 26.253885 15.864813 22.608158 20.920132 ## [750] 11.771987 17.570611 20.586085 25.208490 20.049499 20.545025 22.385286 ## [757] 27.045100 22.773739 12.998953 18.532624 15.560464 17.676388 13.203288 ## [764] 18.897017 22.478345 19.065920 21.805359 16.342205 15.511562 29.192431 ## [771] 22.918919 20.299212 14.677958 19.784591 13.834712 16.187329 26.735234 ## [778] 14.768448 23.777946 29.613875 12.432792 37.751422 20.805643 16.105735 ## [785] 13.631205 14.710745 16.738886 18.346160 17.926659 11.742877 17.411051 ## [792] 18.677338 15.471975 25.991338 16.839181 11.470087 10.955131 26.606070 ## [799] 16.820614 24.129111 44.924757 15.479093 15.751811 21.996591 19.340129 ## [806] 11.096019 18.161506 21.631261 17.006621 27.793634 19.260289 9.979732 ## [813] 17.056580 20.108851 19.097732 18.232420 14.339916 19.669007 18.351885 ## [820] 9.760430 13.369856 17.570671 9.413871 16.272449 26.617002 14.471497 ## [827] 25.732663 21.930818 7.796016 13.842560 15.578838 14.404152 12.709148 ## [834] 37.570245 14.800009 19.169672 20.230360 18.097048 15.598180 24.112340 ## [841] 24.500908 10.507524 22.422073 15.871907 10.750142 15.342556 18.915039 ## [848] 21.791224 10.825029 18.084313 16.609825 13.699246 30.363761 8.009254 ## [855] 7.244581 18.225919 14.612970 16.141131 15.521536 10.797909 10.723517 ## [862] 20.243818 14.060388 15.937919 18.372922 7.746550 23.775462 16.209138 ## [869] 23.600860 16.307198 23.121222 12.642161 29.532925 27.326932 16.182682 ## [876] 20.959115 30.061046 25.554703 10.161560 16.051951 15.319699 13.966131 ## [883] 8.243000 16.726587 11.070633 21.128304 13.846221 14.991823 13.534118 ## [890] 9.887567 14.216242 22.186037 11.263336 29.407918 15.972765 8.487906 ## [897] 16.596630 12.153323 19.406587 20.387614 14.509980 12.219619 10.951065 ## [904] 21.920734 20.764404 27.442317 15.071589 24.042310 20.035989 7.720464 ## [911] 17.979013 28.385669 18.999167 22.648348 16.417482 7.843558 23.528440 ## [918] 21.043034 16.771143 19.063782 13.424514 13.113389 14.166799 13.271906 ## [925] 12.066717 19.910054 10.035170 14.076576 22.261232 14.348785 17.136114 ## [932] 38.194209 12.538656 19.148550 21.865993 20.316560 13.671468 18.326424 ## [939] 23.561805 13.496954 25.213289 23.173681 19.031335 12.886222 11.804907 ## [946] 7.554450 17.139227 22.100363 16.843482 8.286483 18.911796 22.663047 ## [953] 8.503524 22.059767 23.421176 13.994647 16.290766 19.480129 10.363203 ## [960] 15.488181 18.086993 24.379918 17.302920 15.080271 17.519433 19.353402 ## [967] 18.856519 17.311082 27.075140 34.118128 11.110204 10.452672 29.724010 ## [974] 34.261895 19.255181 19.463401 13.531030 19.884112 13.418974 9.565242 ## [981] 14.571297 15.543387 13.262746 16.333737 17.943791 20.294559 16.988730 ## [988] 11.853706 14.768046 19.212293 18.752969 14.972371 8.403704 22.308734 ## [995] 12.866281 16.714866 20.025841 11.190805 16.622595 10.999016 ## [ reached getOption(&quot;max.print&quot;) -- omitted 99000 entries ] plot(density(x2)) points(density(rchisq(1000000,19)),col=&quot;red&quot;,type=&quot;l&quot;) 2.9 Distribución \\(\\chi^2\\) Se dice que una variable aleatoria \\(X\\) tiene una distribución Chi-cuadrado \\(\\chi^2\\) con \\(v\\) grados de libertad. Se escribe como: \\(X \\sim \\chi^2(v)\\), donde el \\(Rx=\\{x&gt;0\\}\\), si su función de densidad es: \\[f(x)=\\frac{1}{2^\\frac{v}{2} \\Gamma(\\frac{v}{2})}*x^{\\frac{v}{2}-1}*e^{-\\frac{x}{2}}\\] curve(dchisq(x,1),xlim=c(0,40),ylim=c(0,0.4)) for(v in 1:30){ curve(dchisq(x,v),add=T,col=v) } Donde, \\[E[X]=v\\] \\[V(X)=2v\\] Ejercicio, Encuentre la probabilidad de que una muestra aleatoria de \\(n=25\\) de un población normal con varianza \\(\\sigma^2=9\\), tenga una varianza muestral \\(\\hat{S^2}\\) entre 4 y 15. #ejemplo para usar R para calcular probabilidades de la Chi2 pchisq(4,10) # F(t)=P(X&lt;t): F(4) ## [1] 0.05265302 2.10 Distribución t-student 2.11 Distribución Fisher 2.12 Ejercicios "]]
